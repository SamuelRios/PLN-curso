{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a4af6f-51ba-4110-9de2-a6e67ea6a8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Coletando links do BRACIS 2024 …\n",
      "[INFO] 116 artigos encontrados.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdb5171180e4dbd960b975782847be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Raspando artigos:   0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CSV 'bracis2024_nlp.csv' criado com 22 artigos.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Crawler STIL 2024 (SOL · OJS) — gera CSV com artigos de NLP.\n",
    "Usa meta-tags para extrair todos os campos e tqdm.notebook para barra.\n",
    "Requisitos:\n",
    "    pip install requests beautifulsoup4 tqdm pandas\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from typing import List, Dict\n",
    "\n",
    "# tqdm notebook ou fallback CLI\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "BASE  = \"https://sol.sbc.org.br\"\n",
    "INDEX = f\"{BASE}/index.php/bracis\"\n",
    "\n",
    "# Filtro opcional NLP (remova se quiser TODO)\n",
    "KW = re.compile(\n",
    "    r\"\\b(nlp|linguagem|language|text|texto|corpus|bert|transformer|llm|\"\n",
    "    r\"summariz|resum|translation|tradu[cç]|sentiment|opini|\"\n",
    "    r\"classifica|parsing|tagging|entity|question|answer|dialog|speech)\\b\",\n",
    "    flags=re.I,\n",
    ")\n",
    "\n",
    "def get_index_links() -> List[Dict[str,str]]:\n",
    "    \"\"\"\n",
    "    Coleta todos os artigos da página principal, stripping de sufixos de versão.\n",
    "    Retorna lista de dicts {'url': URL_base_do_artigo}\n",
    "    \"\"\"\n",
    "    html = requests.get(INDEX, timeout=20).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links, seen = [], set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if \"/article/view/\" in href:\n",
    "            # Mantém só /article/view/<id>\n",
    "            m = re.match(r\"(.*?/article/view/\\d+)\", href)\n",
    "            if not m:\n",
    "                continue\n",
    "            url = urljoin(BASE, m.group(1))\n",
    "            if url not in seen:\n",
    "                seen.add(url)\n",
    "                links.append({\"url\": url})\n",
    "    return links\n",
    "\n",
    "def parse_article(url: str) -> Dict[str,str]:\n",
    "    \"\"\"\n",
    "    Extrai de meta-tags: title, authors, pages, doi, pdf_url, article_url.\n",
    "    \"\"\"\n",
    "    resp = requests.get(url, timeout=20)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Título via meta\n",
    "    meta_title = soup.find(\"meta\", {\"name\": \"citation_title\"})\n",
    "    title = meta_title[\"content\"].strip() if meta_title and meta_title.get(\"content\") else \"N/A\"\n",
    "\n",
    "    # Autores via meta\n",
    "    authors_meta = soup.find_all(\"meta\", {\"name\": \"citation_author\"})\n",
    "    authors = [m[\"content\"].strip() for m in authors_meta if m.get(\"content\")]\n",
    "    authors = \", \".join(authors) if authors else \"N/A\"\n",
    "\n",
    "    # Páginas via metas first/last\n",
    "    first = soup.find(\"meta\", {\"name\": \"citation_firstpage\"})\n",
    "    last  = soup.find(\"meta\", {\"name\": \"citation_lastpage\"})\n",
    "    if first and first.get(\"content\") and last and last.get(\"content\"):\n",
    "        pages = f\"{first['content']}-{last['content']}\"\n",
    "    else:\n",
    "        pages = \"\"\n",
    "\n",
    "    # DOI via meta\n",
    "    meta_doi = soup.find(\"meta\", {\"name\": \"citation_doi\"})\n",
    "    doi = meta_doi[\"content\"].strip() if meta_doi and meta_doi.get(\"content\") else \"\"\n",
    "\n",
    "    # PDF URL via meta\n",
    "    meta_pdf = soup.find(\"meta\", {\"name\": \"citation_pdf_url\"})\n",
    "    pdf_url = meta_pdf[\"content\"].strip() if meta_pdf and meta_pdf.get(\"content\") else url\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"authors\": authors,\n",
    "        \"pages\": pages,\n",
    "        \"doi\": doi,\n",
    "        \"article_url\": url,\n",
    "        \"pdf_url\": pdf_url\n",
    "    }\n",
    "\n",
    "def main() -> None:\n",
    "    print(\"[INFO] Coletando links do BRACIS 2024 …\")\n",
    "    entries = get_index_links()\n",
    "    print(f\"[INFO] {len(entries)} artigos encontrados.\")\n",
    "\n",
    "    rows: List[Dict[str,str]] = []\n",
    "    for item in tqdm(entries, desc=\"Raspando artigos\"):\n",
    "        try:\n",
    "            art = parse_article(item[\"url\"])\n",
    "            # Aplicar filtro NLP opcional\n",
    "            if not KW.search(art[\"title\"]):\n",
    "                continue\n",
    "            rows.append(art)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Falha em {item['url']}: {e}\", file=sys.stderr)\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    # Salva CSV\n",
    "    outfile = \"lista_artigos_bracis_crawler.csv\"\n",
    "    cols = [\"title\", \"authors\", \"pages\", \"doi\", \"article_url\", \"pdf_url\"]\n",
    "    with open(outfile, \"w\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        writer = csv.DictWriter(fp, fieldnames=cols)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"[INFO] CSV '{outfile}' criado com {len(rows)} artigos.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900e862-3964-426b-90b7-7c22187eee65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
