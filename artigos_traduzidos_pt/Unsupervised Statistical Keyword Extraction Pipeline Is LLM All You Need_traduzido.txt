Extração de Palavras-Chave sem Supervisão: É Tudo que Você Precisa? Fernando Rezende Zagatti1, Daniel Lucrédio1 e Helena de Medeiros Caseli1 Universidade Federal de São Carlos, São Carlos, Brasil Departamento de Computação fernando.zagatti@estudante.ufscar.br {daniel.lucredio,helenacaseli}@ufscar.br Resumo. A extração de palavras-chave é um passo importante para a interpretação de textos, servindo para identificar e destacar as palavras ou frases mais significativas dentro de um texto. Este passo é essencial para várias aplicações, tais como resumo, indexação e recuperação de informação. Este artigo apresenta uma pipeline de extração de palavras-chave personalizada denominada USKE (Extração de Palavras-Chave Estatística sem Supervisão) e compara seu desempenho com modelos de linguagem grandes (LLMs). A USKE é capaz de fornecer resultados rápidos e simples com base em métodos estatísticos, mesmo quando lidando com grandes conjuntos de dados. Nossa avaliação demonstra que, embora os LLMs possam alcançar resultados bons em sentenças únicas com contexto mínimo, eles requerem uma grande quantidade de processamento pós-edição e podem produzir respostas inconsistentes, enquanto a USKE destaca-se em eficiência e escalabilidade. Palavras-chave: Extração de palavras-chave · Processamento de Linguagem Natural · Modelos de Linguagem Grande. 1 Introdução Na era dos grandes dados, com volumes enormes de informações sendo geradas continuamente, a capacidade de identificar rapidamente e precisamente os principais tópicos, palavras-chave e termos de um grande volume de textos tornou-se altamente desejável [5,6]. Esta capacidade é essencial para gerenciar e aproveitar informações de forma eficiente em vários domínios, desde a academia até a indústria, onde insights rápidos podem impulsionar tomada de decisão e inovação. Dessa forma, a extração de palavras-chave desempenha um papel crucial em várias aplicações para Processamento de Linguagem Natural (NLP), tais como recuperação de informação [22] e resumo de texto [9].
Tradicionalmente, essa tarefa foi abordada usando métodos clássicos, principalmente não supervisionados e estatísticos, que frequentemente envolvem pré-processamento extensivo e o uso de características específicas do domínio. No entanto, com o advento de Modelos de Linguagem de Grande Escala (LLMs) como Representações de Encoder Bidirecionais para Transformadores (BERT) [4] e Transformador Gerador Treinado (GPT) [16], novas oportunidades surgiram para aumentar a precisão e a eficiência da extração de palavras-chave. Essas novas técnicas de geração de texto aproveitam técnicas de aprendizado profundo para capturar nuances contextual e semânticas, oferecendo maior precisão sobre métodos tradicionais, mas sua complexidade computacional e dependência de grandes conjuntos de dados de treinamento apresentam desafios práticos [10]. Nesse sentido, este estudo não apenas compara o desempenho desses modelos avançados com métodos estatísticos tradicionais, mas também introduz uma pipeline não supervisionada para realizar a extração de palavras-chave em dados textuais. O sistema proposto aberto-fonte, Extração de Palavras-Chave Estatística Não Supervisionada (USKE)1, emprega uma série de passos de pipeline de Processamento de Linguagem Natural (NLP) e técnicas estatísticas para identificar e extrair palavras-chave relevantes de textos. Ao integrar métodos de pré-processamento de texto com técnicas estatísticas, criamos uma extração de palavras-chave versátil e eficiente que não exige conhecimento especializado em LLM ou IA para ser usada. Dessa forma, este artigo visa comparar métodos clássicos contra abordagens baseadas em LLM modernas para avaliar suas respectivas fortes e fraquezas. Enquanto métodos tradicionais frequentemente lutam com grandes conjuntos de dados e contextos multilíngues, LLMs podem ser intensivas em recursos e menos viáveis para aplicações em tempo real. Portanto, por meio dessa análise comparativa, buscamos fornecer insights sobre se LLMs são realmente indissociáveis para a extração de palavras-chave ou se métodos tradicionais ainda possuem valor significativo.
Nossa pesquisa visa oferecer uma comparação direta entre métodos de extração de palavras-chave clássicos e técnicas baseadas em LLM, enfocando suas respectivas fortes e fraquezas em um contexto mais generalizável. Diferentemente de métodos como o KeyBERT, que são intensivos em recursos, o USKE fornece uma alternativa aberta e leve que é tanto versátil quanto eficiente ao integrar técnicas estatísticas em uma pipeline de processamento de linguagem natural otimizada, demonstrando que métodos tradicionais ainda podem produzir resultados competitivos, especialmente quando usados em conjunto com expertise de domínio. Este estudo demonstra a eficácia do nosso método proposto em conjuntos de dados de resenhas e títulos da língua portuguesa, confirmando que métodos tradicionais ainda podem obter resultados competitivos para essa tarefa específica e podem obter resultados satisfatórios quando usados ao lado de especialistas para fazer anotações que servem de entrada para o algoritmo. Este trabalho está organizado da seguinte maneira. A Seção 2 fornece uma revisão abrangente dos trabalhos relacionados, destacando o estado da arte atual em extração de palavras-chave não supervisionada. A Seção 3 apresenta o nosso método proposto, detalhando os algoritmos e técnicas utilizados. A Seção 4 descreve o setup experimental, incluindo os conjuntos de dados e o processo de ajuste de prompt. A Seção 5 discute os resultados e fornece uma análise do desempenho do nosso método quando comparado ao LLM. Finalmente, a Seção 6 conclui o artigo com um resumo dos nossos achados e destaca direções potenciais para pesquisas futuras. 1 Disponível em: https://github.com/fernandozagatti/Unsupervised-Statistical-Keyword-Extraction Pipeline de Extração de Palavras-Chave Estatística Não Supervisionada 3 2 Trabalhos Relacionados Nos últimos anos, a extração de palavras-chave tem ganhado atenção significativa no campo da NLP, levando ao desenvolvimento de várias metodologias que buscam melhorar a precisão e a eficiência desse task [21,12,7].
Esses métodos, embora eficazes, frequentemente requerem pré-processamento extenso e o uso de recursos externos, o que pode limitar sua aplicabilidade em linguagens e domínios diversificados. O KeyBERT [7] é um método de extração de palavras-chave que utiliza modelos baseados em transformers para identificar as palavras-chave mais relevantes dentro de um texto. Dessa forma, as embeddings do BERT são aplicadas no documento e as embeddings de palavras são extraídas em N-grams; em seguida, a similaridade de coseno é usada para encontrar as palavras mais relevantes (ou palavras-chave) no documento. Apesar de suas vantagens, o KeyBERT tem algumas limitações: (i) sua dependência de grandes conjuntos de dados, que podem não desempenhar bem em textos de certos domínios ou em linguagens menos representadas nos dados de treinamento, e (ii) o método pode ser computacionalmente caro, pois requer a geração de embeddings tanto para o documento quanto para numerous candidatas a palavras-chave, tornando-o menos viável para aplicações em tempo real ou de grande escala. O YAKE (Mais um Extrator de Palavras-Chave) [2], ao contrário dos modelos de aprendizado de máquina tradicionais, não requer uma fase de treinamento, tornando-se uma solução versátil e escalável que opera em documentos individuais, permitindo uma adaptação rápida a vários cenários sem a necessidade de ferramentas linguísticas ou bases de dados externas. Esse método identifica palavras-chave relevantes com base em características estatísticas, como analisando a frequência e a co-ocorrência de sequências de palavras dentro de um documento, oferecendo uma solução independente de linguagem e domínio. A eficácia do YAKE foi demonstrada através de testes extensivos em vários conjuntos de dados e linguagens [18,1]. No entanto, é importante destacar que maior é o conjunto de dados analisado e maior é o número de palavras-chave a serem extraídas, maior é o tempo necessário para obter as palavras-chave. Gupta et al. (2024) [8] fizeram uma integração do BERT [4] e do YAKE para analisar relatórios de sustentabilidade de empresas de tecnologia da Índia.
O estudo utiliza a tokenização e embedding do BERT para capturar as nuances contextuais e semânticas do texto, entendendo a importância de cada palavra no contexto mais amplo do documento, enquanto o YAKE é empregado para extrair palavras-chave do texto. O estudo reconhece algumas limitações e áreas para melhoria futura, especificando que o estudo apenas utiliza relatórios de sustentabilidade de organizações de tecnologia da informação, reforçando que para melhorar a generalizabilidade, devem ser incluídos relatórios de setores adicionais. Além disso, o estudo analisa relatórios em um ponto específico do tempo, enquanto um estudo em série temporal poderia fornecer insights sobre como as práticas de sustentabilidade evoluem ao longo do tempo. Com o objetivo de realizar uma análise comparativa, Nadim et al. (2023) [13] avalia o desempenho de ferramentas de extração de palavras-chave baseadas em estatística, gráficos e embedding, destacando suas fortes e fracos em termos de desempenho e tempo de execução. Dessa forma, o KeyBERT alcançou um melhor desempenho de F1-Score em três dos quatro conjuntos de dados, embora tenha tempos de execução mais longos para textos mais longos, enquanto o RAKE [21] e o YAKE são destacados por seus tempos de execução rápidos. Entre todos os trabalhos, foram vistos vários ferramentas de extração de palavras-chave e alguns benchmarks entre as ferramentas. No entanto, não foram encontrados estudos que buscam comparar pipelines clássicos e abordagens novas com LLMs. Portanto, nosso trabalho busca realizar uma comparação entre abordagens clássicas e LLM, ao mesmo tempo em que fornece uma ferramenta de código aberto para extrair palavras-chave de forma simples e rápida usando métodos estatísticos. 3 Extração de Palavras-Chave Estatística Não Supervisionada Propomos um sistema, denominado USKE, que emprega uma pipeline não supervisionada com técnicas estatísticas para identificar e extrair palavras-chave relevantes no texto.
Essa seção descreve como o sistema proposto foi desenvolvido, as técnicas utilizadas e a relação entre seus passos, como ilustrado pela Figura 1. Tokenização Transformação para minúsculas Remoção de caracteres especiais Pipeline de pré-processamento 1 Remoção de palavras-chave (opcional) Frequência de termos - Frequência de documentos inversa (tfidf) Extrator de palavras-chave Yet Another Keyword Extractor (yake) Informação mutua pontual (pmi) Identificação de palavras-chave candidatas 2 Estimativa de máximo verossimilhança (mle) Coeficiente de Sørensen-Dice (dice) Contagem de N-grams Filtro de tokens Geração de combinações Extração de palavras-chave 3 Filtro de candidatos Fig. 1. Pipeline do sistema USKE Como pode ser visto, há 3 estágios principais para o sistema, nomeadamente: (i) pipeline de pré-processamento, (ii) identificação de palavras-chave candidatas e (iii) extração de palavras-chave. 3.1 Pipeline de pré-processamento A etapa de pré-processamento visa preparar e limpar o texto para o uso de técnicas de processamento de linguagem natural e extração de palavras-chave. Nessa etapa, as seguintes técnicas são aplicadas: (i) transformação para minúsculas do documento, (ii) remoção de todos os caracteres especiais (por exemplo, @, #, $) e padronização da acentuação das palavras (por exemplo, á, â, ã -> a), (iii) remoção de palavras-chave (opcional) e (iv) tokenização simples por divisão. Extrator de Palavras-Chave Estatístico Não Supervisionado Pipeline 5 3.2 Identificação de palavras-chave candidatas Essa etapa realiza a identificação de palavras-chave relevantes para o corpus empregando métodos estatísticos, nomeadamente: YAKE (Extrator de Palavras-Chave Yet Another), TF-IDF (Frequência de Termos - Frequência de Documentos Inversa), PMI (Informação Mutua Pontual), Coeficiente de Dice, MLE (Estimativa de Máximo Verossimilhança) e contagem de N-grams.
Aqui está a tradução do texto científico para português do Brasil:

Cada abordagem será detalhada abaixo: – YAKE2: este método extrai palavras-chave ou frases-chave de documentos de texto pré-processando e identificando candidatos com base na frequência de termos (TF) dentro do documento, frequência inversa de documento (IDF) em um corpus maior, padrões de co-ocorrência de palavras, posição do candidato dentro do documento e comprimento da frase do candidato. Os candidatos são pontuados e classificados usando uma combinação ponderada dessas características, com os candidatos mais pontuados sendo selecionados como as palavras-chave ou frases-chave mais relevantes. – TF-IDF: identifica palavras que são frequentemente ocorrentes dentro de um documento específico e relativamente raras em todo o corpus. Dessa forma, essa abordagem atribui um peso numérico a cada palavra e realiza uma classificação com base nesses scores. tf-idf(t, d) = tf(t, d) × idf(t) (1) idf(t) = log  (1 + n) (1 + df(t))  + 1 (2) onde: • tf-idf(t, d): é a “Frequência de Termo-Inversa de Documento Frequência” da palavra t no documento d. • idf(t): é a “Frequência Inversa de Documento” que mede como comum uma palavra (t) é em todos os documentos. • tf(t, d): computa a “frequência de termo” da palavra t em um documento d como o número de vezes que t aparece em d. • n: é o total de documentos. • df(t): é o número de documentos em que a palavra t aparece. – PMI: é uma medida estatística utilizada para determinar a associação ou co-ocorrência entre dois eventos (ou palavras) dentro de um corpus de texto. Um alto score PMI indica uma forte associação, sugerindo que a ocorrência de uma palavra faz a presença da outra mais provável, enquanto um score baixo ou negativo sugere independência ou repulsão; sua cálculo pode ser visualizado através das Equações 3, Equações 4 e Equações 5, PMI(w1 ... wn) = log2 P(w1 ... wn) Qn i=1 P(wi)  (3) P(wi) = Contagem(wi) totalContagemPalavras (4)
wn) contagemTotalDePalavras (5) onde: • P(wi) é a probabilidade individual de cada palavra; • P(w1 ... wn) é a probabilidade conjunta do n-grama; • n é o número de palavras da palavra-chave. • Contagem(wi) é o número de vezes que a palavra aparece no texto; • Contagem(w1 ... wn) é o número de vezes que o n-grama aparece no texto; • contagemTotalDePalavras é o número total de palavras no texto; – Dice: essa é uma medida utilizada para quantificar o grau de sobreposição ou similaridade entre dois conjuntos. O score varia de 0 a 1, com valores mais altos indicando maior similaridade. Dice(w1 ... wn) = n ∗ Contagem(w1 ... wn) Pn i=1 Contagem(wi) (6) – MLE: essa é uma técnica para estimar a distribuição de probabilidade de palavras (ou frases) em um corpus, supondo que a probabilidade de uma palavra ocorrer em um contexto específico é proporcional ao número de vezes que essa palavra aparece no treinamento do corpus. MLE(w1 ... wn) = ( Contagem(w1) contagemTotalDePalavras, se n = 1 Contagem(w1...wn) Contagem(w1...wn-1), caso contrário (7) – contagem de n-gramas: essas são sequências contínuas de n itens, que podem ser caracteres, palavras ou unidades mais amplas como frases. Nesse contexto, é verificado quantas vezes cada n-grama aparece no texto. 3.3 Extração de palavras-chave Esta etapa extrai palavras-chave do texto com base em uma lista de palavras-chave candidatas identificadas na etapa anterior (ou uma lista de palavras-chave fornecida pelo usuário). Para ilustrar o processo, vamos demonstrar os outputs de cada função usando o exemplo de busca por ['fritadeira elétrica', 'fritadeira profunda'] no texto "EGGKITPO Fritadeira Elétrica Profunda". Esta etapa envolve as seguintes funções: – Filtro de tokens: nessa função, o texto é separado em tokens e cada token é verificado se está presente na lista de palavras-chave identificadas. Essa função retorna uma lista contendo as palavras que passaram pelo filtro.
• Após filtro: [‘elétrico’, ‘profundo’, ‘fritadeira’] – Gere combinações: nessa função, a partir da lista gerada pelo passo anterior, uma nova lista ordenada (por ordem de aparição no texto de entrada) é gerada com todas as combinações possíveis de palavras. Esse passo é importante quando os candidatos a palavras-chave têm 2 ou mais tokens.

• Após combinações, considerando até 2 tokens: [‘elétrico’, ‘profundo’, ‘fritadeira’, ‘elétrico profundo’, ‘elétrico fritadeira’, ‘profundo fritadeira’]

Pipelina de Extração de Palavras-Chave Estatística Não-Supervisionada – Filtro de candidatos: essa função seleciona apenas as combinações de palavras que estão presentes na lista de palavras-chave identificadas.

• Após novo filtro: [‘elétrico fritadeira’, ‘profundo fritadeira’]

Essa separação em tokens e geração de combinações em vez de simples busca por sequência de palavras é muito útil para identificar palavras-chave que não são encontradas diretamente no texto. A Tabela 1 exemplifica esse problema.

Tabela 1. Exemplo de eficiência de combinação de tokens versus busca direta por palavra-chave

* Nome do produto Busca direta Nossa método EGGKITPO Elétrico Profundo Fritadeira Tanque Único com Cesto Capacidade 10L (10,5QT) Elétricos de Fritadeira Conta-Altura com Timer de 60 Minutos para Cozinha para Casa e Restaurante [‘fritadeira profunda’] [‘elétrico fritadeira’, ‘fritadeira profunda’]

* Exemplo retirado da Amazon. Disponível em: https://a.co/d/dknbFTX

7 Configuração experimental Os experimentos foram realizados com um processador Intel Core i7-10700K, uma Unidade de Processamento Gráfico de Alta Performance (GPU) NVIDIA GeForce RTX 4070 Ti e 32 GB de Memória de Acesso Aleatório (RAM). Comparamos o desempenho da USKE, KeyBERT e 5 LLMs disponíveis na HuggingFace3 para avaliar o desempenho de alguns modelos de linguagem, que podem ser facilmente encontrados, com algumas pipelines clássicas. Os modelos foram selecionados com base nos modelos mais populares da HuggingFace, selecionando modelos de geração textual que usam inglês e/ou português e que permitam seu uso com a API LangChain.
Entre os modelos selecionados, exceto o Phi-Bode, todos foram treinados com a maioria dos dados em inglês e alguns em português. Os modelos selecionados são descritos abaixo: – Vlt5-base-keywords4: um modelo de geração de palavras-chave baseado na arquitetura encoder-decodificador apresentada pela Google. Foi treinado em um corpus composto por artigos científicos, para prever um conjunto específico de frases de palavras-chave que descrevam o conteúdo do artigo com base apenas no resumo [15]. Entre os modelos selecionados, este é o único construído especificamente para extração de palavras-chave. – Flan-t5-xxl5: um modelo da Google treinado com 540B-parâmetros, realizando fine-tuning com a adição de mais de 1000 novas tarefas e cobrindo mais línguas do que seu predecessor, T5 [17,3]. Pode gerar texto coerente e contextualmente relevante com base em um prompt dado, tornando-o útil para escrita criativa, chatbots e criação de conteúdo. – Bloom6: um modelo treinado para continuar o texto a partir de um prompt, capaz de produzir texto em 46 línguas e 13 línguas de programação. Este modelo busca ajudar com geração de texto e código, e exploração de características. – Blenderbot-400M-distill7: uma versão leve e distilada do BlenderBot 2.7B da Facebook, projetada para inteligência artificial conversacional de domínio aberto. É baseado em um modelo de transformador de 400 milhões de parâmetros que foi fine-tunado em conjuntos de dados conversacionais diversificados para melhorar sua capacidade de engajar em diálogo multi-virada [20]. – Phi-Bode8: um modelo de linguagem fine-tunado a partir do modelo base Phi-2B da Microsoft, refinado usando o conjunto de dados Alpaca traduzido para fine-tuning, com o objetivo de adaptar o modelo às nuances do idioma português [14]. Entre os modelos selecionados, este é o único treinado especificamente para o português.
1 Conjuntos de dados Dois conjuntos de dados compostos por revisões de produtos ou serviços foram selecionados para realizar os testes, nomeadamente: (i) B2W-Reviews01 [19] e (ii) Análise de Sentimento com Base em Aspectos em Português (ABSAPT) 2022. Os conjuntos de dados são descritos abaixo: – B2W-Reviews01: este conjunto de dados constitui um corpus aberto de revisões de produtos, compreendendo mais de 130 mil revisões de clientes de comércio eletrônico, coletadas do site americanas.com entre janeiro e maio de 2018. Este corpus fornece títulos de produtos em português; no entanto, não há anotações especializadas, o que pode tornar certos tarefas difíceis. – ABSAPT 2022: este conjunto de dados contém um corpus de revisões sobre empresas de serviços de hospedagem escritas em português. Os especialistas anotaram os aspectos e suas respectivas polaridades, totalizando mais de 800 revisões anotadas. Os aspectos foram usados como palavras-chave alvo nos experimentos descritos nesse artigo. É importante mencionar que, devido aos desafios associados a fazer inferências com LLMs em conjuntos de dados extensos, reduzimos o tamanho dos conjuntos de dados para nossos experimentos: para o B2W-Reviews01, apenas as primeiras 100 instâncias foram usadas; enquanto para o ABSAPT 2022, usamos o conjunto de dados de teste, que compreende 184 instâncias.

4.2 Ajuste de prompts O ajuste de prompts apenas atualiza os valores de entrada dos prompts enquanto os parâmetros do LLM permanecem inalterados. Este método de ajuste permite que um guie os modelos para produzirem saídas mais relevantes e precisas sem necessariamente ter que reentrená-los [11].

6 Disponível em: https://huggingface.co/bigscience/bloom
7 Disponível em: https://huggingface.co/facebook/blenderbot-400M-distill
8 Disponível em: https://huggingface.co/recogna-nlp/Phi-Bode

Pipelina de Extração de Palavras-Chave Estatística Não-Supervisionada

9 Nesse estudo, experimentamos com algumas estruturas de prompts (Tabelas 2, 3 e 4) para avaliar sua eficácia em diferentes LLMs. A seguir, detalhamos o processo de ajuste e fornecemos exemplos dos prompts testados junto com seus resultados.
Observe que todos os prompts possuem {text}, que é uma variável onde o texto do qual queremos extrair as palavras-chave será solicitado. Para os resultados de exemplo, vamos usar o seguinte {text}: Copo Acrílico Com Canudo 500ml Rocie. Tabela 2. Prompt #1 Com base na seguinte frase, retorne uma lista das palavras mais relevantes: {text} Vlt5-base-keywords Copo Acrílico, Copo Acrílico, Copo Acríli Flan-t5-xxl acilico, canudo, cie, copo Bloom N1056 | Copo com Canudo | Copo com Canudo 500ml | Copo com Canudo Acrílico | Copo com Canudo Acrílico 500ml | Copo com Canudo Acrílico Rocie N1056 Blenderbot-400M-distill Nunca ouvi falar disso. O que é sobre? É um livro? Phi-Bode # Crie uma lista de palavras relevantes palavras_relevantes = [\’Copo Acrílico\’, \’Com\’, \’Canudo\’, \’500ml\’, \’Rocie\’] # Retorne a lista return palavras_relevantes # Teste print(get_relevant_words("Copo Acrílico Com Canudo 500ml Rocie") De Prompt #1, vemos que Vlt5-base-keywords e Flan-t5-xxl foram capazes de extrair palavras de forma mais simples, enquanto Bloom teve alguma dificuldade e Blenderbot parece não ter entendido nossa solicitação. Além disso, é possível notar que todos os modelos geraram palavras que não existiam no texto de entrada, como "Acríli", "cie" e "N1056". Tabela 3. Prompt #2 Pergunta: Com base na seguinte frase, retorne uma lista das palavras mais relevantes: {text} Resposta: Vlt5-base-keywords Copo Acrílico, Copo Acrílico, Copo Acríli Flan-t5-xxl acilico, canudo, com, rocie Bloom Copo Acrílico Com Canudo 500ml Rocie, [...] Blenderbot-400M-distill Olá, como você está hoje? Estou bem, obrigado por perguntar. Phi-Bode Copo Acrílico, Canudo, 500ml, Rocie. Depois de alguns testes, foi encontrado que adicionando "Pergunta:" e "Resposta:" no início e no final do prompt ajudou alguns modelos a gerar respostas mais concisas (Tabela 3). Bloom, nesse caso, apenas replicou o texto de entrada, mas não respondeu corretamente à nossa solicitação de gerar as palavras-chave encontradas no texto.
Finalmente, a Bloom é um LLM que continua textos a partir da prompt, acrescentando-se "A lista é" na parte "Resposta" (Tabela 4), o que ajudou a obter respostas mais assertivas e não afetou as respostas dos outros modelos. Portanto, a Prompt #3 foi escolhida para os experimentos. Para experimentos que contenham listas personalizadas, a Prompt #3 foi usada, mudando "Retorne uma lista das palavras mais relevantes com base na seguinte frase:" para "Retorne apenas a lista de palavras que estão na seguinte sentença, com base nessa lista de palavras {lista}:". Como o modelo Phi-Bode foi treinado com textos em português, versões das mesmas prompts foram usadas, mas traduzidas para português.

10 F. Zagatti et al. Tabela 4. Prompt #3 Pergunta: Com base na seguinte frase, retorne uma lista das palavras mais relevantes: {texto} Resposta: A lista é Vlt5-base-keywords Copo Acrílico, Copo Acrílico, Copo Acrílico Flan-t5-xxl acilico, canudo, com, rocie Bloom Rocie, 500ml, Com, Canudo, Acrílico, Copo, Rocie, Copo, Acrílico, Com, Canudo, 500ml Blenderbot-400M-distill Você sabia que a palavra "copo" significa "pequeno armado" em espanhol? Phi-Bode 1. Copo Acrílico 2. Com Canudo 3. 500ml 4. Rocie 4.3 LLMs pós-processamento Os outputs gerados pelos LLMs não aderiram consistentemente a um formato padrão nas listas de palavras-chave de referência utilizadas nos nossos experimentos, requerendo passos de pós-processamento adicionais. Os outputs dos LLMs frequentemente continham caracteres especiais, duplicação de palavras e falta de separação consistente de tokens, o que requereu limpeza minuciosa antes que qualquer avaliação automática pudesse ser realizada. A Tabela 5 traz um exemplo do resultado obtido ao buscar palavras-chave para o texto de entrada {texto}. Tabela 5. Exemplo de outputs dos LLMs para o texto de entrada {texto} {texto} A localização do hotel é muito boa, porém as partes comuns são pequenas e desconfortáveis para uma classificação 4 estrelas !!!!! O valor da diária não compensa o desconforto das instalações comuns.
As salas de recepção e de café da manhã não refletem um conforto de hotel de 4 estrelas da rede Accor. A equipe de atendimento é gentil e atenciosa. Vlt5-base-keywords Accor, Accor, Accor, Accor, Accor, Accor, Accor, Accor, Accor, Accor, Accor, hotel 4 estrelas Flan-t5-xxl localização, parte, estrela Bloom : localização, hotel, pequena, comum, classificação, estrelas, diárias, compensa, desconforto, instalações, salas, recepção, café, manhã, equipe, atendimento, gentil, atenciosa Blenderbot-400M-distill Você fala alguma língua além do inglês? Eu falo espanhol, francês e português. Phi-Bode Palavras mais relevantes: localização, boa, partes comuns, desconfortáveis, classificação, equipe de atendimento, salas de recepção, café, diário, valor, atenciosa. Como os outputs não seguiram um padrão bem conhecido, removemos qualquer caractere especial dos outputs para evitar que interferissem no processo de métrica de avaliação. Por exemplo, outputs como “: localização, café-da-manhã, diárias” foram limpos para remover “:”, “-” e acentos, resultando em “localizacao cafe da manha diarias”. Este passo foi realizado para evitar desacordos entre as avaliações, onde a palavra “localização” poderia ser considerada diferente de “localizacao”. Após a limpeza, separamos o output em tokens individuais. Este passo é importante para transformar dados textuais (strings) em listas com as palavras contidas nos textos, permitindo comparações palavra por palavra com as anotações dos especialistas. Pipeline de Extração de Palavras-chave Estatística Não Supervisionada 11 Finalmente, removemos todos os tokens duplicados das listas para garantir que cada palavra-chave fosse única. Por exemplo, se uma lista fosse “[“localizacao”, “hotel”, “hotel”, “cafe”]”, o duplicado “hotel” foi removido, resultando em “[“localizacao”, “hotel”, “cafe”]”. Este processo de deduplicação foi necessário para refletir acuradamente a diversidade de palavras-chave geradas pelas LLMs.
Essas etapas de pós-processamento foram críticas para preparar os outputs do LLM para comparação justa e precisa com os dados anotados. Elas garantiram que os métricas de avaliação fossem baseadas em outputs limpos e padronizados, fornecendo uma avaliação mais confiável do desempenho dos LLMs para essa tarefa específica.

4.4 Métricas de avaliação

A avaliação dos outputs foi feita com base nas métricas comumente usadas na literatura: precisão, recall e F1-score. A precisão é a proporção de instâncias classificadas como positivas que são realmente positivas. O recall é a proporção de instâncias positivas reais que são corretamente classificadas como positivas. O F1-score é a média harmônica entre precisão e recall, fornecendo uma medida balanceada entre as duas métricas. Nossos experimentos também consideraram dois medidas de similaridade: similaridade de Cosine e similaridade de Jaccard. A similaridade é uma medida quantitativa usada para determinar o grau de similaridade entre objetos, e é frequentemente usada para comparar dois vetores distintos. Essas métricas são fundamentais em recuperação de informações, processamento de linguagem natural e aprendizado de máquina, onde identificar padrões e benchmarking são essenciais. A similaridade de Cosine calcula o cosseno do ângulo entre dois vetores. Quanto mais próximo o valor de cosseno for de 1, maior a similaridade entre as entidades. A similaridade de Jaccard é definida como a razão entre o tamanho da intersecção dos conjuntos e o tamanho da união dos conjuntos, variando de 0 a 1, onde 1 indica que os conjuntos são idênticos e 0 indica que não há elementos em comum.

5 Resultados e análise

Aplicamos os 5 modelos de LLM apresentados anteriormente aos 2 conjuntos de dados e os comparamos com nossa ferramenta – para USKE, os outputs de cada técnica foram mesclados para avaliar a ferramenta de forma abrangente. Os resultados foram avaliados de forma diferente, pois o conjunto de dados ABSAPT 2022 tinha uma anotação prévia, permitindo análise direta, enquanto B2W-Reviews01 não.
Resultados do ABSAPT 2022 Este conjunto de dados tem uma anotação prévia de aspectos para a tarefa de Análise de Sentimento com Base em Aspectos (ABSA) e esses aspectos foram considerados nossas palavras-chave de referência. Portanto, dois testes específicos foram aplicados: (i) extração de palavras-chave sem a lista de palavras anotadas e (ii) extração direta de palavras-chave das anotações. A Tabela 5.1 apresenta os resultados do primeiro caso. Nesse conjunto de dados específico, desde que 12 palavras-chave estavam disponíveis, os métricas de Similaridade de Jaccard e Cosine foram aplicadas diretamente aos dados anotados para avaliar a similaridade entre os outputs do modelo e os dados anotados. Os resultados demonstram que o USKE obteve uma precisão próxima aos modelos Vlt5-base e Flan-t5 e foi o terceiro melhor método em recall e F1-Score. Por outro lado, o Blenderbot não obteve resultados significativos, pois foi treinado com um objetivo conversacional e não pode gerar respostas diretas para essa tarefa específica, embora o KeyBERT alcance valores de similaridade consideráveis, não tem resultados competitivos em termos de precisão e recall. Além disso, o Phi-Bode e o Bloom obtiveram resultados significativamente melhores em precisão, recall e F1-score, mas os valores de similaridade foram baixos. Isso ocorreu porque os LLMs geraram mais textos do que esperado, reduzindo a similaridade, mas desde que a maioria do tempo os outputs esperados estavam entre o texto gerado, isso levou a uma precisão e recall aumentadas.

Tabela 6. ABSAPT 2022 sem lista de palavras-chave personalizada

Modelo Precisão Recall F1-Score Jaccard Cosine Tempo (segundos)

USKE (nós) 0,16 0,18 0,17 0,18 0,35 0,45s

Vlt5-base 0,17 0,05 0,07 0,14 0,24 408,41s

Flan-t5 0,21 0,08 0,10 0,20 0,31 106,81s

Bloom 0,47 0,36 0,38 0,05 0,13 3561,53s

Blenderbot 0,03 0,00 0,00 0,00 0,00 662,65s

Phi-Bode 0,46 0,38 0,40 0,06 0,16 4896,83s

KeyBERT 0,09 0,10 0,09 0,19 0,33 2,32s
Em contraste, a Tabela 7 apresenta os resultados do teste com a lista de palavras que desejamos extrair do texto, onde avaliamos os resultados da mesma forma que na tabela anterior. Tabela 7. ABSAPT 2022 com lista customizada Modelo Precisão Recall F1-Score Jaccard Cosine Tempo (segundos) USKE (nossos) 0,70 0,84 0,74 0,72 0,83 0,03s Vlt5-base 0,19 0,10 0,11 0,18 0,30 1467,17s Flan-t5 0,12 0,31 0,12 0,17 0,33 745,02s Bloom 0,06 0,69 0,09 0,07 0,20 4537,33s Blenderbot 0,00 0,00 0,00 0,00 0,00 595,72s Phi-Bode 0,08 0,20 0,06 0,08 0,21 6169,38s KeyBERT N/A N/A N/A N/A N/A N/A Como nossa pipeline foi desenvolvida para extrair palavras-chave diretamente do texto, os experimentos com lista pré-definida levaram a valores altos em todos os métricas avaliadas. Os modelos LLM não foram capazes de extrair consistentemente as palavras requeridas. Embora apenas os modelos Vlt5-base e Flan-t5 tenham obtido um F1-Score ligeiramente mais alto do que o teste anterior, todos os modelos (exceto Blenderbot e Phi-Bode) obtiveram resultados superiores em recall. No entanto, é importante mencionar que os valores de precisão diminuíram em todos os casos, e o tempo de execução também aumentou. Finalmente, o KeyBERT foi desenvolvido apenas para encontrar palavras-chave de forma não supervisionada, e não é capaz de lidar com uma lista pré-definida. Além disso, o tempo de processamento tornou-se mais longo em todos os LLMs (exceto Blenderbot), pois as prompts foram maiores devido às listas enviadas junto com os textos. Pipeline de Extração de Palavras-Chave Estatística Não Supervisionada 13 5.2 Resultados B2W-Reviews01 Embora esse conjunto de dados não tenha anotações prévias, é possível avaliar se os resultados são significativos usando testes de similaridade. Como a tarefa é buscar palavras explícitas no texto, as respostas dos modelos precisam ter algum grau de similaridade com o texto de entrada. Dessa forma, se a similaridade for igual ou próxima a 0, é possível dizer que os modelos de linguagem não foram capazes de atender à solicitação. Os resultados podem ser vistos na Tabela 8. Tabela 8.
B2W-Reviews01 resultados Modelo Jaccard Cosine Tempo (segundos) USKE (nossos) 0,38 0,55 0,31s Vlt5-base 0,49 0,65 72,13s Flan-t5 0,30 0,50 71,66s Bloom 0,52 0,62 2090,01s Blenderbot 0,02 0,03 369,70s Phi-Bode 0,69 0,81 1390,15s KeyBERT 0,37 0,59 1,81s O Phi-Bode obteve os valores de similaridade mais altos, demonstrando que seus resultados estavam mais frequentemente relacionados ao texto de entrada. O USKE também alcançou escores respeitáveis com uma similaridade de Jaccard de 0,38 e uma similaridade de Cosine de 0,55, mantendo-se à média das LLMs. O KeyBERT alcançou valores de similaridade muito próximos do USKE. Em termos de tempo de processamento, o USKE superou os outros, concluindo a tarefa em apenas 0,31 segundos, o que é significativamente mais rápido do que as LLMs. O KeyBERT também teve um tempo de processamento rápido de 1,81 segundos. No entanto, o Phi-Bode teve um dos tempos de processamento mais longos, alcançando 1390,15 segundos.

5.3 Análise crítica A análise revela insígnias críticas sobre o desempenho de vários modelos em tarefas de extração de palavras-chave, particularmente enfatizando a eficácia dos modelos treinados em português, a possibilidade de ajuste de prompts e a tendência a gerar respostas que não se adequarem à solicitação e ao texto de entrada. O Phi-Bode, treinado especificamente em português, demonstrou resultados mais satisfatórios em métodos sem listas pré-definidas. Esse modelo alcançou os valores de similaridade mais altos, destacando a importância de treinar modelos na língua específica do conjunto de dados para melhorar o desempenho em tarefas específicas da língua. Além disso, a possibilidade de melhorar os prompts pode levar a respostas melhores – o ajuste de prompts deve ser realizado com cuidado, pois mesmo pequenas variações e ajustes podem levar a grandes diferenças no desempenho. Experimentos com estruturas de prompts diferentes podem impactar significativamente a eficácia das LLMs. Por fim, reforçamos que alguns modelos, como o Blenderbot, não são ideais para atividades de extração de palavras-chave devido à sua otimização para tarefas conversacionais.
Seu desempenho, com valores de similaridade baixos e tempos de processamento longos, destaca essa limitação. Prevenir que os modelos gerem informações irrelevante ou incorreta permanece um desafio, enfatizando a necessidade de seleção e otimização cuidadosas do modelo para aplicações específicas. 14 F. Zagatti et al. Nesse sentido, demonstramos que o uso de pipelines mais rigorosos ainda é extremamente útil para desenvolvedores e pode salvar muito tempo escolhendo modelos, ajustando prompts e pós-processamento. 6 Conclusões e trabalho futuro Nossa pesquisa revela que tanto o pipeline proposto quanto os LLMs têm vantagens e desvantagens distintas para extração de palavras-chave relevantes em dados textuais. O USKE demonstrou vantagens significativas em velocidade de processamento e simplicidade, especialmente quando lidando com grandes conjuntos de dados com recursos computacionais mínimos. Além disso, como é um pipeline tradicional e mais controlado, os resultados sempre estarão relacionados à entrada e garantirão seguir o formato desejado. Pipelines de extração de palavras-chave tradicionais podem ser eficientes, mas a falta de um contexto mais amplo (sentenças múltiplas) e pré-processamento mal otimizado pode levar a palavras-chave menos precisas serem extraídas, comprometendo a qualidade dos resultados. Por outro lado, os LLMs podem lidar com sentenças únicas ou parágrafos pequenos eficazmente sem precisar de um contexto profundo, mas têm dificuldade em manter o desempenho em ambientes que requerem lidar com um fluxo alto de dados, demandando poder computacional substancial e hardware especializado (por exemplo, GPUs). Portanto, se os LLMs são necessários depende das exigências e restrições específicas da aplicação. Em trabalho futuro, poderíamos realizar um benchmarking qualitativo mais aprofundado, onde especialistas poderiam avaliar e comparar os resultados gerados pelos métodos diferentes.
Este tipo de análise permitiria uma compreensão mais profunda da relevância e precisão das palavras-chave extraídas, bem como forneceria insights valiosos que não podem ser capturados por métricas quantitativas sozinhas. Além disso, explorar a integração de técnicas de processamento de linguagem natural mais avançadas, como embeddings de palavras contextualizadas, com experimentos em conjuntos de dados de outros domínios, poderia potencialmente aumentar a qualidade das extrações de palavras-chave. Agradecimentos. Esta pesquisa foi financiada em parte pela Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Código de Financiamento 001.