ptt5-v2: Um Olhar Mais Próximo ao Treinamento Continuado de Modelos T5 para a Língua Portuguesa

Marcos Piau1[0009−0001−1490−3476], Roberto Lotufo1,2[0000−0002−5652−0852], e Rodrigo Nogueira1,3[0000−0002−2600−6035]

1 Escola de Engenharia Elétrica e de Computação, Universidade Estadual de Campinas (UNICAMP), Campinas, Brasil
2 NeuralMind, Brasil
3 Maritaca AI, Brasil

Resumo. Apesar dos avanços na Processamento de Linguagem Natural (PLN) e da crescente disponibilidade de modelos pré-treinados, a língua inglesa permanece o foco primário do desenvolvimento de modelos. O treinamento continuado em corpora específicas da língua fornece uma solução prática para adaptar modelos a outras línguas. No entanto, o impacto de diferentes configurações de treinamento sobre tarefas downstream permanece pouco explorado. Este trabalho apresenta ptt5-v2, investigando o treinamento continuado de modelos T5 para o português. Primeiramente, desenvolvemos um conjunto de configurações de base e treinamos modelos com tamanhos até 3 bilhões de parâmetros. A finetuning em três tarefas downstream portuguesas (ASSIN2 STS, ASSIN2 RTE e TweetSentBR) produz resultados SOTA nos dois últimos. Em seguida, exploramos os efeitos de diferentes configurações de treinamento, incluindo filtros de qualidade, estratégias de otimização e treinamento em múltiplos epochs. Surpreendentemente, seu impacto permanece subtílio em comparação com nossa configuração de base. Lançamos checkpoints pré-treinados ptt5-v2 e rerankers MonoT5 finetunados em HuggingFace em suas respectivas coleções em https://huggingface.co/unicamp-dl.

Palavras-chave: T5 português · Treinamento continuado · Recuperação de Informação
A dinâmica do processo de pré-treinamento foi estudada em profundidade por muitos trabalhos, como Raffel et al. [34], que introduziram o T5 e escalaram modelos para bilhões de parâmetros e estabeleceram novos recordes em muitas tarefas. A tendência em aumentar o tamanho dos modelos e dos conjuntos de dados para melhorar o desempenho motivou estudos como Kaplan et al. [23] sobre leis de escala e Hoffmann et al. [18], que demonstraram a importância do tamanho do conjunto de dados de treinamento em relação ao tamanho do modelo para regimes de treinamento ótimos em termos de computação; mais recentemente, trabalho de Gadre et al. [14] examinou especificamente a influência do pré-treinamento prolongado no desempenho de tarefas downstream. Embora a estudo da dinâmica do pré-treinamento tenha sido extensa, o foco tem sido predominantemente em inglês, deixando línguas não-inglês menos exploradas. O pré-treinamento contínuo apresenta uma abordagem estratégica para adaptar esses modelos a línguas e domínios adicionais usando significativamente menos dados e recursos computacionais do que treinar desde o início. Este método envolve pré-treinar adicionalmente em corpora específicas da língua, o que foi mostrado para melhorar substancialmente o desempenho do modelo em tarefas downstream na língua-alvo [32,6,25,5,42,11]. No entanto, há uma falta de investigações detalhadas sobre como diferentes configurações durante a fase de pré-treinamento contínuo influenciam o desempenho de tarefas downstream, com a maioria dos estudos apenas buscando resultados líderes em benchmark sem uma análise minuciosa dos fatores subjacentes. Neste trabalho, estudamos o pré-treinamento contínuo de modelos T5 para a língua portuguesa, analisando o impacto de várias configurações no desempenho de tarefas downstream. Em vez de apenas focar em alcançar resultados de ponta, nosso estudo também investiga como fatores como tamanho do modelo, escalas de otimização e aplicação de filtros de qualidade no conjunto de dados de pré-treinamento afetam o desempenho. Continuamos o pré-treinamento do modelo T5 da Google com até 3 bilhões de parâmetros em textos portugueses.
Ao experimentar com diferentes configurações na etapa de pré-treinamento, observamos efeitos nuances nos tarefas downstream, com alguns ajustes apenas marginalmente superando os baseline. Nossos achados também sugerem que, embora o pré-treinamento contínuo melhore as capacidades do modelo, os incrementos no desempenho diminuem à medida que o tamanho do modelo aumenta. Os modelos T5 [34] demonstram adaptabilidade em várias tarefas de processamento de linguagem natural (NLP) devido à sua arquitetura encoder-decoder. Esta estrutura permite que eles processem texto tanto para compreensão quanto para geração, fornecendo um advantage sobre modelos apenas encoder como BERT. Embora não seja o foco desse trabalho, a adaptabilidade do T5 para fins-tuning com base em instruções, como visto em FLAN-T5 [8], também permite aplicações zero-shot e few-shot eficazes. Esses fatores, combinados com a escassez de modelos de encoder-decoder pré-treinados em português, motivam nossa escolha de continuar a investigação da arquitetura T5 nesse estudo.

2 Trabalhos Relacionados

O modelo T5 [34] é um transformador encoder-decoder, e uma de suas principais inovações foi transformar todas as tarefas em um formato texto-para-texto, permitindo uma abordagem unificada; escalando os modelos para 11 bilhões de parâmetros, eles consolidaram a abordagem de transferência de aprendizado, estabelecendo novos recordes de desempenho para GLUE [46], SuperGLUE [45], CNN/Daily Mail [17] benchmarks. Foi pré-treinado usando o objetivo de "corrupção de span" sobre o conjunto de dados C4 ("Colossal Clean Crawled Corpus"), onde sequências consecutivas aleatórias no input são substituídas por tokens de máscara especial, e o modelo é treinado para prever esses tokens corrompidos (modelos com até 11 bilhões de parâmetros). Com base nessa fundação, mT5 [48] estendeu o framework T5 para configurações multilíngues, tendo sido pré-treinado no conjunto de dados multilíngue mC4, que cobre 101 línguas (modelos com até 13 bilhões de parâmetros). PTT5 [6] adaptou ainda mais o T5 para o português, continuando o pré-treinamento de modelos T5 no conjunto de dados BrWac [44].
Este abordagem levou a melhorias significativas em tarefas de linguagem portuguesa downstream, que foram ainda mais melhoradas por um tokenizador de linguagem portuguesa. Para clareza, faremos referência ao trabalho de Carmo et al. como ptt5-v1. Outras adaptações internacionais notáveis do T5/mT5 incluem it5 [40] (italiano), AfriTeVa [22] (línguas africanas de baixa recursos), AraT5 [29] (árabe) e plT5 [7] (polonês). Bertimbau [42], uma adaptação popular do modelo de encoder BERT, permanece influente no modelo de linguagem portuguesa. Outros que exploram arquiteturas de encoder incluem Albertina [37], DeBERTinha [5], o trabalho de Gomes et al. [16] (que pré-treina um modelo de Roberta), e de Morais et al. [28]. Refletindo uma tendência mais ampla, vários modelos portugueses recentes priorizam arquiteturas de decoder apenas, como Sabiá [32], Glória [27], Bode [15], Cabrita [25] e Gervásio [39]. No espaço encoder-decoder, o trabalho de Carmo et al. (ptt5-v1) explorou a adaptação de modelos T5 para o português. Além de modelos portugueses genéricos, vários trabalhos se especializam em domínios customizados: de Barros et al. [2] e BERTabaporu [10] foram projetados para dados de mídia social portuguesa, enquanto Bertaú [13] se concentra em linguagem financeira.

3 Metodologia

Esta seção descreve a metodologia para pré-treinar e avaliar nossos principais experimentos, cobrindo o conjunto de dados de pré-treinamento, vocabulário de linguagem específica, arquiteturas de modelo, estratégias de otimização, processos de fine-tuning e validação para tarefas downstream.

3.1 Preparação contínua sem supervisão

Como dados de pré-treinamento, utilizamos a segmentação portuguesa do conjunto de dados mC4 (a partir de agora referido como mC4-pt), compreendendo aproximadamente 524 GB de texto não comprimido em 169 milhões de documentos. Este conjunto de dados é significativamente maior do que o utilizado para pré-treinar modelos ptt5-v1, que originou-se do conjunto de dados BrWac [44] e consistiu em cerca de 15 GB de texto de 7,4 milhões de documentos após pré-processamento. Adotamos o vocabulário de linguagem portuguesa do ptt5-v1.
Este tokenizer de sentença Unigram [24], composto por 32.000 tokens, foi treinado sobre um corpus de 2 milhões de documentos da Wikipedia em português. Essa vocabulária compartilha o mesmo número de tokens e tokens de controle que o T5, facilitando o uso direto dos checkpoints do modelo da Google. Como objetivo de pré-treinamento, foi empregada a tarefa de corrupção de span, utilizando batches de 128 sequências de 512 tokens (65.536 tokens) - uma metodologia consistente com o experimento de base de Raffel et al. [34]. O otimizador Adafactor [41] com uma taxa de aprendizado constante de 0,001 e cross-entropia como perda foi utilizado durante todo o processo de pré-treinamento. Usando esses ajustes experimentais, começamos a partir dos checkpoints originais da Google com tamanhos que variam desde t5-pequeno (60M parâmetros) até t5-3B (3B parâmetros), e realizamos uma época completa de pré-treinamento sobre o conjunto de dados mC4-pt. Considerando esses ajustes, uma época completa sobre o conjunto de dados mC4-pt compreende aproximadamente 1.764.515 passos de treinamento e 116 bilhões de tokens de treinamento. Experimentos de pré-treinamento adicionais estão detalhados na Seção 5.1. Ambos os experimentos de pré-treinamento e fine-tuning utilizaram dispositivos TPUv2-8 e TPUv3-8, aproveitando os frameworks t5 [34] e seqio [36].

3.2 Ajuste supervisionado para tarefas downstream

Avaliamos o impacto do nosso pré-treinamento em três tarefas de linguagem portuguesa downstream: ASSIN2 RTE, ASSIN2 STS e TweetSentBR. O conjunto de dados ASSIN2 [31] fornece duas tarefas: RTE (Reconhecimento de Entailment Textual), que envolve determinar se uma sentença implica outra, e STS (Semântica de Similaridade Textual), que quantifica a similaridade semântica entre pares de sentenças em uma escala de 1 a 5. O conjunto de dados TweetSentBR [4] é uma tarefa de análise de sentimento para tweets em português brasileiro, classificando-os como positivos, negativos ou neutros. As Tabelas 1 e 2 mostram detalhes e exemplos adicionais para cada tarefa.
Conjunto de tarefas Preferido métrica Treinamento/validação/teste Pontuação aleatória Saídas possíveis ASSIN2 RTE Classificação binária F1-macro 6.500/500/2.448 50 {Entailment, None} ASSIN2 STS Regressão Pearson 6.500/500/2.448 0 [1, 5] TweetSentBR Classificação multiclass (3) F1-macro 11.525/1.281/1.982 32,4 {negativo, positivo, neutro} Tabela 1: Conjuntos de tarefas downstream. Conjunto de dados Exemplos de entrada Exemplos de alvo ASSIN2 RTE assin2_rte sentença1: Uma pessoa está escovando um gato sentença2: O pelo de um gato está sendo penteado por uma pessoa Entailment ASSIN2 STS assin2_stsb sentença1: Uma mulher está cortando vegetais sentença2: Uma mulher está cortando brócolis 4,2 TweetSentBR ttsbr_neg_pos_neu_sentiment_pt: adorando esse com dr dráuzio varela positivo Tabela 2: Conjuntos de tarefas de entrada e alvo Finetunamos os modelos pré-treinados por 100 épocas com batches de 128 sequências e um comprimento máximo de 512 tokens, usando Adafactor como otimizador com uma taxa de aprendizado constante de 0,001. O checkpoint do modelo que apresentou o melhor desempenho no conjunto de validação foi selecionado para testes, e a decodificação greedy foi utilizada como método de decodificação. Porque TweetSentBR não tem um conjunto de validação, reservamos 10% dos dados de treinamento para validação e usamos o restante de 90% para treinamento. Todas as tarefas foram abordadas usando um formato de texto-para-texto. Especificamente para a tarefa ASSIN2 STS, que envolve a previsão de valores contínuos no intervalo entre 1 e 5, adotamos a estratégia de Raffel et al. [34], arredondando os scores de alvo para o incremento mais próximo de 0,2 e convertendo esses para strings, o que a configura como um problema de classificação multiclass compatível com o formato de texto-para-texto. ptt5-v2 5 Para comparar a qualidade dos novos checkpoints com alternativas existentes, também utilizamos o mesmo procedimento de finetuning nos modelos Google T5 e mT5. 3.
3 MonoPTT5 Rerankers Para avaliar a adaptação dos modelos ptt5-v2 para tarefas de recuperação de informação, especificamente para reranking de passagens, treinamos rerankers MonoT5 [30] utilizando checkpoints gerados como descrito na Seção 3.1. Chamamos esses modelos de MonoPTT5. Os rerankers MonoT5 são usados para reranking de passagens, um processo em duas etapas: primeiro, um método menos computacionalmente oneroso como BM25 gera um conjunto inicial de documentos relevantes para uma dada consulta; o modelo de reranker então reranka um subconjunto desses documentos para melhorar a ordenação de relevância. Durante o treinamento, o modelo aprende de forma supervisionada em texto para texto para gerar tokens correspondentes a rótulos relevantes e não relevantes. Para inferência, decodamos de forma avarice um token único e calculamos o softmax sobre os logits dos dois tokens possíveis, usando a probabilidade da classe positiva como a pontuação de relevância. Adaptamos o formato de entrada e alvo para o idioma português à estrutura "Pergunta: {query} Documento: {document} Relevante:", atribuindo os tokens "Sim" (relevante) e "Não" (não relevante). Esse formato é aplicado durante tanto o treinamento quanto a inferência, independentemente do idioma de entrada. O conjunto de treinamento originou-se do conjunto de dados mMARCO [3], uma versão traduzida do conjunto de dados de recuperação de passagens MS MARCO [1], originalmente em inglês, para 13 idiomas, incluindo o português. O conjunto de treinamento consiste em tríplices (consulta, passagem relevante, passagem não relevante), que dividimos em pares de exemplos de treinamento, com cada par contendo a consulta associada a uma passagem – seja relevante ou não relevante – o que cria um exemplo para cada rótulo. Criamos um conjunto de treinamento bilingue português-inglês atribuindo aleatoriamente um dos dois idiomas a cada tripla de treinamento. Essa abordagem "translate-train" [9,48,20] aproveita a ampliação de dados sintéticos por meio da integração de traduções de máquina com dados de texto originais para expandir substancialmente o material de treinamento disponível na língua-alvo.
Pesquisas anteriores [3, 38, 9] demonstraram a eficácia dessa estratégia de treinamento bilingue, o que motivou nossa adoção desse método. Os modelos foram treinados por 100k passos com tamanhos de lote de 128 sequências e comprimento máximo de 512 tokens, utilizando Adafactor com uma taxa de aprendizado constante de 0,001 como otimizador. Instâncias que excediam o comprimento de token máximo foram excluídas do conjunto de treinamento; essas constituíram aproximadamente 0,01% do conjunto de treinamento e foram predominantemente atribuídas a dados de tradução ruídos. Dado os recursos computacionais significativos necessários para treinar esses rerankers, nos concentramos exclusivamente em modelos baseados nos principais checkpoints do ptt5-v2. Para avaliar os rerankers, primeiro usamos BM25 para gerar um conjunto inicial de documentos relevantes e, em seguida, rerankamos os documentos mais relevantes. As métricas de recuperação são calculadas comparando essa lista ordenada com os julgamentos de relevância de 4 Todos os aplicativos de BM25 nessa obra utilizam a implementação do Pyserini [26] com parâmetros padrão k1 = 0,9 e b = 0,4 6 M. Piau et al. em cada conjunto de dados. Consideramos dois cenários de recuperação: in-domínio (usando o conjunto "pequeno dev" de 6.980 consultas do mMARCO-pt) e zero-shot (usando o conjunto português de 249 consultas anotadas do mRobust [21]). Devido ao comprimento de documento mais longo no mRobust, segmentamos documentos em janelas de janela de sentença movível usando um pipeline de sentencizador Spacy [19], com comprimento máximo de 8 e passo de 4 sentenças para mitigar a truncagem durante a reranking. 4 Resultados Principais A Tabela 3 mostra os resultados nas tarefas downstream consideradas. No task de ASSIN2 RTE, nosso modelo 3B estabelece um novo SOTA, superando o atual por 0,61 pontos de F1-macro. Para o conjunto de dados TweetSentBR, alcançamos desempenho melhor do que os SOTAs finetunados atuais com ptt5-v2-large e ptt5-v2-3B, por 0,52 e 1,54 pontos de F1-macro, respectivamente, mas nossos resultados são piores quando comparados ao GPT-4.
Destacamos que nossos ptt5-v2 foram treinados exclusivamente em dados de treinamento para cada tarefa utilizando o framework de texto para texto sem qualquer ampliação de dados ou adaptação à arquitetura do modelo, ao contrário dos trabalhos de [2] e [38], que detinham o SOTA para TweetSentBR e ASSIN2 RTE. No entanto, nossos modelos não superaram o SOTA atual no task ASSIN2 STS; no entanto, ptt5-v2 ainda apresenta desempenho melhor que os modelos mT5 e T5 com tamanhos aproximados, e isso também é o único task em que um modelo ptt5-v2 menor (ptt5-v2-large) apresenta melhor desempenho que um modelo grande ptt5-v2-3B.

Binary classification Regression Multiclass classification ← Retrieval ASSIN2 RTE ASSIN2 STS TweetSentBR ← mMARCO-pt mRobust-pt Parâmetros do Modelo F1-macro Pearson F1-macro NPM RR@10 nDCG@20 T5 t5-small 60M 83,66 0,738 62,14 61,71 - - t5-base 220M 85,80 0,764 65,43 65,63 - - t5-large 770M 88,91 0,790 68,22 69,95 - - t5-3B 3B 90,78 0,827 72,58 74,56 - - mT5 mt5-small 300M 75,36 0,688 61,81 54,35 - - mt5-base 580M 79,39 0,749 70,76 63,46 - - mt5-large 1,2B 88,25 0,753 61,11 64,76 - - mt5-xl 3,7B 91,81 0,827 77,05 77,45 - - ptt5-v2 ptt5-v2-small 60M 87,14 0,782 70,99 69,86 0,273 0,344 ptt5-v2-base 220M 88,36 0,814 73,20 72,82 0,311 0,384 ptt5-v2-large 770M 91,73 0,839 76,78 77,68 0,315 0,462 ptt5-v2-3B 3B 92,68 0,829 77,80 78,48 0,332 0,512 SOTA supervisionado - 92,07 [38] 0,868 [37] 76,26 [2] - 0,306 [38] 0,391 [21] Baselines de poucas iterações de [32] GPT-4 - 90,96 0,776 82,40 - - - GPT-3.5-turbo - 88,28 0,664 74,39 - - - Tabela 3: Principais resultados após fine-tuning. Para SOTAs supervisionadas, tamanhos de modelo são os seguintes: [38] (335M), [37] (900M), [2] (110M), [38] e [21] (580M). Resultados NPM excluem tarefas de recuperação. ptt5-v2

Além disso, também incorporamos o Metrico Normalizado Preferencial (NPM) [43] para facilitar a avaliação do desempenho geral de um modelo treinado prévio em múltiplas tarefas. O NPM normaliza o métrico preferencial de uma tarefa (por exemplo,
, F1-macro para ASSIN2 RTE), atribuindo um valor de 0 para representar desempenho aleatório e 100 para denotar desempenho máximo. A seguir está a equação utilizada para calcular o NPM para um modelo dado e um conjunto N de tarefas: NPM = 100 N N X i=1 [métrica preferida bruta]i −[pontuação aleatória]i [pontuação máxima]i −[pontuação aleatória]i (1) Dado que os rerankers MonoPTT5 foram treinados exclusivamente a partir de checkpoints pré-treinados ptt5-v2, as tarefas de recuperação foram excluídas desta avaliação. Portanto, apenas consideramos tarefas ASSIN2 RTE, ASSIN2 STS e TweetSentBR. Para cada modelo, calculamos seu desempenho agregado primeiro determinando o NPM para cada tarefa e em seguida computando a média desses valores. Nossos modelos ptt5-v2 têm valores de NPM mais altos do que os modelos T5 e mT5 com consideravelmente mais parâmetros: por exemplo, ptt5-v2-base é apenas ultrapassado por t5-3B (∼13,6x maior) e t5-xl (∼16,81x maior); essa diferença de desempenho, no entanto, é mais pronunciada em modelos menores, se estreitando à medida que o tamanho do modelo aumenta. Um resultado semelhante também foi observado por Xue et al. [48], que analisou o desempenho dos modelos T5 e mT5 em relação ao benchmark SQuAD [35], observando uma diferença de desempenho entre t5-small e t5-base vs modelos mT5 de tamanhos equivalentes, que se diminui a partir de t5-large. Essa diferença de desempenho observada em tamanhos de modelo menores é vantajosa quando consideramos ambientes limitados por recursos computacionais, aumentando o nível máximo atingível de desempenho; adicionalmente, um tokenizador específico para língua reduz a divisão do texto em tokens menores, levando a uma latência mais baixa e ao potencial de acomodar mais texto no mesmo contexto de janela de tokens máxima. Interessantemente, os modelos mT5 tendem a mostrar valores de NPM mais baixos, exceto na faixa de 3 bilhões de parâmetros, onde eles ligeiramente superam os modelos T5. Para as tarefas de recuperação, nossos rerankers MonoPTT5 foram capazes de estabelecer novos recordes de SOTA para ambos mMARCO-pt e mRobust-pt.
Para o conjunto de dados mMARCO-pt, os modelos que começaram com o tamanho t5-base foram capazes de superar o SOTA atual; o reranker com 3 bilhões de parâmetros obteve um ganho de +0,026 pontos em MRR@10. No mRobust-pt, nossos rerankers grandes e de 3B superaram o SOTA atual em +0,071 e +0,121 em termos de nDGC@20, respectivamente. Uma análise mais aprofundada das tarefas de recuperação é realizada na seção 5.2.

5 Ablações
5.1 Experimentos de pré-treinamento adicionais
Essa seção inclui experimentos de pré-treinamento adicionais aos descritos na seção 3.1.

8 M. Piau et al.
Comparação com ptt5-v1:
Dada a título do nosso trabalho, uma pergunta pertinente surge: como os modelos ptt5-v2 se comparam com o trabalho em ptt5-v1? Algumas diferenças-chave existem no pré-treinamento do ptt5-v1. Notavelmente, ele utilizou BrWac, um conjunto de dados significativamente menor, e um objetivo de pré-treinamento ligeiramente diferente (denoising, onde alguns tokens de entrada são mascarados e o modelo é treinado para prever o texto original, em vez de corrupção de spans). Além disso, o ptt5-v1 empregou modelos variando de t5-pequeno a t5-grande com um otimizador Adafactor e uma taxa de aprendizado de 0,003 (três vezes maior do que nosso setting). O ptt5-v1 também explorou tanto a vocabulária original do T5 quanto um tokenizador de linguagem portuguesa específica. Em contraste, o ptt5-v2 exclusivamente utiliza o último. Para garantir uma comparação justa, finetunamos os checkpoints do ptt5-v1 seguindo a metodologia descrita na seção 3.2. A figura 1 apresenta os valores de NPM para ambos ptt5-v1 e ptt5-v2, ao lado de comparações com os modelos mT5 e T5. Os dados corroboram as melhorias alcançáveis através do pré-treinamento monolíngue na língua-alvo, o que é ainda mais amplificado pelo emprego de um tokenizador dedicado. Em comparação entre as duas iterações do ptt5, uma disparidade de desempenho favorável ao ptt5-v2 é aparente para os tamanhos pequeno e grande, com o maior gap observado para o grande; no entanto, os modelos do tamanho base apresentam desempenho marginalmente superior na variante ptt5-v1.
Surpreendentemente, o desempenho do mT5 atrás de todos os outros modelos, incluindo o T5 monolíngue inglês, exceto na faixa de parâmetros de 3B, onde se aproxima do ptt5-v2 e supera o T5. 10 8 10 9 Parâmetros não de embedding 55 60 65 70 75 Base pequena grande 3B T5 mT5 ptt5-v2 ptt5-v1 (vocabulário T5) ptt5-v1 (vocabulário PTT5) Fig. 1: Parâmetros vs NPM para configurações de T5 variáveis. Filtros de qualidade: Em nossos experimentos primários, utilizamos todo o conjunto de dados mC4-pt, contendo aproximadamente 116 bilhões de tokens de treinamento, na fase de pré-treinamento. Nesse experimento adicional, consideramos os filtros de qualidade MassiveText [33] para investigar o impacto desse processo de filtragem em tarefas downstream. Aplicando esses filtros ao mC4-pt reduz o número de tokens de treinamento para aproximadamente 82 bilhões, uma redução de cerca de 30%. Esse experimento foi restrito a modelos de tamanho t5-base, mantendo o mesmo tamanho de lote e estratégia de otimização utilizados nos experimentos principais. A Figura 2 mostra o efeito da aplicação dos filtros de qualidade MassiveText ptt5-v2 na performance de tarefas downstream, medida em termos de NPM. O pré-treinamento com o conjunto de dados filtrado apresenta uma tendência ascendente na performance, que continua sem saturação até o final de um epoch de mC4-pt, que é o último ponto na plotagem. É importante notar que um epoch no conjunto de dados filtrado tem menos passos do que o conjunto de dados completo, então, no final do conjunto de dados completo, os dados são repetidos no conjunto de dados filtrado. Apesar da tendência ascendente favorável ao pré-treinamento com o conjunto de dados filtrado, a diferença em termos de NPM é pequena no marco de um epoch de mC4-pt. 0 20 40 60 80 100 120 Tokens de pré-treinamento português (B) 67 68 69 70 71 72 73 74 NPM um epoch de mC4-pt um epoch de mC4-pt com filtros de qualidade mC4-pt mC4-pt com filtros de qualidade Fig. 2: Efeito da aplicação de filtros de qualidade no mC4-pt. Linhas verticais indicam o número de tokens de pré-treinamento para cada conjunto de dados considerado.
Tática de otimização pré-treinamento: Em nossa exploração de estratégias de otimização, inicialmente utilizamos Adafactor com uma taxa de aprendizado constante. Esta ablação estende nossa investigação à "taxa de aprendizado inversa do quadrado" agendada, utilizada por [34] em seus experimentos de pré-treinamento finais. Esta taxa de aprendizado computa a taxa como 1 √ max(n,k), onde n representa o passo atual e k é o número de passos de aquecimento. Raffel et al. [34] utilizaram k = 10.000, o que estabelece a taxa de aprendizado de 0,01 para os primeiros 10k passos, subsequentemente diminuindo exponencialmente. A taxa de aprendizado no final do pré-treinamento, que consistiu em cerca de 1 milhão de passos, estava próxima de 0,001, a mesma utilizada durante a fine-tuning. A figura 3 ilustra a diferença entre essas estratégias de otimização. Tentando refletir de forma próxima a receita de pré-treinamento do T5, aplicamos essa mesma agenda em nossos experimentos iniciais. No entanto, observamos um overshoot rápido nos perdas de treinamento para t5-large e t5-3B dentro de horas; ajustar n apenas atrasou o overshoot sem evitá-lo. Mudando para uma taxa de aprendizado constante de 0,001 resolveu o problema de overshoot, levando a perdas de treinamento estáveis em todos os tamanhos de modelo e simplificando nosso setup experimental. Como o overshoot foi observado apenas nos modelos maiores e também considerando os custos computacionais associados ao pré-treinamento dos modelos maiores, realizamos experimentos adicionais com 10 milhões de tokens de pré-treinamento. Figura 3: Agendamentos de taxa de aprendizado: constante vs. agendador de taxa de aprendizado inversa como função de passos de treinamento. Figura 4: Efeito do agendador utilizado durante o pré-treinamento. Épocas são relativas ao mC4-pt.
Experimentos de pré-treinamento com agendador "raiz quadrada inversa" para modelos t5-small e t5-base apenas. Número de épocas de pré-treinamento: No conjunto primário de experimentos, o conjunto de dados mC4-pt foi utilizado integralmente para pré-treinamento durante uma época. Para explorar a influência do número de épocas de pré-treinamento no desempenho de tarefas downstream, realizamos experimentos em épocas variadas, incluindo épocas parciais (0,25, 0,5 e 0,75 de uma época). Considerando o tempo e recursos computacionais significativos requeridos para pré-treinamento estendido, especialmente com modelos maiores, limitamos o pré-treinamento do modelo t5-large a duas épocas e do modelo t5-3B a uma época. A duração reduzida da época para os modelos t5-small e t5-base permitiu períodos de pré-treinamento mais extensos para essas configurações. Na Figura 4, os valores de NPM para o modelo t5-base são mostrados com o agendador constante e o agendador da raiz quadrada inversa, em uma variedade de épocas de pré-treinamento. Observa-se que há uma diferença entre essas duas estratégias de otimização: o agendador da raiz quadrada inversa tem o advantage até dois episódios; após isso, a taxa de aprendizado constante assume o comando, e até o último episódio considerado, alcançam o mesmo valor. Além disso, é notado um aumento na tendência dos valores de NPM para mais épocas.

5.2 Rerankers MonoPTT5

As tarefas de recuperação de informações relatadas em 3 representam o desempenho de nossos experimentos MonoPTT5, desenvolvidos com a metodologia descrita em 3.3; nessa seção, também relatamos os resultados para outras abordagens, incluindo BM25, e recuperação densa usando modelos multilingues-e5 [47]. Os modelos densos são usados como sistema de recuperação em uma única etapa sem reranking; índice e recuperação densa são realizados com GPUs A100 e V100 no Google Colab, aproveitando o framework Pyserini. Para o mRobust-pt, que contém documentos mais longos, mitigamos a truncagem de documentos usando a mesma estratégia de divisão descrita na seção 3.
3, utilizando a pontuação máxima entre os segmentos do documento como a pontuação do documento. Figuras 5 e 6 são utilizadas para ilustrar a discussão apresentada nesta seção. Os resultados apresentados na Tabela 3 apenas mostram a eficácia em cada tarefa de recuperação de nossos modelos MonoPTT5 e os competidores SOTA. Para a tarefa de recuperação em domínio, o conjunto de dados mMarco-pt, observamos que o BM25 é facilmente superado por todas as alternativas consideradas, e as figuras de eficácia para os rerankers MonoPTT5 e os modelos multilingue-e5 são semelhantes quando consideramos o tamanho comum, e os rerankers MonoPTT5 apresentam eficácia acima da SOTA a partir de modelos de tamanho t5-base. Para o mRobust-pt, representando um setting zero-shot, o BM25 é apenas superado pelo reranker mT5 de Jeronymo et al. [21], e os modelos MonoPTT5 a partir do tamanho t5-base. 

8 10 10 9 Parâmetros não de embedding 0,150 0,175 0,200 0,225 0,250 0,275 0,300 0,325 RR@10 pequeno base grande 3B MonoPTT5 multilingue-e5 mColBert mT5 BM25 Fig. 5: Resultados de recuperação no mMarco-pt. Os valores de mColbert e mT5 são da Bonifácio et al. [3]. O tamanho total exclui parâmetros de embedding.

12 M. Piau et al. 10 8 10 9 Parâmetros não de embedding 0,25 0,30 0,35 0,40 0,45 0,50 pequeno base grande 3B MonoPTT5 multilingue-e5 mColBert mT5 BM25 Fig. 6: Resultados de recuperação no mRobust-pt. Os valores de mColbert e mT5 são da Jeronymo et al. [21]. O tamanho total exclui parâmetros de embedding.

6 Conclusão Nesta pesquisa, introduzimos o ptt5-v2, explorando a pré-treinamento contínua de modelos T5 para a língua portuguesa. Pre-treinamos modelos T5 usando um tokenizador de língua portuguesa, sobre um corpus de língua portuguesa. Os modelos fine-tuned alcançaram a SOTA nos conjuntos de dados ASSIN2 RTE e TweetSentBr, dois dos três tarefas downstream consideradas. Além disso, aplicamos esses checkpoints pré-treinados para desenvolver rerankers MonoT5 personalizados para a língua portuguesa, alcançando desempenho top nos conjuntos de dados mMARCO-pt e mRobust-pt.
Nossos principais resultados corroboram a evidência de um gap de desempenho a favor de modelos monolíngues em relação a modelos focalizados em inglês e multilíngues, um gap que se estreita à medida que aumenta a capacidade do modelo. Isso destaca a importância do pré-treinamento específico para língua e nossa análise de configurações de pré-treinamento sugere que, embora o filtro de dados, estratégias de otimização e duração do pré-treinamento possam oferecer melhorias incrementais, os efeitos globais foram limitados em comparação com nossos padrões de base e a receita de pré-treinamento básica permaneceu robusta. Agradecimentos Obrigado à Google pelo apoio financeiro através do programa TRC.