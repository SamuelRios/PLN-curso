Avaliando LLMs europeus e brasileiros para NER em domínios especializados

Rafael Oleques Nunes1[0009−0007−8842−421X], Joaquim Santos2[0000−0002−0581−4092], Andre Spritzer1[0009−0002−4232−1585], Dennis Giovani Balreira1[0000−0002−0801−9393], Carla Maria Dal Sasso Freitas1[0000−0003−1986−8435], Fernanda Olival3[0000−0003−4762−3451], Helena Freire Cameron3[0000−0001−7719−6994], e Renata Vieira4[0000−0003−2449−5477]

1 Universidade Federal do Rio Grande do Sul, Brasil {ronunes,dgbalreira,spritzer,carla}@inf.ufrgs.br
2 Universidade do Vale do Rio dos Sinos, Brasil nejoaquim@edu.unisinos.br
3 CIDEHUS - Universidade Portalegre Polytechnic, Portugal helenac@ipportalegre.pt
4 CIDEHUS - Universidade de Évora, Portugal {mfo,renatav}@uevora.pt

Resumo. Este artigo discute o impacto das variantes do português em Modelos de Linguagem Grande para a tarefa de reconhecimento de entidades nomeadas (NER) em domínios especializados. Os testes foram realizados em um corpus legal brasileiro e um corpus histórico europeu. Os modelos considerados são BERTimbau (PT-BR), Albertina (PT-PT e PT-BR), e XML-R (multilíngue). O impacto foi mais evidente no corpus histórico português, o que resultou em medidas F1 mais altas em comparação com trabalhos anteriores que não consideraram a mesma variante linguística. Além disso, o estudo destaca o impacto da arquitetura do modelo na performance, destacando o papel crítico da alinhamento linguístico e do tamanho do modelo para melhorar a NER em domínios especializados.

Palavras-chave: Reconhecimento de entidades nomeadas · Modelos de Linguagem Grande · Variantes da língua portuguesa.

1 Introdução

O reconhecimento de entidades nomeadas (NER) é uma tarefa crucial em Processamento de Linguagem Natural (NLP). O objetivo do NER é identificar e classificar termos específicos em uma sentença, como nomes de pessoas, organizações, localizações, datas e outras entidades. Esta tarefa é fundamental para várias aplicações de NLP, incluindo extração de informações, resposta a perguntas e resumo automático de texto.
Embora muitos estudos e modelos tenham sido desenvolvidos com resultados satisfatórios, a maioria se concentrou em entidades gerais, baseando-se em conjuntos de dados como o CoNLL 2002 para o inglês [23] e o HAREM para o português [17]. No entanto, domínios especializados frequentemente não podem utilizar esses classificadores gerais de forma eficaz devido ao uso de linguagem específica da área ou à necessidade de identificar tipos de entidades diferentes não comumente presentes em conjuntos de dados gerais. Por exemplo, enquanto nomes de genes e doenças são fundamentais no domínio biomédico, em textos legais, entidades como citações de leis e nomes de tribunais são mais relevantes. Estudos anteriores abordaram esses desafios propondo conjuntos de dados e modelos personalizados para domínios específicos [20, 18, 13]. Como modelos especializados em variantes do português tornaram-se recentemente disponíveis [21, 15], este estudo avalia o desempenho desses modelos em textos especializados escritos em dois variantes do corpus de NER anotado em português (Europeu e Brasileiro). Nossa intenção é comparar como esses modelos se comportam nessas variantes, considerando dois domínios especializados, histórico e legislativo, que apresentam desafios únicos e requerem conhecimento específico da área. Ao avançar as capacidades de NER em textos especializados em português em duas variações da língua, pretendemos facilitar a extração de informações mais precisas e relevantes no contexto, apoiando várias aplicações, desde a pesquisa histórica até o processamento de documentos legais. Os campos legislativo e histórico têm o benefício de serem abrangentes, apesar de alguma especialização. Este estudo contribui para o entendimento acadêmico de NER especializado e fornece insights práticos e ferramentas para melhorar aplicações de NLP em campos especializados.
Nossas principais contribuições são: (i) uma avaliação abrangente de diferentes modelos em tarefas de Reconhecimento de Entidades Nomeadas (REN) em textos portugueses, (ii) uma análise das diferenças no desempenho dos modelos quando treinados com variantes europeia e brasileira da língua portuguesa, e (iii) insights sobre a aplicação desses modelos em diferentes corpora de domínios especializados: jurídico e histórico.

2 Trabalhos Relacionados O Reconhecimento de Entidades Nomeadas (REN) envolve identificar e classificar entidades como locais, organizações e pessoas dentro de um texto. Entidades gerais têm sido amplamente exploradas em várias línguas e corpora. Dados populares para avaliar o REN em diferentes línguas incluem o CoNLL 2002 para inglês [23] e o HAREM para português [17]. Esses dados têm sido instrumentais para avançar a pesquisa de REN e estabelecer padrões de desempenho para modelos. No entanto, os classificadores treinados com esses dados não são sempre suficientes para todos os domínios. Por exemplo, Silva et al. (2023) [20] propuseram um conjunto de dados focado em cachaça, um espírito destilado feito de suco de cana-de-açúcar, onde categorias importantes de entidades incluem classificação, preço, tempo de armazenamento e características sensoriais, como cor. Essas entidades específicas não podem ser adequadamente representadas usando apenas as categorias tradicionais de REN. O domínio jurídico também foi encontrado para beneficiar do uso de entidades especializadas, como citações legais de legislação ou casos de tribunal [3]; produtos de lei e base legal [2]; cortes, origens de procedimento legal e datas de julgamento [7]; e decisões e sentenças [5]. Estudos recentes exploraram essas áreas usando técnicas variadas com modelos de transformadores, incluindo adaptação de domínio [4, 9], fine-tuning [25], aprendizado em contexto [14] e auto-aprendizado [13].

3 Textos históricos representam outro domínio importante com características únicas.
Estudos que se concentraram em tarefas de reconhecimento de entidades nominais (NER) em textos históricos [10, 24, 18] destacaram os desafios apresentados pelas diferentes ortografias e estruturas de sentenças em relação a textos contemporâneos na mesma língua. Todos os trabalhos discutidos até agora se concentraram em tarefas de NER no contexto de textos em língua portuguesa. Neste estudo, estendemos essa abordagem ao avaliar o impacto de modelos específicos da língua portuguesa em textos especializados, especificamente nos domínios histórico e jurídico, explorando tanto variantes da língua europeia quanto brasileira.

3 Corpora
3.1 Textos Históricos Anotados em Português Europeu
O primeiro sub-corpus considerado neste estudo é um subconjunto do corpus histórico PT-Eu, a coleção de Memórias Paroquiais. Trata-se de um corpus do século 18 composto pelas respostas a uma enquete enviada em 1758 aos padres portugueses para obter retroalimentação sobre o estado do território após o grande terremoto de Lisboa em 1755 e para recolher informações para a elaboração de um Dicionário Geográfico de Portugal. O sub-corpus sob análise contém 71 Memórias das municipalidades das principais cidades na maior região de Portugal, Alentejo, ou seja, Portalegre, Évora e Beja. Também adicionamos as Memórias da municipalidade de Vila Viçosa, uma municipalidade historicamente relevante, dominada pela Casa de Bragança. Os manuscritos originais foram transcritos e normalizados para a ortografia padrão contemporânea do português europeu. O sub-corpus foi anotado com entidades nominais, personalizadas para a realidade histórica. Em uma abordagem inicial (Anônimo, 2021), três categorias básicas foram consideradas (pessoa, local, organização), pois, para um historiador, elas visam responder às principais perguntas: Quem, Onde e Quando. Após essa abordagem inicial, considerando a necessidade de descrever realidades passadas melhor, um estudo baseado em corpus foi conduzido para definir extensões dessas categorias (Anônimo, 2022) de acordo com sua relevância para a inquirição do historiador.
Para isso, a principal categoria de pessoa foi dividida em várias subcategorias. A categoria pessoa (PER) se refere a referências feitas pelo nome, primeiro nome e sobrenome (PER_NAM); ocupação (PER_OCC); ou categoria social (PER_CAT). Todos esses atributos refletem a estrutura hierárquica da sociedade portuguesa do século 18, pois frequentemente, títulos e posições ocupacionais eram quase parte do nome e da identidade de uma pessoa. Um exemplo de menções a pessoas pela ocupação é Reitor da Universidade de Évora (Rector da Universidade de Évora). Ainda relacionada à categoria PER, e porque constituem detalhes específicos de pessoa, estabelecemos outras subcategorias para rotular santos (PER_SAINT); divindades (PER_DIV); grupos de pessoas (PER_PGRP); e autores (PER_AUT). A subcategoria para grupos de pessoas é usada para anotar grupos orgânicos, famílias e membros de uma organização, entre outros. Monges Cartuxos (monges cartuxos) e Sarracenos (os sarracenos) são exemplos dessa categoria.

Quanto à categoria local, generalizamos-a para lugar (PLC). Esta categoria inclui entidades geopolíticas (PLC_GPE), aquíferos (PLC_AQU), montanhas (PLC_MOUNT), instalações (PLC_FAC) e uma subcategoria adicional para outros locais (PLC_LOC), como por exemplo, Bispado de Portalegre (Bispado de Portalegre, PLC_GPE) e Rio Guadiana (rio Guadiana, PLC_AQU). As entidades geopolíticas foram incluídas para evitar ambiguidades entre localizações e organizações, pois essa categoria as agrupa indistintamente. Outras referências a pontos geográficos, como rios e montanhas, são essenciais para referências geográficas. As categorias restantes são para organização, tempo e obra autoral. A categoria ORG rotula várias organizações, como por exemplo, Companhia de Jesus (Sociedade de Jesus), e Universidade de Coimbra (Universidade de Coimbra).
Para o TIM_CRON, apenas anotamos referências específicas a datas, por exemplo, o ano de 1755 (ano de 1755). Também estabelecemos uma categoria, AUTWORK, que permite tratar as fontes de texto mencionadas no corpus para reconhecer as fontes de texto mencionadas por padres. Um conjunto estendido de categorias NE para contabilizar idades passadas também implica mais complexidade na anotação e nos processos computacionais. Todos os documentos do sub-corpus foram anotados manualmente com base no julgamento consensual de quatro anotadores, e foi feito usando a plataforma INCEPTION5. Como pode ser visto na Tabela 1, temos 5.031 NEs anotadas. As principais classes são entidades geo-políticas, nomes de pessoas e santos. As pessoas são referenciadas apenas por categoria, e os montes são os menos representados. Para treinamento, desenvolvimento e teste, a distribuição é 70%, 10% e 20%. Tabela 1. Frequência de entidades nomeadas nas Memórias Paroquiais para cada tipo. CATEG Treinamento Desenvolvimento Teste Total NE AUTWORK 106 12 19 137 ORG 287 52 54 393 PER_AUT 101 13 15 129 PER_CAT 37 4 8 49 PER_DIV 119 25 40 184 PER_NAM 520 62 136 718 PER_OCC 88 11 25 124 PER_PGRP 153 25 21 199 PER_SAINT 435 76 133 644 PLC_AQU 147 13 68 228 PLC_FAC 202 18 69 289 PLC_GPE 785 84 232 1101 PLC_LOC 336 24 87 447 PLC_MOUNT 50 10 13 73 TIM_CRON 217 33 66 316 Total 3.583 462 986 5031 5 https://inception-project.github.io Avaliando Modelos de Linguagem Grande Portuguesa para NER 5 3.2 Textos Legislativos Portugueses Anotados Tabela 2. Frequência de entidades nomeadas no UlyssesNER-Br para cada tipo.
O corpus legislativo utilizado neste trabalho é o UlyssesNER-Br [2]. Ele compreende 150 projetos de lei da Câmara dos Deputados do Brasil. As anotações foram realizadas em três fases por dois estudantes de graduação e um estudante de mestrado como curador. O UlyssesNER-Br [2] incluiu ambos os níveis grosseiros e finos. O nível grosseiro compreende sete categorias de entidades, enquanto o nível fino inclui dezoito tipos de entidades. As entidades seguem as entidades tradicionais do HAREM [16] (pessoa, localização, organização, evento e data) com a adição de entidades específicas do domínio, como fundações e produtos legais. A entidade PESSOA (pessoa) foi especializada em três tipos: PESSOAcargo (ocupação), PESSOAgrupocargo (grupo de ocupações) e PESSOAcindividual (individual). Nessa divisão, é possível incluir níveis finos de citações de pessoas, como Deputado HILDO ROCHA (Deputado HILDO ROCHA) e 16 de março de 2011 [16 de março de 2011]. FUNDAMENTO (fundação legal) refere-se a várias entidades legais, como leis, projetos de lei e consultas legislativas solicitadas por congressistas. Essa categoria inclui entidades finas, como FUNDlei (norma legal), FUNDapelido (nome de norma legal) e FUNDprojetodelei (proposta de lei). Exemplos de entidades finas incluem art. 34 do Estatuto do Idoso (art. 34 da Lei do Idoso) e Código Brasileiro de Trânsito (Código de Trânsito do Brasil). A categoria final, PRODUTODELEI (produto legal), se refere a qualquer coisa criada em virtude de legislação.
Este classe também inclui três tipos refinados: DUTOsistema (produto do sistema), PRODUTOprograma (produto do programa) e PRODUTOoutros (outros produtos). Exemplos de cada classe refinada são o Sistema Único de Saúde (SUS) e o salário mínimo. 4 Marco e Modelos Adotamos o framework Flair[1], uma biblioteca de reconhecimento de entidades nomeadas para múltiplos idiomas desenvolvida em PyTorch6. Esse framework fornece modelos de linguagem pré-treinados, modelos de reconhecimento de entidades nomeadas e redes neurais para treinamento de modelos de linguagem e marcação de sequências. Com o Flair, podemos construir pipelines para treinar classificadores de tokens e alimentá-los com vários tipos de modelos de linguagem, como Embeddings de Palavras, modelos baseados em Transformer e Embeddings do Flair em si. Aqui, analisamos quatro versões de modelos de linguagem. O BERTimbau [21] é um modelo de linguagem pré-treinado baseado em Transformer treinado especificamente para o português brasileiro. Foi treinado no corpus brWaC [8], que soma um total de 2,6 bilhões de tokens, resultando em 17,5 GB de dados pré-processados. O BERTimbau foi treinado usando máscara de tokens em sentenças de entrada. Em outras palavras, é um Modelo de Linguagem Mascada (MLM). Escolhemos esse modelo porque o estado-da-arte atual [22] em NER para o português o utiliza. Utilizamos a versão Large do BERTimbau. 7. A Albertina PT-* [15] é um modelo de linguagem grande projetado explicitamente para o português. Funciona como um codificador dentro da família BERT e é construído sobre o modelo DeBERTa usando a arquitetura neural Transformer. A Albertina PT- tem dois variantes: Albertina PT-PT e Albertina PT-BR. Ambas as variantes são distribuídas gratuitamente, sob uma licença permitida. A Albertina PT-PT é a versão do português europeu. Esse modelo está disponível em três tamanhos, especificamente com 1,5 bilhão de parâmetros, 900 e 100 milhões de parâmetros. Os corpora de treinamento prévio da Albertina PT-PT 1,5B compreendem domínios gerais e legislativos.
Aqui está a tradução do texto científico para português do Brasil:

A Albertina PT-BR se concentra no português brasileiro. Sua versão mais ampla, Albertina 1,5B PT-BR [19], tem um modelo licenciado mais permissivo sem utilizar o conjunto de dados BrWac, consistindo em um conjunto de tokens de 36 bilhões compilado a partir de um corpus multilíngue. Como esse corpus inclui tanto o português europeu quanto o português brasileiro, foi aplicada uma filtragem adicional para manter apenas documentos com metadados indicando o código de domínio de internet do Brasil. O XLM-RoBERTa [6] é um modelo multilíngue projetado para entender 100 línguas sem requerer tensors específicos para língua, pois pode identificar a língua diretamente a partir dos identificadores de entrada. Ele incorpora técnicas do RoBERTa [12] no framework XLM [11], se concentrando exclusivamente em modelagem de linguagem mascarada para sentenças de língua única, e não emprega modelagem de linguagem de tradução. A tabela 3 compara os quatro modelos descritos de acordo com algumas de suas características. 6 https://pytorch.org 7 https://huggingface.co/neuralmind/bert-large-portuguese-cased Avaliação de Modelos de Linguagem Grande para NER em Português 7 Tabela 3. Comparação dos quatro Modelos de Linguagem Grande em Português. Característica Albertina PT-PT Albertina PT-BR XLM-R BERTimbau Parâmetros 1,5B 1,5B 550M 355M Corpus CulturaX, DCEP, Europarl, ParlamPT CulturaX Multi-data brWaC Arquitetura DeBERTa, 24L, 16H DeBERTa, 24L, 16H Trans, 24L, 16H BERT, 24L, 16H Língua PT (PT) PT (BR) 100 línguas PT (BR) Domínio Geral, Legislativo Geral Geral Geral Ano 2023 2023 2019 2021 5 Avaliação Experimental Realizamos nossos experimentos em um GPU A100 com 80GB de RAM e um RTX 4090 com 64GB de RAM. Os experimentos utilizaram Python 3,7,6 e a biblioteca Flair para utilizar modelos treinados. Os hiperparâmetros foram definidos para os valores padrão recomendados pela biblioteca: uma taxa de aprendizado de 5e-5, um tamanho de mini-lote de 4 e treinamento por 10 épocas. Foi aplicada truncagem ao comprimento máximo e padding foi definido como verdadeiro.
Utilizamos métricas padrão para avaliação do modelo, incluindo precisão, recall e micro F1-score utilizando o script CoNLL-2002 [23]. Essas métricas forneceram uma avaliação completa do desempenho do modelo em diferentes classes, garantindo uma avaliação rigorosa de sua eficácia.

6 Resultados e Discussão

6.1 Resultados Gerais

Primeiramente, detalhamos os resultados para cada corpus individualmente. Em seguida, combinamos os resultados para análise comparativa, destacando a influência do contexto linguístico e do domínio textual no desempenho dos modelos de NER especializados.

Modelo Precisão Recall F1 ∆↑
Albertina PT-PT 72,76 76,10 74,39 +3,02
Albertina PT-BR 69,71 73,11 71,37 +0,61
XLM-R-Large 68,31 73,38 70,76 +0,23
BERTimbau-Large 67,36 74,00 70,53 bl

Tabela 4. Resultados dos modelos Parish Memories.

Corpus Parish Memories. A Tabela 4 apresenta os resultados gerais, destacando que a Albertina, treinada em português europeu, alcançou resultados de F1-Score superiores em comparação com outros modelos. Esse sucesso é provavelmente não apenas devido à Albertina ter mais parâmetros do que modelos anteriores, mas principalmente devido à sua pré-treinamento em textos em português europeu. Dado que o corpus consiste em textos portugueses do século 18, eles foram usados em sua versão normalizada em português europeu, e essa alinhamento linguístico pode ter contribuído significativamente para o desempenho da Albertina. Outra observação notável concerne as diferenças nos scores F1, como indicado na coluna ∆↑. A maior discrepância entre os resultados dos modelos é observada com a Albertina em português europeu, que supera significativamente sua contraparte em português brasileiro. O métrica de recall também fornece conclusões insig- nificantes. O BERTimbau-Large exibe o maior recall entre os modelos em português brasileiro e multilíngues, superando a Albertina e o XLM-R-Large, sugerindo que sua arquitetura menor pode alcançar identificação de entidades comparável.
No entanto, as tendências de precisão se alinham com o tamanho do modelo, indicando que os modelos maiores tendem a identificar instâncias positivas verdadeiras com mais precisão. A utilização de um modelo treinado na mesma variante do português do Brasil como o corpus sempre produz os melhores resultados em todos os métricos. Precisão Recall F1 ∆↑ Albertina PT-PT 86,83 91,32 89,02 +0,08 Albertina PT-BR 87,93 89,98 88,94 +0,18 BERTimbau-Large 84,14 90,32 87,12 +1,18 XLM-R-Large 82,39 89,82 85,94 bl Tabela 5. Resultados dos modelos de nível fino do UlyssesNER-Br. Textos Legislativos Brasileiros. A Tabela 5 apresenta os resultados para o nível fino-grained no corpus UlyssesNER-Br. Ela destaca que os modelos maiores podem alcançar resultados melhores, mas a contribuição da especificidade linguística para o desempenho aprimorado permanece inconclusiva. O desafio da especificidade linguística é evidente na Tabela 5, onde o incremento no Escore F1 para os dois melhores modelos (as variações da Albertina) é apenas de 0,08, possivelmente devido à dados legislativos durante o treinamento prévio do português europeu da Albertina (ver Tabela 3), ou possivelmente à inicialização aleatória do classificador e flutuações nos resultados. Comparando os modelos, um achado significativo é a alta recall do BERTimbau em comparação com os modelos maiores, realçando sua eficácia em identificar instâncias positivas precisamente dentro de contextos legislativos específicos, mesmo sendo o modelo menor. No entanto, sua precisão mais baixa sugere possíveis erros em instâncias positivas. Os modelos maiores, como o XLM-R e a Albertina, excel em capturar categorias fine-grained. Embora o BERTimbau demonstre recall superior em análise fine-grained, o trade-off entre precisão e recall resulta em modelos maiores alcançando resultados de Escore F1 melhores com recall comparável. Os modelos Albertina alcançam o melhor equilíbrio entre precisão e recall. Este equilíbrio indica sua eficácia em capturar um grande número de instâncias relevantes e manter a precisão. Análise comparativa.
A variação linguística entre o português brasileiro e o português europeu diferiu significativamente entre os dois corpora, destacando como o corpus de Memórias Paroquiais poderia aproveitar modelos personalizados para avaliar modelos de linguagem grande para Entidade Reconhecimento de Nome (ERN) para sua variante específica de forma mais eficaz do que textos legislativos brasileiros. Uma explicação plausível reside na domínio e estrutura dos textos. O corpus de Memórias Paroquiais compreende cartas escritas por padres com níveis variados de formação acadêmica e formalidade. Esses textos diferem dos contemporâneos do português europeu, mas mantêm proximidade linguística significativa, especialmente em suas referências textuais. Enquanto as entidades mencionadas se referem a coisas, pessoas e eventos do século 18, muitas dessas referências são ainda usadas e mencionadas hoje, como nomes de lugares como Coimbra e Évora e nomes de paróquias e devoções locais que continuam no país. Portanto, é razoável argumentar que um modelo treinado no português europeu está melhor equipado para extrair entidades desse corpus. Conversemente, embora o corpus legislativo brasileiro contenha textos contemporâneos, muitos desses detalhes não foram necessariamente aprendidos com textos gerais em qualquer variante do português, devido à estrutura e jargão específicos dos textos legislativos. Assim, torna-se que, para esse corpus, o tamanho do modelo foi mais decisivo do que a variação linguística. Nossos resultados e análise ilustram como o contexto linguístico e o domínio textual jogam um papel crucial na forma como se selecionam e desempenham modelos de ERN. Sublinham a importância de avaliar cuidadosamente o corpus para selecionar um modelo adequado que otimize a extração de entidades nomeadas em contextos específicos.
45 67.86 PER_AUT 77.78 87.50 82.35 78.95 93.75 85.71 75.00 93.75 83.33 83.33 93.75 88.24 PER_CAT 87.50 87.50 87.50 50.00 75.00 60.00 38.89 87.50 53.85 53.33 100.00 69.57 PER_DIV 76.74 82.50 79.52 69.57 80.00 74.42 82.50 82.50 82.50 87.80 90.00 88.89 PER_NAM 61.04 67.63 64.16 66.23 71.83 68.92 61.88 71.22 66.22 68.59 76.98 72.54 PER_OCC 44.12 60.00 50.85 60.71 62.96 61.82 55.17 64.00 56.26 70.37 76.00 73.08 PER_PGRP 50.00 61.90 55.32 55.17 76.19 64.00 69.57 76.19 72.73 69.57 76.19 72.73 PER_SAINT 77.37 79.10 78.23 75.69 78.99 77.30 78.79 77.61 78.20 77.21 78.36 77.78 PLC_AQU 66.20 67.14 66.67 72.73 76.71 74.67 81.25 74.29 77.61 80.00 74.29 77.04 PLC_FAC 65.33 67.12 66.22 59.52 66.67 62.89 68.12 64.38 66.20 67.14 64.38 65.73 PLC_GPE 77.87 81.55 79.66 78.84 77.87 78.35 79.22 78.54 78.88 78.45 78.11 78.28 PLC_LOC 65.35 74.16 69.47 60.00 72.53 65.67 55.24 65.17 59.79 65.38 76.40 70.47 PLC_MOUNT 56.25 69.23 62.07 75.00 92.31 82.76 75.00 92.31 82.76 80.00 92.31 85.71 TIM_CRON 69.33 77.61 73.24 66.67 65.71 66.19 70.00 73.13 71.53 65.28 70.15 67.63 Tabela 6. Resultados da memória paroquial por entidade. 10 Autores Suprimidos devido à Excessiva Duração de Texto Presentamos os resultados para cada corpus no nível de entidade. Discutimos como cada modelo aprendeu cada entidade e qual é a influência das entidades especializadas. Posteriormente, apresentamos uma análise comparativa para concluir como diferentes níveis de entidades podem ajudar ou afetar os resultados finais do modelo. Memórias Paroquiais. A Tabela 6 mostra os resultados no nível de entidade. A Albertina em português europeu tende a alcançar consistentemente os melhores resultados no score F1 em muitas categorias, o que destaca que as características específicas da língua no modelo podem ser vantajosas para o corpus. Modelos como a Albertina (PT) e (BR) mostram um melhor equilíbrio entre precisão e recall, levando a escores F1 mais altos em muitas categorias. Esse equilíbrio é importante para aplicações práticas onde falsos positivos e negativos devem ser minimizados.
A tabela também destaca que a Albertina (PT) alcançou os resultados mais elevados, sem F1-Scores próximos de 50%, ao contrário de um classificador aleatório. Em contraste, o BERTImbau teve três classes com escores próximos de 50% e uma com resultados ainda mais baixos. Além disso, é notável que a Albertina (BR) teve cinco classes com resultados próximos de 50% (o maior número entre os modelos). No entanto, o resultado global compensou o maior número de classes com escores F1 de 70% ou mais. Observamos tendências e outliers específicos nos escores F1 em uma análise de bird's eye das categorias na Tabela 6. Em AUTWORK e PLC_MOUNT, o resultado aumentou quando o tamanho do modelo foi aumentado (ver Tabela 3 para referência sobre tamanhos de modelo), e o melhor resultado foi obtido pelo modelo Albertina português europeu. O modelo Albertina (PT) consistentemente se destaca em identificar pessoas especializadas. No entanto, um outlier emerge na categoria PER_CAT (categoria social), onde o BERTimbau apresenta um aumento notável de 17,93 pontos em relação ao segundo melhor resultado alcançado pela Albertina (PT). Este resultado inesperado pode ser atribuído ao pré-treinamento do BERTimbau no corpus brWaC [8], que inclui uma ampla variedade de conteúdo potencialmente abrangendo dados de categoria social. Isso pode ter melhorado a capacidade do BERTimbau para reconhecer corretamente entidades em contextos sociais. Além disso, enquanto o BERTimbau apresenta um leve melhoramento na categoria PER_SAINT com um aumento marginal de 0,45 pontos, este melhoramento não é significativo o suficiente para concluir decisivamente que ele se destaca melhor que a Albertina nessa categoria. Da mesma forma, entidades de localização consistentemente se saíram bem com o modelo Albertina (PT), frequentemente alcançando resultados que eram os melhores ou muito próximos. Por exemplo, em PLC_AQU, PLC_FAC e PLC_GPE, a Albertina (PT) demonstrou alta precisão e recall, com diferenças marginais do melhor desempenho variando de 0,57 a 1,38 (a maior diferença observada).
Isso sugere que a Albertina (PT) pode identificar e classificar várias entidades relacionadas ao local dentro do corpus. No entanto, é notável que o BERTimbau ocasionalmente superou a Albertina (PT) em categorias específicas, como TIM_CRON, indicando sua capacidade de excelência em contextos onde referências temporais são cruciais. Essas nuances de desempenho destaquem as fortes habilidades de cada modelo em categorias de entidades diferentes. Destaca a importância de considerar contextos específicos e o equilíbrio nas distribuições de resultados ao avaliar a eficácia em diferentes tipos de entidades.

 Avaliação de Modelos de Linguagem Portuguesa para NER

CATEG BERTimbau XLM-R Albertina (BR) Albertina (PT) P R F1 P R F1 P R F1 P R F1 DATA 100,00 94,23 97,02 96,00 96,97 97,92 95,92 96,91 98,00 100,00 98,99 98,99

... (continua)
00 54.55 70.59 PRODUTOsistema 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 Tabela 7. Resultados do tipo de nível de entidade do UlyssesNER-Br para textos legislativos brasileiros. A Tabela 5 apresenta os resultados no nível de entidade. Similarmente às conclusões apresentadas na Seção 6.1, observamos que os resultados mais altos estão bem distribuídos entre os modelos BERTimbau, XLM-R, Albertina (BR) e Albertina (PT), com 4, 6, 6 e 8 instâncias de alcançar os melhores escores, respectivamente. Além disso, enquanto há várias instâncias de resultados competitivos próximos entre os modelos, ter o pior resultado em uma categoria não implica desempenho ruim, como evidenciado por cinco instâncias em que o pior resultado ainda excede 89%. Isso demonstra que os modelos aprenderam eficazmente as representações de muitas entidades alvo. Em relação às classes específicas, a classe Data (DATA) é notavelmente bem aprendida por todos os modelos, com a variabilidade mínima do F1-Score variando entre 97,02% e 98,99%. É um resultado esperado, pois Data é a classe mais grande, com 433 exemplos no conjunto de treinamento, e apresenta padrões específicos em sua formatação. A classe Evento (EVENTO) também foi aprendida por todos os modelos, embora os resultados exibissem maior variabilidade em comparação à classe Data, variando entre 75,00% e 87,50%. O Evento é uma classe minoritária no conjunto de treinamento, composta apenas por 9 exemplos. O BERTimbau Large destacou-se ao alcançar o melhor desempenho para essa classe. Isso contrasta com achados anteriores [13], que demonstraram que o BERTimbau Base não aprendeu Evento, resultando em um F1-Score de 0%. Essa observação apoia a hipótese discutida na Seção 6.1 de que o tamanho do modelo, em vez da variação idiomática, desempenha um papel crucial no desempenho nesse corpus. As classes de localização (LOCAL) exibiram resultados e tendências variáveis. Primeiramente, lugares concretos (LOCALconcreto) foram entidades bem aprendidas, com Escores F1 variando entre 89,25% e 92,31%, demonstrando um aumento no desempenho com modelos maiores.
Locais concretos possuem semântica específica relacionada a marcos geográficos, o que ajuda os modelos a identificar contexto e padrões. Em contraste, os locais virtuais (LOCALvirtual) obtiveram resultados piores, com Escores F1 entre 29,41% e 51,61%. Um local virtual é uma entidade mais diversa que abrange diferentes tipos de locais, que não necessariamente compartilham o mesmo significado e podem incluir entidades como jornais e páginas da internet [16], tornando mais desafiadora a aprendizagem para os modelos. Além disso, as anotações nessa categoria mais ampla podem levar a conflitos semânticos. Por exemplo, o termo Jornal Diário Catarinense (jornal Diário Catarinense) na sentença "Destaco que nos anos 90, o Jornal Diário Catarinense realizou uma pesquisa popular que colocou entre os 20 catarinenses do século XX" pode ser entendido como tanto um local quanto uma organização geral [14], o que é desafiador para o modelo discernir. As organizações (ORG) mostraram uma variação ligeira nos resultados em diferentes modelos. Os resultados das classes especializadas de organizações destacam a relação entre o número de exemplos de treinamento em cada classe e seus respectivos Escores F1. Como visto na Tabela 2, organizações governamentais (ORGgovernamental), organizações não governamentais (ORGnão governamental) e partidos políticos (ORGpartido) tinham 324, 88 e 23 exemplos no conjunto de treinamento, respectivamente. Apesar do número de exemplos, os resultados foram inversamente relacionados, com a classe minoritária alcançando o melhor Escore F1 e a classe majoritária também alcançando um alto score, como mostrado na Tabela 5. Uma possível explicação para esse comportamento é a especificidade da classe. Por exemplo, embora a classe de organização governamental pertença a um domínio específico, abrange uma ampla gama de entidades, desde guardas municipais até a Câmara dos Deputados. O mesmo se aplica às instituições não governamentais.
Conversamente, os partidos políticos costumam aparecer em contextos mais específicos ou compartilhar características semelhantes em seus nomes, como seguir o nome de um deputado ou incluir a palavra "partido" em seus títulos. As classes relacionadas à Fundação de Direito — Apelido de Norma Jurídica (FUNDapelido), Norma Jurídica (FUNDlei) e Propostas de Lei (FUNDprojetodelei) — apresentaram níveis variados de aprendizado. As discrepâncias nos resultados são provavelmente atribuídas ao número de exemplos de treinamento disponíveis para cada classe. Por exemplo, Apelido de Norma Jurídica e Norma Jurídica alcançaram altos escores F1 de 97,00% e 94,38%, respectivamente, com 123 e 359 exemplos. Em contraste, Propostas de Lei, apesar de ter um formato específico, como o exemplo “PEC 187/2016”, teve apenas 8 exemplos no conjunto de treinamento. Os resultados mais elevados foram concentrados no BERTimbau nas classes de Fundação de Direito. As classes especializadas de produtos jurídicos (PRODUTO) apresentaram uma tendência de desempenho melhorado com modelos maiores. Isso foi particularmente evidente na classe de outros produtos (PRODUTOoutros), que alcançou seus melhores resultados com Albertina (BR). Essa classe mostrou um desempenho forte, especificamente nos modelos em português, com o resultado mais alto sendo a versão brasileira do português de Albertina. Quanto à classe de produto do programa (PRODUTOprograma), os melhores resultados foram compartilhados entre XLM-R e ambas as variações de Albertina, com escores idênticos em todos os métodos. Essa consistência exige uma investigação mais detalhada sobre a distribuição da classe e o aprendizado do modelo. Por fim, a classe de produto do sistema (PRODUTOsistema) teve apenas um exemplo no conjunto de teste, e o sistema não apresentou resultados significativos.
Avaliando Modelos de Linguagem Grande Portugueses para NER

13 O mesmo termo apareceu na base de treinamento, tornando difícil determinar se o F1-Score de 100% reflete uma classe bem aprendida ou se o modelo se especializou em reconhecer esse termo específico. Análise comparativa. Em relação aos modelos, Albertina (PT) demonstrou desempenho superior no corpus de Memórias Paroquiais devido ao seu treinamento em português europeu, especialmente no reconhecimento de entidades de pessoa e local. BERTimbau destacou suas habilidades em identificar entidades influenciadas por seus dados de pré-treinamento diversificados, alcançando resultados comparáveis a modelos maiores, especialmente no corpus de Textos Legislativos Brasileiros. XLM-R e Albertina (BR) também mostraram desempenho competitivo em vários tipos de entidades, destacando sua versatilidade. O foco em entidades específicas do português europeu permitiu que Albertina (PT) superasse outros modelos no corpus de Memórias Paroquiais. Além disso, aumentar o tamanho do modelo melhorou significativamente o desempenho nesse conjunto de dados. No corpus de Textos Legislativos Brasileiros, a diversidade e especificidade das entidades, juntamente com o número de exemplos de treinamento, influenciaram significativamente o desempenho dos modelos. Modelos maiores geralmente desempenharam melhor, especialmente em classes de entidades minoritárias e específicas. Além disso, a inclusão de dados legais no pré-treinamento da Albertina (PT) provavelmente contribuiu para seu desempenho forte nesse corpus.

7 Conclusão Investigamos a influência das variantes da língua portuguesa nos Modelos de Linguagem Pre-treinados na Reconhecimento de Entidades Nomeadas (NER) em domínios especializados. Nossos experimentos utilizaram dois conjuntos de dados distintos: um corpus histórico em português europeu e um corpus legislativo em português brasileiro. Avaliamos modelos pre-treinados especificamente para o português brasileiro (BERTimbau e Albertina PT-BR), o português europeu (Albertina PT-PT) e contextos multilíngues (XLM-R). Nossa análise penetrou qualitativamente em exemplos de classes específicas, semânticas e composições.
Vários conclusões podem ser traçadas com base nos nossos achados. Modelos adaptados ao contexto textual e linguístico, como o Albertina para o português europeu, demonstraram desempenho superior em tarefas de reconhecimento de entidades (NER), particularmente no corpus de Memórias Paroquiais. O Albertina destacou-se em precisão e recall, extraíndo entidades de textos do século 18 graças à sua alinhamento linguístico. Por outro lado, modelos maiores como o XLM-R e o Albertina mostraram um equilíbrio mais aprimorado entre precisão e recall nos textos legislativos brasileiros, realçando o papel crítico do tamanho do modelo no manejo de categorias finamente granulares. A pesquisa futura deve se concentrar na análise de erros granulares e interpretação. Compreender melhoramentos e desafios dos modelos é crucial, particularmente em ambiguidade e complexidade linguística. A exploração do desempenho de técnicas como aprendizado semi-supervisionado e métodos de transferência de aprendizado irá avançar ainda mais o campo de NER em domínios textuais específicos. Agradecimentos. Este trabalho recebeu financiamento da Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Código de Financiamento 14, da agência de financiamento brasileira CNPq e da Fundação Ciência e Tecnologia portuguesa FCT, no contexto dos projetos CEECIND/01997/2017 e UIDB/00057/2020 https://doi.org/10.54499/UIDB/00057/2020.