Ranker InRanker: Distiladores de Rankers para Recuperação de Informação sem Treinamento

Thiago Soares Laitz1,2[0000−0001−7205−2094], Konstantinos Papakostas3[0000−0002−2096−2931], Roberto Lotufo1,4[0000−0002−5652−0852] e Rodrigo Nogueira1,2,3[0000−0002−2600−6035]

1 Escola de Engenharia Elétrica e de Computação, Universidade Estadual de Campinas (UNICAMP), Campinas, Brasil
2 Maritaca AI, Brasil
3 Zeta Alpha, Países Baixos
4 NeuralMind, Brasil

Resumo. Embora os rankers neurais com milhões de parâmetros sejam componentes comuns de pipelines de recuperação de informação de ponta, eles são raramente usados em produção devido ao enorme quantidade de computação requerida para inferência. Neste trabalho, propomos um método para distilar grandes rankers em suas versões menores, com foco na eficácia fora do domínio. Introduzimos o InRanker, uma versão do monoT5 [25] distilada do monoT5-3B com eficácia aumentada em cenários fora do domínio. Nossa insígnia é usar modelos de linguagem e rerankers para gerar o máximo possível de dados de treinamento sintéticos "em domínio", ou seja, dados que se assemelham ao que será visto no tempo de recuperação. O pipeline consiste em duas fases de distilação que não requerem consultas de usuário adicionais ou anotações manuais: (1) treinamento em rótulos suaves de professor supervisionados existentes e (2) treinamento em rótulos suaves do professor para consultas sintéticas geradas usando um grande modelo de linguagem. Consequentemente, modelos como o monoT5-60M e o monoT5-220M melhoraram sua eficácia ao usar o conhecimento do professor, apesar de serem 50x e 13x menores, respectivamente. Além disso, mostramos que é possível transferir conhecimento de modelos em inglês para modelos fine-tuned em português. Os modelos e o código estão disponíveis em https://github.com/unicamp-dl/inranker

Palavras-chave: Aprendizado Profundo · Modelos de Linguagem · Recuperação de Informação.
Por exemplo, ranqueadores de parâmetros de bilhão e modelos densos alcançam posições de liderança em líderboards de benchmarks e competições de IR [10,11,12]. Esses modelos grandes aproveitam a capacidade de representação aumentada, permitindo que encodeiem características que poderiam escapar a modelos menores. No entanto, a implantação desses modelos grandes não está isenta de desafios. Os custos computacionais são substanciais, frequentemente requerendo hardware especializado como GPUs ou TPUs para operar em aplicações críticas de latência. O alto custo está diretamente relacionado ao grande número de parâmetros que esses modelos contêm, pois eles requerem hardware com capacidade de memória e cálculo alta e porque a latência escala quase linearmente com o número de parâmetros. Em um ambiente de produção, isso significa custos de operação mais altos e reduzida escalabilidade. Para abordar esses desafios, houve esforços para criar modelos mais eficientes sem reduzir significativamente a eficácia. Uma abordagem como essa é a distilação de modelos [17]. Modelos distilados, como MiniLM [32], usam um professor ou um conjunto de modelos maiores para transferir conhecimento para um modelo de aluno menor. Rosa et al. [30] mostram que o MiniLM superou a eficácia zero-shot do monoT5-base, que é um modelo seq2seq treinado para classificação binária, em tarefas de IR, apesar de ser cerca de uma ordem de magnitude menor em tamanho. Isso demonstrou que a transferência de conhecimento via distilação de modelos não apenas é viável, mas também eficaz. No entanto, a maioria das técnicas de distilação se concentrou em otimizar a eficácia em tarefas específicas de benchmark e não se concentrou em eficácia fora do domínio. Rosa et al. também mostram que, embora os modelos menores sejam capazes de alcançar resultados altos dentro do domínio, similarmente a seus counterparts maiores, a disparidade em eficácia se torna evidente em cenários fora do domínio.
Como o conceito de fora do domínio é subjetivo, definimos como uma distribuição de teste significativamente diferente da distribuição de treinamento. Um exemplo direto de um cenário fora do domínio é quando um modelo é treinado em dados relacionados à química e testado em dados legais. No entanto, reconhecemos que essa distinção se torna nebulosa em muitos cenários. Geralmente, treinar um modelo de recuperação exige rótulos duros anotados por humanos para informar qual passagem é relevante para cada consulta. No entanto, com o avanço dos Modelos de Linguagem de Grande Escala (LLMs), tornou-se possível gerar consultas sintéticas para passagens, fornecendo uma abordagem viável para a augmentação de dados [2,19,4,26,1]. Nossa obra apresenta um método para a geração de dados sintéticos projetados especificamente para destilar ranqueadores que aumentam sua eficácia fora do domínio. Presentamos o InRanker, um modelo destilado derivado do monoT5-3B [25], que utiliza as previsões do professor direta-mente com consultas sintéticas geradas a partir de um corpus fora do domínio e pares de consulta-documento reais. Efetivamente, essa abordagem converte qualquer corpus para ser em domínio, pois o modelo será treinado usando consultas do domínio alvo. Como resultado, essa abordagem leva a tamanhos de modelo reduzidos enquanto mantém melhor eficácia fora do domínio, como apresentado na Figura 1. A metodologia, resultados e experimentos de ablação são apresentados em detalhes nas seções seguintes. 2 Trabalhos Relacionados A comunidade de pesquisa tem sido usando LLMs em uma variedade de tarefas destinadas a aumentar a disponibilidade de dados e melhorar a eficácia dos sistemas existentes. Magister et al. [20] empregou texto sintético gerado pelo PaLM 540B [8] e GPT-3 175B [5] para transferir conhecimento para modelos menores como o T5.
[15] especializou com sucesso modelos de estudantes em razão multi-passo utilizando FlanT5 [9] e code-davinci-002 como professores. No entanto, todos esses trabalhos se baseiam no treinamento dos modelos de estudantes usando texto sintético em vez de rotulagem suave direta. Além disso, Muhamed et al. [21] distilou scores de atenção cruzada de um modelo de linguagem para prever taxa de cliques, obtendo resultados melhores quando expostos a características contextuais como dados tabulares. Wang et al. [32] distilou o módulo de atenção auto, que é uma parte crucial dos transformers, e transferiu conhecimento com sucesso para uma variedade de tarefas. Estudos anteriores também exploraram o treinamento de um estudante a partir de rotulagens suaves produzidas por um professor: Hofstätter et al. [18] propuseram uma abordagem de distilação de conhecimento entre arquiteturas diferentes usando a perda MarginMSE. Similarmente, Formal et al. [14] usaram a perda MarginMSE para distilar conhecimento para modelos neurais esparsos. Finalmente, Hashemi et al. [16] propuseram um método para gerar dados sintéticos para adaptação de domínio de recuperadores de passagens densas. Essa abordagem envolve criar novas consultas e uma coleção alvo, juntamente com pseudo-rótulos extraídos usando um BERT cross-encoder. No entanto, eles não avaliaram o desempenho do modelo em conjuntos de dados para os quais não foi adaptado ao domínio. A pesquisa existente se concentrou principalmente na avaliação em domínio, onde o objetivo foi aumentar a eficácia do modelo de estudante em conjuntos de dados de teste cujo domínio é semelhante aos conjuntos de dados para os quais foi treinado. Nossa pesquisa também se concentra na robustez do estudante e sua capacidade de realizar bem mesmo em cenários out-of-domain, semelhantes às habilidades do modelo de professor maior. 3 Metodologia Nossa proposta de método consiste em duas fases-chave de distilação, cada uma projetada com objetivos específicos para maximizar a eficácia zero-shot do modelo.
A primeira fase utiliza dados reais para familiarizar o modelo de estudante com a tarefa de ranking, enquanto a segunda fase utiliza dados sintéticos projetados para melhorar a generalização sem treinamento e melhorar a eficácia do modelo em um conjunto de dados específico. O conjunto de dados utilizado para distilar o InRanker consiste em conjuntos de triplets {consulta, trecho, logits}, onde os logits (rotulos suaves) originam-se de um modelo de professor treinado para a tarefa de relevância. Para a primeira etapa, escolhemos usar pares de consultas-documentos do conjunto de dados MS MARCO [23], dada sua variedade, o grande número de pares anotados e sua eficácia demonstrada em melhorar a eficácia de recuperação [29]. Em seguida, fontemos as consultas sintéticas do InPars [2], que usou um LLM para gerar consultas para os conjuntos de dados da BEIR de forma em poucas iterações. A distilação de rerankers envolve usar a perda de Erro Quadrado Médio (MSE) para alinhar os logits do professor e do estudante, como parte de uma pipeline de dois estágios ilustrada na Figura 2. A primeira fase consiste em dois passos: (1) gerar os logits do professor dados uma consulta e um trecho positivo (relevante) ou negativo (não relevante), onde os negativos são amostrados aleatoriamente usando BM25 nos candidatos top-k = 1000 e os positivos são amostrados das pares anotadas pelo homem; e (2) treinar o InRanker com as consultas e trechos como entrada usando a perda MSE para alinhar os logits do estudante com os do professor, que permanece congelado durante o treinamento. Essa abordagem pode ser benéfica pois remove a necessidade de fazer decisões difíceis sobre a relevância de um trecho, ou seja, determinar um limiar para obter rótulos de relevância binários, e em vez disso se concentra em um objetivo de alvo suave direcionado a alinhar a percepção do estudante de relevância com a do professor. A segunda fase, com um foco na eficácia sem treinamento, utiliza os mesmos dois passos.
No entanto, em vez de utilizar consultas reais obtidas por meio de um processo de anotação humana custosa, ele utiliza consultas sintéticas geradas por um LLM com base em documentos amostrados aleatoriamente do corpus. Nesse cenário, o documento positivo é o utilizado para criar a consulta, e os negativos são coletados usando o mesmo método de amostragem top-k anterior. Além disso, realizamos normalização zero-média nos logits do professor para cada par consulta-documento, independentemente da distribuição do conjunto de dados. Essa abordagem visa tornar a distribuição de dados simétrica para cada par consulta-documento do InRanker, minimizando assim o viés que o InRanker precisa aprender. Formalmente: L′ verdadeiro = Lverdadeiro −Lverdadeiro + Lfalso 2 L′ falso = Lfalso −Lverdadeiro + Lfalso 2 (1) com Lverdadeiro e Lfalso denotando os logits do professor para as classes relevantes e não relevantes, respectivamente, e L′ sendo os valores normalizados. Isso resulta na seguinte perda para cada exemplo de treinamento: LMSE = ([Yverdadeiro −L′ verdadeiro]2 + [Yfalso −L′ falso]2) (2) com Yverdadeiro e Yfalso representando os logits do estudante. Devido ao objetivo de treinamento descrito na equação (2), o modelo não mais determina a relevância de passagens e sim se concentra em replicar a saída do professor, eliminando assim a necessidade de ajustar um limiar de relevância que seria necessário para produzir uma etiqueta binária. Com essa abordagem, podemos facilmente expandir o conhecimento fora do domínio dos modelos distilados gerando novas consultas para documentos usando um LLM e fine-tuning o modelo distilado usando os logits do professor. Na seção de experimentos, demonstramos a eficácia dessa abordagem em melhorar a eficácia do modelo estudante em 16 conjuntos de dados do BEIR simultaneamente. Presentamos os hiperparâmetros utilizados para o treinamento e a curation do conjunto de dados na Anexa A, e discutimos variações da perda de treinamento na Anexa C.
1 Resultados da Distilação de Conhecimento em Inglês Distilamos o modelo monoT5-3B para modelos com parâmetros variando de 60M a 3B, utilizando combinações das seguintes configurações: Duro Humano: representando a abordagem comum para treinar ranqueadores com rótulos humanamente anotados duros (ou seja, binários) do conjunto de dados de ranqueamento de passagens MS MARCO. Nesse caso, é utilizado um perda de cross-entropy vana: LCE = −log Prelevant −log Pnão-relevante (3) onde Prelevant e Pnão-relevante são as probabilidades atribuídas pelo modelo à dupla de consulta-documento relevante e não relevante, respectivamente. Pares não relevantes são amostrados do top-1000 recuperados pelo BM25. Duro Suave: representando um passo de distilação para matching os logits de um modelo mestre e um modelo aluno, usando consultas reais (geradas por humanos) do conjunto de dados de ranqueamento como entrada, mas sem julgamentos de relevância binários para alvos. Suave Sintético: representando um passo de distilação para matching os logits dos dois modelos, similar à configuração anterior, mas usando exclusivamente consultas sintéticas geradas a partir dos correspondentes corpora BEIR com InPars [2,19]. Tabela 1. Resultados da distilação (nDCG@10) em 16 conjuntos de dados BEIR. O modelo marcado com * representa o modelo mestre. Não treinamos InRanker-3B com rótulos suaves humanos devido a restrições computacionais. Configurações de Treinamento Modelo Duro Humano Duro Suave Suave Sintético Média (1) monoT5-60M ✓ 0,4125 (2) ,→w/ suave humano ✓ 0,4356 (3) InRanker-60M ✓ ✓ 0,4807 (4) monoT5-220M ✓ 0,4638 (5) ,→w/ suave humano ✓ 0,4870 (6) InRanker-220M ✓ ✓ 0,5008 (7) monoT5-3B* ✓ 0,5174 (8) InRanker-3B ✓ ✓ 0,5253 A partir da Tabela 1, vemos que ambos os passos de distilação foram essenciais para melhorar a média do score nDCG@10 em comparação ao modelo treinado exclusivamente com rótulos humanos duros do MS MARCO.5 Como resultado, InRanker-60M (linha 3) e InRanker-220M (linha 6), apesar de serem 50x e 13x menores que o modelo mestre, foram capazes de melhorar sua eficácia no benchmark BEIR significativamente.
Além disso, modelos treinados exclusivamente com soft labels do MS MARCO (linhas 2 e 5) apresentaram um aumento na eficácia em comparação com o treinamento apenas com labels duras (linhas 1 e 4), corroborando achados de estudos anteriores sobre a eficácia de soft labels [17,18,14,16]. Além disso, observamos um aumento na eficácia mesmo no treinamento de auto-distilação (linha 8), onde o aluno aprende soft labels geradas por si mesmo. Hypotetizamos que a melhoria se deve ao conhecimento adicional fornecido pelo modelo de linguagem usado para gerar as consultas sintéticas. Não fornecemos resultados para o modelo de 3B treinado em soft labels humanas e sintéticas simultaneamente devido a custos computacionais. Além disso, na Tabela 2, apresentamos uma comparação de eficácia entre In-Ranker, Promptagator [13] e RankT5 [33]. Embora tenhamos usado o monoT5-3B como professor para nossos experimentos, o que tem uma eficácia mais baixa em média quando comparado a Promptagator ou RankT5-3B, nosso método é modelo-agnostic e, portanto, alguém poderia usar um modelo de professor mais forte e antecipar resultados ainda mais fortes. No entanto, o In-Ranker permanece competitivo em ambos os grupos de modelos de 220M e 3B de parâmetros, superando os outros dois baseline em 6 dos 10 conjuntos de dados avaliados, apesar do escore médio não refletir isso devido a Promptagator e RankT5 atingirem um escore significativamente mais alto em dois conjuntos de dados: ArguAna e Touché. 5 Os resultados por conjunto de dados estão apresentados no Apêndice D. In-Ranker: Distilladores de Rank para Pesquisa de Informação Zero-shot 7 Tabela 2. Comparação da eficácia para vários modelos de re-rank, medida pela nDCG@10 no benchmark BEIR. O modelo marcado com * representa o modelo de professor usado para treinar o In-Ranker. Os escores em negrito correspondem ao melhor desempenho em um conjunto de dados específico para um tamanho de modelo dado, enquanto os escores sublinhados indicam o melhor desempenho global. Dataset In-Ranker 60M In-Ranker 220M Promptagator 110M + 110M RankT5-Enc 220M In-Ranker 3B monoT5 3B* RankT5-Enc 3B TREC-COVID 0,7775
7984 0,7620 0,7896 0,8175 0,7936 0,8237 Corpus NFC 0,3547 0,3658 0,3700 0,3731 0,3825 0,3801 0,3990 HotpotQA 0,7563 0,7742 0,7360 0,7269 0,7800 0,7595 0,7536 Climate-FEVER 0,2729 0,2914 0,2030 0,2462 0,2931 0,2835 0,2753 DBPedia 0,4451 0,4650 0,4340 0,4373 0,4762 0,4719 0,4598 ArguAna 0,2466 0,2873 0,6300 0,3094 0,4243 0,3824 0,4069 Touché-2020 0,2883 0,2897 0,3810 0,4449 0,2924 0,3026 0,4869 SCIDOCS 0,1788 0,1911 0,2010 0,1760 0,1990 0,1978 0,1918 SciFact 0,7490 0,7618 0,7310 0,7493 0,7831 0,7773 0,7600 FiQA-2018 0,4043 0,4431 0,4940 0,4132 0,5027 0,5068 0,4932 Média 0,4474 0,4668 0,4942 0,4666 0,4951 0,4856 0,5050 4,2 Resultados da Distilação de Conhecimento em Português Para avaliar mais a eficácia da técnica em diferentes línguas, avaliamos o InRanker em um conjunto de dados portugues para recuperação de informações: QUATI [6]. Em vez de usar o mesmo modelo T5, começamos com o PTT5, uma versão fine-tunada em português do T5 [7,27]. Usamos o mesmo abordagem de treinamento em dois passos como antes, mas com uma estratégia que permitiu distilar um modelo em português usando um professor inglês (monoT5-3B). Dado a disponibilidade de uma versão traduzida do MS MARCO em português [3], o primeiro passo (suave) envolveu treinar o modelo usando o texto português, enquanto se matchava as etiquetas suaves geradas pelo professor usando o texto original inglês. Essa abordagem permitiu que aproveitássemos um modelo mais forte que não está disponível em português para o processo de distilação. No segundo passo, envolvendo etiquetas suaves sintéticas do BEIR, treinamos usando o texto inglês, pois não há versão traduzida do BEIR disponível em português. Os resultados dessa avaliação são apresentados na Tabela 3. Concluímos que, similarmente ao inglês, o processo de distilação foi capaz de melhorar a eficácia dos modelos de maneira zero-shot, pois os modelos foram treinados usando apenas dados reais do MS MARCO e dados sintéticos do BEIR. Notavelmente, o InRanker-740M superou a eficácia do mT5-3.7B no QUATI.
Observe que para a avaliação QUATI utilizamos o mesmo prompt apresentado no artigo para anotar todos os documentos não julgados usando gpt-4-turbo. Portanto, todos os resultados são apresentados com um jugded@10 de 100%. Resultados adicionais usando dados sintéticos da QUATI e dados de treinamento mistos estão apresentados no Apêndice E. 4.3 Experimentos de ablação Nesta seção, apresentamos nossos experimentos de ablação destinados a validar a melhor configuração para distilar o modelo monoT5-3B em modelos menores baseados em T5, bem como o modelo T. S. Laitz et al. Tabela 3. Resultados do InRanker na QUATI, um conjunto de dados de avaliação português para recuperação de informação usando PTT5-v2 [27]. Todos os rótulos suaves sintéticos foram gerados usando os conjuntos de dados BEIR. Configurações de Treinamento Modelo Dados Duros Dados Suaves Dados Suaves Média Pontuação (1) PTT5-v2-60M ✓ 0,4225 (2) ,→com dados suaves humanos ✓ 0,4372 (3) InRanker-60M ✓ ✓ 0,5121 (4) PTT5-v2-220M ✓ 0,5662 (5) ,→com dados suaves humanos ✓ 0,5693 (6) InRanker-220M ✓ ✓ 0,6108 (7) PTT5-v2-740M ✓ 0,5917 (8) ,→com dados suaves humanos ✓ 0,6362 (9) InRanker-740M ✓ ✓ 0,6624 (10) monoT5-3B ✓ 0,4864 (11) mT5-3.7B ✓ 0,6593 para avaliar suas capacidades zero-shot. Os experimentos iniciais que conduzimos se concentraram em avaliar como a distilação afetaria a eficácia do modelo em distribuições de conjuntos de dados novos que não foram vistos durante o treinamento, ou seja, não geramos consultas sintéticas para eles. Para alcançar isso, criamos dois subconjuntos, cada contendo 8 conjuntos de dados selecionados aleatoriamente de 16 conjuntos de dados do BEIR6, que nomeamos de conjuntos de amostra 1 e 2 e usamos apenas um conjunto por experimento. Os conjuntos utilizados para treinamento são designados como a "categoria in-domínio", enquanto os conjuntos restantes, ou seja, os outros 8 conjuntos de dados que não fazem parte do conjunto de treinamento, representam a "categoria fora do domínio" (O.O.D.). Impacto da distilação de conhecimento suave na eficácia O.O.D. O primeiro experimento de ablação se concentrou em avaliar o processo de distilação inicial usando o conjunto de dados MS MARCO com rótulos suaves.
Para alcançar isso, geramos logits com monoT5-3B e treinamos modelos T5-base e T5-small por 10 épocas. Como mostrado na Tabela 4, linhas 1-2 e 5-6, ambos os modelos demonstraram uma melhoria em suas pontuações nDCG@10 em comparação com o baseline, que foi treinado com rótulos duros da MS MARCO. Notavelmente, a pontuação global aumentou em ambos os cenários, mesmo que os modelos não tenham sido expostos a nenhuma passagem BEIR durante essa fase.7 Adicionando alvos sintéticos suaves como uma segunda fase de distilação Para o próximo experimento, aplicamos uma segunda fase de distilação com rótulos suaves sintéticos em cima do modelo que adquirimos da última fase (monoT5 w/ soft human 6 Listamos sua composição exata no Apêndice B. 7 Os resultados individuais para cada conjunto de dados são apresentados no Apêndice D. Em seguida, adicionamos alvos sintéticos suaves como uma segunda fase de distilação para o modelo que adquirimos da última fase (monoT5 w/ soft human 6 Listamos sua composição exata no Apêndice B. 7 Os resultados individuais para cada conjunto de dados são apresentados no Apêndice D. 

Tabela 4. Comparação da eficácia em domínio vs fora do domínio do nosso método, medida pela nDCG@10. O modelo marcado com * representa o modelo mestre usado para o processo de distilação. Configurações de Treinamento Conjunto de Exemplos 1 Conjunto de Exemplos 2 Modelo T5 Humanos Duros Humanos Suaves Sintéticos Suaves Em domínio Fora do domínio Em domínio Fora do domínio (1) 60M (monoT5) ✓ 0,4141 0,4109 0,4817 0,3434 (2) 60M ✓ 0,4422 0,4290 0,5124 0,3587 (3) 60M (InRanker) ✓ ✓ 0,4768 0,4716 0,5558 0,3852 (4) 60M ✓ ✓ 0,4475 0,4587 0,5355 0,3617 (5) 220M (monoT5) ✓ 0,4647 0,4629 0,5475 0,3801 (6) 220M ✓ 0,4867 0,4873 0,5692 0,4048 (7) 220M (InRanker) ✓ ✓ 0,4945 0,5028 0,5874 0,4083 (8) 220M ✓ ✓ 0,4905 0,4942 0,5832 0,3941 (9) 3B* (monoT5) ✓ 0,5095 0,5253 0,6053 0,4295 rótulos). Para isso, usamos as 100K consultas sintéticas geradas por InPars para cada conjunto de dados indicado como "em domínio" e treinamos por 10 épocas.
A seguir está a tradução do texto científico para o português do Brasil:

 Como mostrado na Tabela 4, linhas 3 e 7, embora fosse esperado que os conjuntos de dados intra-dominio tivessem um aumento nos scores de nDCG@10, observamos que os conjuntos de dados extra-dominio também tiveram melhorias, sugerindo que as capacidades de generalização do modelo foram melhoradas. Usando alvos humanos duros para a primeira fase de distilação Finalmente, investigamos o impacto de pular a primeira fase de distilação nos logits MS MARCO e, em vez disso, começar com um modelo treinado em rótulos humanos duros (monoT5-small e monoT5-base) e treinar diretamente usando os alvos sintéticos BEIR suaves. Como podemos ver na Tabela 4, linhas 3-4 e 7-8, ao compararmos com o modelo treinado usando os alvos humanos suaves, a eficácia geral foi reduzida. A partir disso, concluímos que a etapa de distilação que inclui os alvos humanos suaves em MS MARCO é benéfica, pois melhora a eficácia do modelo em ambos os cenários intra-dominio e extra-dominio. Limite superior para distilação suave Para estimar o limite superior da eficácia que esses modelos poderiam atingir através da distilação, repetimos o processo usando consultas reais da BEIR (ou seja, as consultas de avaliação) em vez das sintéticas. Os resultados apresentados na Tabela 5 mostram que, para ambos os tamanhos de modelo, houve um aumento na eficácia para os conjuntos de dados intra-dominio, pois o modelo foi exposto às consultas de avaliação durante o treinamento. No entanto, também observamos um aumento na eficácia para os conjuntos de dados extra-dominio, indicando que as consultas sintéticas usadas para o treinamento poderiam ser melhoradas.
4202 5 Conclusão

Este artigo apresenta um método para destilar o conhecimento de modelos de recuperação de informações e melhorar o uso de dados sintéticos, com o objetivo de melhorar a eficácia fora do domínio de estudantes. O estudo revela que, por meio desse processo de destilação de conhecimento, modelos menores podem alcançar resultados comparáveis ao professor, mesmo no contexto de transferência de conhecimento multilíngue. Essa abordagem é particularmente significativa para aplicações em que os recursos computacionais são limitados, em ambientes de produção ou para línguas com falta de modelos disponíveis que possam servir como professor. A metodologia envolve dois passos de destilação: (1) uso de um corpus humano-curado e (2) uso de dados sintéticos gerados por um LLM. Consequentemente, nosso trabalho demonstra que é possível melhorar as capacidades de um reranker em domínios específicos sem requerer rótulos humanamente anotados adicionais. Finalmente, observamos que a geração de consultas sintéticas pode ser melhorada, pois as consultas reais alcançaram uma melhor eficácia fora do domínio em comparação ao modelo treinado exclusivamente com dados sintéticos. No entanto, o método apresentado tem limitações. Especificamente, não está claro como adaptar a perda proposta para treinar recuperadores densos, que tipicamente usam uma perda contrastiva. Declaração de Interesses. Os autores não têm interesses competitivos relevantes para declarar que sejam relevantes ao conteúdo deste artigo.