Aroeira: Um Corpus Curado para a Língua Portuguesa com um Grande Número de Tokens

Thiago Lira, Flávio Cação, Cinthia Souza, João Valentini, Edson Bollis, Otávio Oliveira, Renato Almeida, Márcio Magalhães, Katia Poloni, André Oliveira e Lucas Pellicer
Instituto de Ciência e Tecnologia Itaú-Unibanco (ICTi), São Paulo, Brasil {thlira15, flavio.nakasato}@gmail.com, {cinthia.mikaela-souza, joao.valentini22, edson.bollis, otavio.rodrigues-oliveira, renato-augusto.almeida, marcio.chiara-magalhaes, katia.poloni, andre.seidel-oliveira, lucas.pellicer}@itau-unibanco.com.br

Resumo. A ênfase na construção de conjuntos de dados extensivos para treinar modelos de linguagem grande (LLM) tem aumentado recentemente, e a literatura atual apresenta predominantemente conjuntos de dados para línguas de alto recurso, como o inglês e o chinês. No entanto, há uma escassez notável de corpora de alta qualidade para a língua portuguesa. Para abordar essa limitação, propomos Aroeira, um corpus curado projetado explicitamente para treinar modelos de linguagem grande na língua portuguesa, com foco na variante brasileira. O Corpus Aroeira consiste em 100 GB de textos provenientes de várias plataformas da internet, processados por uma pipeline abrangente para garantir qualidade superior. A pipeline manipula o download, extração de texto, identificação de língua, aplicação de filtros de qualidade e viés, e armazenamento, todos projetados para a língua portuguesa. O corpus resultante contém 35,3 milhões de documentos e mais de 15,1 bilhões de tokens, ultrapassando o maior corpus disponível anteriormente nessa área.

1 Introdução
A maioria dos métodos de processamento de linguagem natural (NLP) modernos, incluindo Modelos de Linguagem Grande (LLM), dependem de conjuntos de texto extensivos para treinamento preciso e adaptação de pesos [18]. Corpora de treinamento a grande escala, ou corpora de pré-treinamento, são fundamentais para o desenvolvimento de modelos fundacionais, que servem como base para adaptações específicas de tarefa [7].
Pipelines de treinamento de LLM de vanguarda utilizam vários tipos de conjuntos de dados: (i) corpora de treinamento prévio para adquirir estrutura linguística, sintaxe e semântica; (ii) conjuntos de dados de fine-tuning de instruções para melhorar a capacidade do modelo em seguir instruções; (iii) conjuntos de dados de preferência para classificar respostas; e (iv) conjuntos de dados de avaliação para medir o desempenho do modelo [23]. Pesquisas recentes mostram que o tamanho e a diversidade de corpora de treinamento prévio têm um impacto significativo no desempenho de LLM [14, 18]. A maioria dos conjuntos de dados de treinamento prévio está disponível em inglês e chinês, que são línguas de alta recursos, enquanto outras línguas têm significativamente menos tokens [23]. Embora corpora multilíngues possam ajudar a mitigar a escassez de dados para línguas de baixa recursos, esses conjuntos de dados são frequentemente desbalanceados, favorecendo línguas de alta recursos [17]. Essa desbalanceamento afeta o desempenho de modelos multilíngues para línguas menos representadas e modelos treinados em corpora multilíngues não performam tão bem quanto aqueles treinados em corpora monolíngues [34]. Portanto, é essencial treinar ou fine-tuning modelos nas línguas-alvo para capturar nuances linguísticas, estruturas e conhecimento específico ou cultural [28]. Uma consequência direta desse cenário é a necessidade de disponibilizar corpora de texto plano de alta qualidade para encorajar pesquisas sobre línguas específicas de modelo e o desenvolvimento de abordagens melhor desempenhadas. Portanto, introduzimos Aroeira: um corpus de língua portuguesa específica, curado, composto por aproximadamente 100 GB de texto. O conteúdo foi extraído de páginas web recentes do Common Crawl1 (CC) (até 2023) e foi completamente curado para remover tags de web, garantindo qualidade e filtragem de viés. Ao melhor de nossa conhecimento, Aroeira é o maior corpus de língua portuguesa curado disponível até o momento. Ele tem o potencial de influenciar novos estudos de conjuntos de dados de fine-tuning de instruções e avaliação, enquanto orienta o desenvolvimento de conjuntos de dados de preferência e modelos grandes.
Aroeira foi criada com base em uma dupla-pipeline inspirada em [30]. A pipeline compreende dois passos-chave: gerenciamento de qualidade de dados e garantia de segurança de conteúdo. Esses passos garantem o tamanho e a qualidade necessários para um corpus português treinar LLMs seguras de forma eficaz. Como parte do passo de segurança de conteúdo, investigamos técnicas para filtrar conteúdo perigoso e mitigar bias em nossos corpora [16]. Esse esforço resultou em um dicionário de palavras português personalizado, que abrange palavras ofensivas, termos, expressões e frases que incluem sexismo, homofobia, ableismo, racismo, discurso de ódio e preconceito político, religioso e regional [13, 24, 26]. Destacamos nossas principais contribuições: – Introdução de Aroeira, um corpus português de 100 GB proveniente de fontes de internet diversificadas. Nossa base de dados supera o maior corpus disponível atualmente para treinar modelos de linguagem em português em termos de tamanho, qualidade e representatividade. – Desenvolvimento de uma pipeline dupla-parametrizável, que inclui: download, extração, identificação de linguagem, filtragem de qualidade, armazenamento de texto no passo de dados; filtragem de conteúdo sexual, dados tóxicos e bias no passo de segurança de conteúdo. – Criação de um dicionário para filtrar termos prejudicados e mitigar bias social na língua portuguesa. Este artigo está organizado da seguinte maneira. Na Seção 2, apresentamos trabalhos relacionados à extração de corpus. Seções 3 e 4, descrevemos a metodologia para gerar o corpus e a configuração de hiperparâmetros utilizados nos filtros de qualidade, respectivamente. Na Seção 5, analisamos a volumetria de Aroeira em termos de distribuição por ano, domínios de conhecimento, comprimento de documentos e outros resultados relevantes. Finalmente, na Seção 6, apresentamos conclusões e trabalhos futuros. 1 Disponível em: https://commoncrawl.org/ Aroeira 3 2 Trabalhos Relacionados O maior corpus de língua portuguesa é o BrWac [35], que tem aproximadamente 25 GB de dados textuais distribuídos em 3,53 milhões de documentos totalizando 2,68 bilhões de tokens.
Outro grande corpus é o Carolina 1.2 Ada [11], que contém aproximadamente 2,11 milhões de documentos e um total de 11 GB de dados textuais. Quando compararmos este corpus com os corpora de outras línguas, os gaps tornam-se evidentes. Gao et al [14], por exemplo, propõem The Pile, um corpus com 825 GB de textos em inglês. O corpus é derivado de várias fontes de dados, incluindo artigos científicos, documentos de patente e fóruns. Um trabalho inspirador para Aroeira é o Colossal Clean Crawled Corpus (C4) [30], um corpus inglês apenas. O C4 foi criado usando dados extraídos do Common Crawl (CC) em abril de 2019 e compreende aproximadamente 750 GB de texto limpo em inglês. Similarmente à nossa abordagem, eles aplicam filtros aos dados brutos. O CLUECor-pus2020 [36] foi construído usando dados limpos do CC, resultando em um corpus de treinamento pré-linguístico chinês de alta qualidade de 100 GB e 36 bilhões de tokens. MassiveText [29] é uma coleção de grandes conjuntos de dados em inglês criados com dados de fontes diferentes. O MassiveText contém 2,35 bilhões de documentos, equivalentes a 10,5 TB de texto. Mais recentemente, Sabiá [28] aplicou uma metodologia de filtragem semelhante ao MassiveText para a seção portuguesa do dataset ClueWeb [27] e conseguiu recuperar um conjunto de dados curado. WuDaoCorpora [38] é um corpus chinês de 3 TB com 1,08 trilhões de caracteres Hanzi coletados de 822 milhões de páginas da web. É também digno de menção que uma tendência atual é a proposta de corpora multilíngues. Os autores do C4Corpus [17] apresentam a construção de um corpus de 12 milhões de páginas da web contendo mais de 50 línguas, incluindo o português. O inglês tem um volume de 7,7 milhões (64,2%) de documentos, enquanto o português tem apenas 0,3 milhões (2,5%). RedPajama [10] é um grande corpus multilíngue, contendo 100 bilhões de documentos de texto extraídos de 84 snapshots do CC. Foram aplicadas sinalizações de qualidade em 30 bilhões de documentos e foi realizada deduplicação em 20 bilhões de documentos. Afirma ter inglês (69,8%), alemão (9,2%), espanhol (8,8%), francês (7,8%) e italiano (4,4%).
Vemos que os corpora disponíveis em inglês e chinês possuem um grande volume de dados, facilmente superando corpora em português e outras línguas. No entanto, sabemos que a quantidade de informações disponíveis na internet em inglês e chinês é maior do que em português. Outro aspecto importante é os viés presentes em corpora e textos. A linguagem é uma via altamente relevante para manifestar hierarquias sociais, conceitos pré-estabelecidos e formas padrões de tratamento [6]. Várias esforços estão sendo feitos para avaliar os viés de dados e como eles impactam o comportamento de modelos de linguagem. O artigo [24] analisou 93 grupos sociais que recebem tratamento estigmatizado por modelos de processamento de linguagem. O trabalho [25] criou o StereoSet para medir o tratamento estereotipado em certos grupos étnicos, e o artigo [26] desenvolveu um conjunto de dados de benchmark para medir viés relacionado ao gênero, raça, idade, orientação sexual e outros. Esses aspectos são relevantes em um contexto com motivações normativas fortes e a necessidade de criar inteligência artificial responsável. Muitas maneiras existem para mitigar viés de texto, como ampliação de dados, filtragem de conteúdo, rebalanceamento, mascaramento e muitos outros [13]. Nossa obra utiliza o conceito estudado por [16], onde a filtragem de conteúdo sensível pode resultar em modelos com tratamento mais equitativo para diferentes grupos étnicos. O trabalho especificamente utiliza a co-ocorrência de palavras no processo de filtragem. Com base nos trabalhos passados, podemos ver que, em geral, eles se concentram em criar corpora para treinar modelos de linguagem para tarefas de linguagem de alta recursos. Assim, há uma necessidade de criar um corpus em português, pois o volume de grandes modelos de português brasileiro aumentou drasticamente recentemente. Podemos citar Bertimbau [33], PTT5 [9], Bertaú [12], Sabiá [28, 4], Cabrita [22] e Bode [15]. 3 Aroeira Nesta seção, detalharemos os passos da criação do corpus (doble-pipeline), que é dividido em dois objetivos, (i) coletar (Data Pipeline) e (ii) garantir a segurança do conteúdo (Content Safety Pipeline).
Nossa pipeline inteira contém nove etapas: coleta e amostragem de dados, extração de texto, identificação de linguagem, deduplicação e filtros de qualidade no Pipeline de Dados, e filtro de conteúdo sexual, filtro de dados tóxicos, filtro de viés e categorização no Pipeline de Segurança de Conteúdo. A Figura 1 apresenta o fluxo de trabalho integral. Fig. 1. Pipeline dupla: o Pipeline de Dados contém coleta e amostragem de dados, extração de texto, identificação de linguagem, deduplicação e filtros de qualidade; e o Pipeline de Segurança de Conteúdo abrange filtro de conteúdo sexual, filtro de dados tóxicos, filtro de viés e categorização.

3.1 Coleta e amostragem de dados

A etapa de coleta e amostragem de dados envolve baixar e extrair texto em português de arquivos Web ARChive (WARC) brutos. Todos os dados são provenientes do Common Crawl (CC), que contém petabytes de conteúdo de internet raspado de milhões de páginas da web. Utilizamos os arquivos HTTP brutos como material inicial, dos quais extrai e filtra texto em português, como detalhado nas Subseções 3.2 e 3.3. O CC organiza seus conjuntos de dados por data, cada um compreendendo milhares de fragmentos individuais de conteúdo raspado. Amostramos fragmentos de conjuntos de dados que abrangem o período de 2015 a 2023, priorizando dados mais recentes.

Aroeira 5

3.2 Extração de texto

Essa etapa computacional envolve usar máquinas em nuvem em paralelo. Essas instâncias baixam e processam arquivos brutos, extraíndo texto do HTML e filtrando para texto em português. Os dados resultantes são então processados adicionalmente usando uma máquina única contendo um banco de dados de chave-valor para deduplicação, como explicado na Subseção 3.4. Optamos por trabalhar com arquivos WARC para garantir melhor qualidade de texto, o que inclui manipular arquivos HTML brutos e extrair textos próprios. Uma biblioteca Python chamada Trafilatura [5] extraiu apenas texto de linguagem natural dos arquivos HTML. Os metadados das páginas da web foram salvos para uso posterior na pipeline.

3.3 Identificação de linguagem

Aproximadamente 0,2% das páginas em cada fragmento estão em português.
Para filtrar essas páginas, é necessário detectar automaticamente a língua em que foram escritas. Como utilizados por [14] e [1], empregamos o modelo pre-treinado fastText da Meta AI, que pode detectar 176 línguas. Para cada página baixada, o texto é extraído utilizando a biblioteca Trafilatura [5] e os modelos fastText2 [20] são utilizados para determinar a língua. As páginas identificadas como portuguesas com a maior probabilidade pelo fastText foram selecionadas.

3.4 Deduplicação

O objetivo desse passo é remover dados duplicados do corpus. Para atingir esse objetivo, utilizamos dois abordagens de deduplicação. A primeira é uma abordagem de nível de página, que identifica e remove páginas com URLs duplicados. A segunda é a abordagem de nível de documento, que visa remover documentos significativamente sobrepostos. Empregamos o algoritmo MinHashLSH para calcular a similaridade de Jaccard entre documentos, considerando se a similaridade entre dois documentos excede 0,7 [29].

3.5 Filtros de qualidade

Uma quantidade significativa de dados disponíveis na internet pode ser insuficiente em termos de qualidade para a formação de modelos linguísticos. Alguns exemplos incluem texto gerado automaticamente e texto não escrito para consumo humano [29]. Este passo visa manter apenas páginas escritas por humanos para humanos. Para atingir isso, aplicamos uma série de dez filtros de qualidade: – Número de tokens: Remove páginas com menos de um número mínimo de tokens (neste trabalho, utilizamos o mesmo tokenizador empregado pelo GPT-2), pois textos com contagens de tokens baixas geralmente não são informativos; – Número de palavras: Remove páginas que não atendem a limites especificados de palavras superiores e inferiores, excluindo pontuação e caracteres especiais; – Razão de tipo-token (TTR): A razão de palavras únicas (tipos) para palavras totais (tokens) [31]. A TTR [37] serve como indicador de qualidade de texto; – Razão de símbolos-palavra: Remove páginas cuja porcentagem de símbolos-palavra excede limites.
Qualquer caractere especial é considerado um símbolo; – Símbolos no início do texto: Remove páginas com um número excessivo de símbolos no início do texto; – Palavras-chave: A presença de palavras-chave pode indicar coesão textual [29]; – Repetição de N-grama: Repetição excessiva de sentenças, parágrafos ou N-gramas indica baixo conteúdo informativo [29]; – Número de sentenças: Remove páginas com menos de um número especificado de sentenças; – Lorem ipsum: Remove páginas contendo o termo “Lorem ipsum” [30]; – Palavras válidas: Remove páginas cuja porcentagem de palavras encontradas em um dicionário de língua está abaixo de um limiar especificado. Os limiares para cada filtro são detalhados na Seção 4.

3.6 Filtro de conteúdo sexual

Para manter a integridade do corpus, um filtro foi aplicado para remover conteúdo sexual da dados. Verificamos se uma URL estava presente na lista de bloqueio da Université Toulouse 13 (UT1) para cada página coletada. Como observado por [2], a lista de bloqueio da UT1 é uma compilação extensa de listas de bloqueio frequentemente usadas para controle de acesso à internet em escolas. Foi desenvolvida com a ajuda de sistemas automatizados e contribuidores humanos e atualmente inclui 3,7 milhões de entradas. Para este trabalho, utilizamos uma versão filtrada desta lista adaptada para websites brasileiros. Deve ser notado que este filtro apenas exclui websites marcados como conteúdo adulto. Para o conteúdo restante, selecionamos aleatoriamente 25.000 exemplos e usamos um modelo Mistral 7B [19] para extrair termos sexuais pejorativos. Esses termos foram então revisados por humanos e usados como o filtro final de conteúdo sexual.

3.7 Filtro de dados tóxicos

Neste passo, nosso objetivo é identificar e remover conteúdo potencialmente tóxico. A definição de toxicidade é um comentário rude, desrespeitoso ou irrazoável provável de incitar uma discussão [13]. Nossa filtro compreende um dicionário de insultos e termos pejorativos.
Avaliamos matches exatas de palavras do dicionário com termos do documento e removemos documentos que ultrapassaram uma porcentagem específica de palavras no dicionário. O dicionário utilizado para essa filtragem foi criado pela junção de duas listas de palavras4,5. Reforçamos que essa filtragem não visa eliminar todos os dados contendo palavras tóxicas, mas sim remover conteúdo com uma proporção significativa de conteúdo tóxico. 3 https://dsi.ut-capitole.fr/blacklists/index_en.php 4 https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/pt 5 https://github.com/dunossauro/chat-detox/blob/main/palavras.txt Aroeira 7 3.8 Filtro de viés Nossa obra envolve um passo para identificar e eliminar potenciais viés em texto com base em pistas contextuais. Construímos um dicionário de expressões portuguesas utilizadas em contextos de viés. Quando compilamos esse dicionário, é crucial considerar a dinâmica social e sua relação com a linguística [6]. Filtramos o corpus verificando matches exatas entre as palavras do dicionário e as do texto [16]. Vários tipos de viés social foram mapeados, incluindo gênero, religião, raça, expressões sexistas, xenofobia, homofobia, ableismo, fatofobia e política [24, 26]. 3.9 Categorização Esta fase visa categorizar cada página em um domínio de conhecimento específico. Identificamos 27 domínios de conhecimento, cobrindo várias categorias e assuntos, como: posts de blog, artigos de notícias, marketing, filmes, mídias sociais, saúde, receitas culinárias, livros, artigos científicos, política, etc. A informação relativa a cada domínio pode ser aproveitada para balancear os dados para tarefas específicas ou ampliar conjuntos de dados onde o conhecimento é deficiente. Basicamente, mapeamos as URLs de páginas diferentes para cada escopo de conhecimento e atribuímos um tópico a cada URL. Foi introduzido um categoria de texto pt-pt para diferenciar entre português brasileiro e português europeu, pois o português brasileiro predomina no conjunto de dados.
Os domínios de conhecimento identificados e sua distribuição são discutidos na subseção 5.2. 4 Teste de Configuração Qualitativa Geramos uma amostra de 1 GB de textos e analisamos a distribuição de métricas como o número de tokens, contagem de palavras e TTR. Utilizamos essa amostra para encontrar a melhor configuração para nosso pipeline duplo e produzir os conjuntos de dados resultantes. Diferentes conjuntos de valores foram testados empiricamente, e para cada um, verificamos a correção das remoções de páginas e registramos o número de páginas excluídas após a aplicação dos filtros. A análise foi realizada nesta amostra para encontrar a configuração ótima. Além disso, realizamos análises qualitativas para verificar a adequação das remoções. Este teste qualitativo nos ajudou a definir valores satisfatórios que devemos filtrar para obter conteúdo com boa qualidade textual, ou seja, um texto que seja diverso em palavras, fluído, com poucas repetições e com conteúdo semântico relevante. É importante notar que também avaliamos o número de palavras potencialmente tóxicas e possíveis viéses contidos nos textos. A Tabela 1 mostra a configuração final do pipeline duplo para obter textos que excedam nossos requisitos de qualidade mínimos.

5 Resultados O corpus criado foi avaliado em relação a cinco grupos de requisitos. O primeiro requisito é que o corpus criado seja maior que o corpus existente.
2 máximo_bias_palavra Número máximo de palavras viesas: 10 para a língua portuguesa (ver Subseção 5.1). A segunda exigência é que o corpus seja diverso, ou seja, conter dados de diferentes fontes (ver Subseção 5.2). A terceira exigência é que o corpus abranja informações recentes até as menos recentes (ver Subseção 5.3). A quarta exigência é que o corpus apresente indicadores de texto de alta qualidade (ver Subseção 5.4). Finalmente, a quinta exigência é que o corpus evite introduzir ou aumentar viés. Devido ao tamanho do corpus, as Subseções 5.4 e 5.5 utilizam uma amostra aleatória de 10% para apresentar os resultados. Consequentemente, a Figura 4 e a Tabela 3 foram criadas com base nessa tamanho de amostra.

5.1 Tamanho do corpus A primeira exigência avaliada foi o tamanho do corpus. Coletamos terabytes de dados de diferentes dumps de CC. Cada dump tem aproximadamente 0,2% de textos em português. É importante notar que os documentos podem ser de baixa qualidade, conter conteúdo inadequado ou viesado, e ser duplicados devido ao CC não filtrar os dados. Portanto, um passo de limpeza do corpus foi necessário para garantir que o corpus final fosse composto apenas por documentos não duplicados e respeitasse critérios de qualidade. No final desse processo, obtivemos um corpus de 100 GB. A Tabela 2 apresenta estatísticas do corpus criado ao lado de outros corpora portugueses. Aroeira supera brWac [35] e Carolina 1.2 Ada [11] em tamanho, quantidade de documentos e número de tokens. Assim, nosso corpus é potencialmente uma fonte mais diversa em relação a textos e tokens do que os recursos disponíveis.

5.2 Domínios de conhecimento A segunda exigência avaliada foi a distribuição dos domínios de conhecimento dentro do corpus criado. Foi realizada uma mapeamento de URLs diferentes para seus respectivos domínios de conhecimento. Cada URL base foi verificada contra um dicionário. Quando não houve matches, palavras-chave foram usadas para determinar o domínio de conhecimento.
Corpus de Linguagem Tamanho #Documentos #Token Portuguese Aroeira 100 GB 35,3 milhões 15,1 bilhões brWac 25 GB 3,53 milhões 2,68 bilhões Carolina 1,2 Ada 11 GB 2,11 milhões 0,82 bilhões Inglês MassiveText 10,5 TB 2,35 bilhões 2,3 trilhões* The Pile 825 GB - - C4 750 GB - - Chinês WuDaoCorpora 3 TB 822 bilhões - CLUEcorpus2020 100 GB 2,35 bilhões 36 bilhões Multilingue RedPajama 260 TB* 100 bilhões 30,4 trilhões C4Corpus 29 GBc 12 milhões 10,8 bilhões Nota: Letras em negrito são os melhores valores, “*” indica valores calculados ou sem artigo, e “c” indica valores compactados. A distribuição dos documentos por domínio (Subseção 3.9). A figura 2 ilustra a distribuição dos documentos por domínio. A complexidade do corpus correlaciona fortemente com o desempenho de dados downstream [3]. Portanto, uma representação extensa de domínios de conhecimento pode contribuir para a geração de modelos mais robustos, potencialmente melhorando o desempenho de aprendizado por poucas tentativas em contexto [32]. A maioria dos documentos não pôde ser atribuída a um domínio específico e está marcada como NR (Não Reconhecido). Entre aqueles que foram categorizados, posts de blog e artigos de notícias foram os mais frequentes, embora outras categorias, como textos institucionais, comércio eletrônico e fóruns da internet também fossem encontrados. Os domínios de conhecimento são essenciais para avaliar a qualidade dos dados no corpus e para filtrar dados utilizados na fase de pré-treinamento de modelos de linguagem específicos de domínio. 5.3 Distribuição dos documentos ao longo do tempo Nossa terceira análise é a distribuição dos documentos do corpus ao longo do tempo. Essa análise temporal é importante para identificar possíveis viéses temporais, como textos desatualizados. Nossa corpus apresenta uma distribuição de dados recente, o que indica mais textos atuais. A figura 3 ilustra que nosso conjunto de dados compreende documentos que abrangem até 7 anos, começando em 2017. A maior parte dos dados é de 2017 a 2019, mas uma porção notável de dados recentes é de 2021, 2022 e 2023. Essa distribuição atende ao necessário de dados recentes e extensos.
Como resultado, os modelos podem treinar com informações atualizadas e incluir termos recentes e comuns utilizados em português. 5.4 Indicadores de Qualidade Usamos o valor de TTR, porcentagem de palavras simbólicas, porcentagem de palavras stopwords, palavras válidas e conteúdo tóxico como indicadores de qualidade. A figura 4 mostra os resultados obtidos. 10 Lira et al. Fig. 2. Distribuição de domínios de conhecimento. Fig. 3. Distribuição de documentos ao longo do tempo. Fig. 4. Indicadores de qualidade. Maior TTR e porcentagem de palavras válidas indicam melhor qualidade do corpus, enquanto valores mais baixos para outros indicadores também significam melhor qualidade. Temos dois métricas que indicam a diversidade do conteúdo presente em nosso corpus. O TTR indica a variabilidade de tokens em uma sentença, e nosso objetivo é que esse valor seja o mais alto possível, pois é um sinal de textos compostos por tokens variados com baixa repetição de palavras. Um limiar de TTR de 0,5 é um parâmetro de qualidade para o texto. Obtivemos uma curva de distribuição com o primeiro quartil próximo de 0,5 TTR, uma média de 0,57 e o terceiro quartil acima de 0,65. Assim, a maioria dos dados em nosso corpus atinge um valor de TTR satisfatório, o que é um forte indicador de textos não repetitivos. Outro indicador de variabilidade é a porcentagem de tokens válidos ou palavras-chave. Este indicador mede a frequência de palavras relevantes no contexto dentro do texto, e também queremos valores mais altos para textos mais diversificados e fluídos. Obtivemos uma distribuição excelente nesse indicador, com a maioria dos dados acima de 0,7. Essa distribuição é outro forte sinal de textos lexicalmente diversificados. Em contraste, os métricas para a porcentagem de símbolos e a porcentagem de stopwords são indicadores de textos menos fluídos, com muitos símbolos interrompendo o texto ou palavras comuns não adicionando valor semântico aos documentos (por exemplo, "a", "para" ou "o"). Nossa meta é minimizar esses valores o mais possível. Obtivemos nossa meta de reduzir esses valores, obtendo distribuições com valores baixos, com o terceiro quartil abaixo de 0.
2 em ambos os métricos. Este resultado é uma forte indicação de que os textos no corpus são fluidos. Finalmente, objetivamos minimizar a porcentagem de palavras potencialmente tóxicas, diminuindo para cerca de 0 (nenhuma palavra potencialmente tóxica). Os resultados mostram que alcançamos este objetivo em quase todos os textos no corpus, exceto por alguns outliers que não ultrapassam 0,2% de conteúdo tóxico.

5.5 Bias

Realizamos uma análise de co-ocorrência de palavras para identificar bias no nosso corpus. Essa técnica provou ser eficaz em demonstrar tratamento estereotipado de um grupo social específico [13], o que não desejamos. É importante notar que esse método é apenas uma das abordagens de mitigação e o corpus pode ainda exibir bias. Analisamos três grupos de bias, como mostrado na Tabela 3, que são: (i) Gênero, (ii) Religião e (iii) Raça. Selecionamos palavras diferentes para cada grupo e analisamos o contexto em que essas palavras foram inseridas. Escolhemos termos representativos indicativos de grupos sociais e conduzimos uma análise focalizando nos que apresentam as frequências de co-ocorrência mais altas. Isolamos os termos "Homem" e "Mulher" para avaliar o bias de gênero. Selecionamos "Ateu", "Cristão", "Budista", "Evangelista", "Judeu", "Muçulmano" e "Ateísta" para o bias religioso. Por fim, destacamos as palavras "Branco", "Preto", "Asiático" e "Hispano" para o bias racial. As Tabelas 3 representam respectivamente os resultados para bias de gênero, religioso e racial. Não há tratamento estereotipado ou comportamentos perigosos descritos na literatura em grupos analisados, como associações com crime, renda e outros [16]. Além disso, as palavras selecionadas entre os vários grupos sociais são muito semelhantes, o que sugere um tratamento mais equânime em nosso corpus proposto.

6 Conclusão

Recentes estudos demonstraram melhoras significativas no desempenho de modelos de linguagem treinados em grandes corpora [8, 14]. Consequentemente, o interesse em criar grandes conjuntos de dados cresceu.
A maioria dos estudos existentes se concentra em línguas de alto recurso como o inglês e o chinês, com esforços consideráveis para desenvolver corpora multilíngues. No entanto, há uma necessidade imperiosa de desenvolver grandes conjuntos de dados para línguas de baixo recurso. Este trabalho visa abordar essa lacuna ao desenvolver modelos de linguagem para línguas de baixo recurso, especificamente o português. Criamos o maior corpus curado para treinar ou pré-treinar modelos de linguagem portugueses. Para alcançar isso, implementamos um processo de dupla pipeline para extrair dados garantindo a segurança do conteúdo. O processo inclui download, extração de texto, identificação de língua, deduplicação, filtro de qualidade, filtro de conteúdo sexual e toxicidade, filtro de viés, categorização e armazenamento. Esse esforço envolveu coletar terabytes de dados.
Aqui está a tradução do texto científico para português do Brasil:

Palavra-chave 1 2 3 4 5 6 7 8 9 10 Gênero Mulher homem dia ano vida filho mulheres mãe casa contra marido
Mulher homem deu ano vida mundo filho aranha bem dia porque

Religião Ateu deu fé cristã toda religião pessoa vida porque estado religioso
Cristão deu vida toda mundo Cristo igreja deve Jesus pode amor

Budista templo monge meditação zen prática budismo ano tradição todo sobre
Evangelico pastor igreja meio dia cristão deu ano hospital católico Brasil

Judeu povo estado Jesus todo deu Israel dia história cristão outro
Muçulmano mundo cristão todo país árabe judeu sobre estado pode outro

Umbanda Umbanda religião grande dia bom todo católico sobre verdadeiro fim
Raça Branco preto rio castelo cor azul core vinho vermelho verde primeiro
Negro rubro rio buraco branco movimento lado sobre ano Brasil humor

Hispânico mundo espanhol qualificado negro trabalho sobre grande futebol público México
(México) País da Ásia do Sudeste, mercado, continente, países, China, sul, leste, ano, África. Dados, resultando em um conjunto de dados curado de aproximadamente 100 GB para a construção de Aroeira. Nossos resultados demonstram que nosso corpus atende aos requisitos de tamanho do corpus, domínios de conhecimento, distribuição de documentos ao longo do tempo, indicadores de qualidade e mitigação de viés. Realizamos análises estatísticas do corpus para compreender melhor o tamanho dos documentos coletados em termos de número de tokens, palavras e sentenças. Além disso, analisamos viés para reconhecer potenciais danos no corpus criado, um fator distinguindo na construção de modelos livres de viés social. Nossos achados concluem que o corpus criado é de alta qualidade e diversidade, com minimal viés. Estamos ansiosos para avançar nosso trabalho treinando modelos com arquiteturas de codificadores, como modelos BERT [21]. Além disso, planejamos treinar modelos de linguagem com Aroeira para obter modelos de alta qualidade em português. Pretendemos investigar conjuntos de dados de instrução e avaliação existentes. Finalmente, realizar análises de viés comparativas em modelos treinados com Aroeira em relação a outros conjuntos de dados disponíveis [13] será altamente valioso. Agradecimentos. Agradecemos ao Instituto de Ciência e Tecnologia Itaú-Unibanco (ICTi) e Itaú-Unibanco SA pelo suporte técnico, recursos e ajuda financeira na desenvolvimento do corpus Aroeira. É também digno de nota o fato de que o ChatGPT (OpenAI) foi empregado no processo de escrita, contribuindo para revisões gramaticais e semânticas minuciosas. Disponibilidade de Dados O nosso corpus Aroeira está disponível para download no repositório Hugging Face: https://huggingface.co/datasets/Itau-Unibanco/aroeira e está sob a licença CC-BY-NC 4.0.