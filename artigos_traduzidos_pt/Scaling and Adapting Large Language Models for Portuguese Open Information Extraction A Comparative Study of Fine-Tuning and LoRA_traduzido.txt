Escalabilidade e Ajuste de Modelos de Linguagem Grande para Extração de Informação Aberta em Português: Um Estudo Comparativo de Ajuste Fino e LoRA

Alan Melo1, Bruno Cabral1[0000−0002−5221−2860], e Daniela Barreiro Claro1[0000−0001−8586−1042] Centro de Pesquisa FORMAS sobre Dados e Linguagem Natural Instituto de Computação - Universidade Federal da Bahia – Salvador–Bahia, Brasil {alan.melo, bruno.cabral, dclaro}@ufba.br http://www.formas.ufba.br Resumo. Este artigo realiza uma investigação abrangente sobre a eficácia de diferentes técnicas de ajuste para Modelos de Linguagem Grande (LLMs) no contexto da Extração de Informação Aberta (OpenIE) em português. Comparamos Ajuste Fino Completo (FFT) e Ajuste de Baixo Rango (LoRA) em um modelo com 0,5 bilhão de parâmetros. Nossa pesquisa avalia o impacto do tamanho do modelo e do método de ajuste na performance da OpenIE, considerando precisão, recall e pontuação F1, bem como a eficiência computacional durante as fases de treinamento e inferência. Contribuímos para um modelo de LLM de alta performance e novas perspectivas sobre os trade-offs entre escala do modelo, técnica de ajuste e transferência interlinguística na tarefa de OpenIE. Nossos achados revelam variações significativas de desempenho em diferentes configurações, com LoRA demonstrando resultados competitivos. Também analisamos as nuances linguísticas na OpenIE em português que apresentam desafios para modelos treinados principalmente em dados de inglês. Esta pesquisa avança nossa compreensão sobre o ajuste de LLM para tarefas de NLP especializadas e fornece diretrizes práticas para o deployment desses modelos em cenários de recursos limitados e multilíngues. Nossa obra tem implicações para o campo mais amplo de extração de informação aberta interlinguística e contribui para o debate em curso sobre estratégias de ajuste eficientes para modelos pré-treinados grandes. Palavras-chave: OpenIE · Modelo de Linguagem · Extração de Informação. 1 Introdução A Extração de Informação Aberta (OpenIE) é uma tarefa de NLP que extrai dados estruturados de documentos [2].
Recentemente, tarefas como essas têm sido inseridas em diferentes pipelines para facilitar a complexidade de um conjunto de aplicações de Processamento de Linguagem Natural (NLP), como sistemas de QA, mapas mentais, abordagens de notícias falsas, etc. Seguindo a evolução do domínio de Modelos de Linguagem Grande (LLM), o task OpenIE tem ampliado suas abordagens para empregar arquiteturas de LLM. Como OpenIE viu avanços significativos na língua portuguesa nos últimos anos, a aplicação de Modelos de Linguagem Grande (LLMs) ainda precisa ser explorada. Como a evolução dos LLMs aumenta o custo computacional [10], métodos de adaptação avançados de modelos de linguagem, como Fine-Tuning Completo (FFT) e Adaptação de Baixo Rango (LoRA), são essenciais para melhorar desempenho e eficiência. Assim, a língua portuguesa beneficia desses métodos de adaptação para evoluir suas aplicações de NLP. Este estudo visa examinar o potencial dos LLMs quando aplicados à OpenIE portuguesa, avaliar o impacto do tamanho do modelo e do método de adaptação, considerar precisão, recall e escores F1, e examinar a eficiência computacional durante as fases de treinamento e inferência. Contribuímos para um modelo de linguagem grande de alta performance e novos insights sobre os trade-offs entre escala do modelo, técnica de adaptação e transferência interlingual na tarefa de OpenIE para a língua portuguesa. Nossos resultados sugerem uma performance significativa entre as técnicas e fornecem insights sobre as implicações práticas de sua aplicação na tarefa de Extração de Informação Aberta para a língua portuguesa. Este trabalho está organizado da seguinte maneira: Seção 2 descreve a tarefa de OpenIE, Seção 3 apresenta o trabalho relacionado e situa nossa abordagem no estado da arte. Seção 4 apresenta as Fundações e Técnicas de Adaptação, Seção 5 descreve nossa metodologia e Seção 6 apresenta nossas avaliações e conclusão e direções de pesquisa futura que a seguem.
2 Extração de Informação Aberta (OpenIE)

Introduzimos a definição de Extração de Informação Aberta (OpenIE) e o modelo para avaliar o desempenho dos Modelos de Linguagem de Grande Escala (LLMs) como extratores de triplos para o idioma português. Esta seção explora a definição e as potenciais aplicações da OpenIE, destacando contribuições chave e desafios em curso.

2.1 Definição de OpenIE

Seja X = {x1, x2, ..., xn} uma sentença composta por tokens xi. Um extrator de triplos de OpenIE é uma função que mapeia X para um conjunto Y = {y1, y2, ..., yj}, onde cada elemento é um tuplo yi = {reli, arg1i, arg2i} que encapsula a informação transmitida na sentença X. Supomos que os tuplos sempre estejam no formato y = (arg1, rel, arg2), com arg1 e arg2 sendo frases nominais criadas a partir de tokens em X, e rel representando uma relação entre arg1 e arg2. Como é comum na área, não consideramos extrações que consistam em relações n-árias.

2.2 Aplicação de OpenIE

A OpenIE tem sido usada em uma pipeline para servir a diversas aplicações, mas não está limitada a elas. Escalabilidade e Ajuste de LLMs para PT Open IE: Fine-Tuning X LoRA 3

– Grafos de Conhecimento: A extração de informações estruturadas de texto, a OpenIE facilita a criação automática de grafos de conhecimento, que são fundamentais para buscas semânticas, sistemas de resposta a perguntas e sistemas de suporte a decisões.

– Summatização de Texto: A OpenIE pode ajudar a identificar fatos e relações-chave em um texto, ajudando a gerar resumos concisos.

– Consulta e Resposta: Triplos extraídos diretamente de perguntas em um sistema de QA podem fornecer respostas factuais ou ajudar a aumentar a extração de inferência implícita.

2.3 Desafios de OpenIE

Apesar de avanços significativos na Extração de Informação Aberta (OpenIE), vários desafios persistem que obstaculizam sua aplicabilidade mais ampla. A extração de relações envolvendo entidades complexas ou expressas por construções linguísticas intricadas permanece desafiadora.
Além disso, embora o OpenIE busque ser agnóstico em relação ao domínio, seu desempenho ainda pode variar significativamente em diferentes gêneros e domínios de texto, como jurídico e saúde, o que requer esforços contínuos para adaptação ao domínio. Enquanto a extração de informações em tempo real se torna mais comum, é crítico melhorar a escalabilidade e a eficiência computacional dos sistemas OpenIE. Além disso, a presença de ambiguidades semânticas e dependências contextuais em textos apresenta desafios substanciais à precisão da extração, enfatizando a necessidade de continuidade de pesquisas nessa área. O OpenIE continua a ser um campo viva de pesquisa com considerável potencial para impactar uma ampla gama de aplicações, e avanços em aprendizado de máquina - especialmente em aprendizado profundo e não supervisionado - são esperados para impulsionar melhorias adicionais.

3 Trabalhos Relacionados

A Extração de Informação Aberta (OpenIE) evoluiu significativamente desde sua incepção, passando de abordagens baseadas em regras para técnicas de aprendizado de máquina sofisticadas. Esta seção explora a evolução dos sistemas OpenIE, com um foco particular nos avanços recentes em Modelos de Linguagem Grande (LLMs) e sua aplicação em cenários multilíngues, especialmente em português. Os sistemas OpenIE iniciais se baseavam fortemente em análise sintática e regras manualmente construídas. O OpenIE foi introduzido por Banko et al. [2], que se concentrou em abordagens simbólicas baseadas em regras pré-definidas para descrever cada frase nominal e verbal apresentada na sentença. Motivados pela relação distante entre a frase nominal e seu núcleo, a abordagem de árvore de dependência emergiu como um sistema promissor, apresentado por Del Corro et al. [8]. Esses sistemas usam padrões linguísticos pré-definidos para extrair informações. Embora sejam transparentes e fáceis de depurar, sua eficácia é limitada pela cobertura e complexidade das regras. No entanto, o campo já se shiftou para abordagens de aprendizado de máquina, que demonstraram desempenho superior e adaptabilidade [30, 7, 32, 38].
4 Melo A., Cabral B.S., Claro D. B. Zhou e colaboradores [39] categorizaram os sistemas de OpenIE modernos em dois principais tipos: modelos de marcação sequencial e modelos gerativos. Os modelos de marcação sequencial consideram a OpenIE como um task de rotulação de tokens, atribuindo etiquetas para indicar o papel de cada palavra em uma sentença (por exemplo, argumento, predicado) [39]. Esses modelos tipicamente incorporam embeddings de tokens, codificadores contextuais (como BERT [9]) e decodificadores de etiquetas (frequentemente usando Campos Aleatórios Condicionais [20]). Trabalhos notáveis nessa categoria incluem Stanovsky et al.[30], que introduziram uma arquitetura de Rede Neural Recorrente (RNN) para OpenIE em inglês. A falta de corpora criou uma onda nova para evoluir recursos, particularmente aqueles com corpora para línguas diferentes do inglês [30]. Como modelos de linguagem têm visto um uso significativo empregando redes neurais, a OpenIE evoluiu em descrever a tarefa e gerar tríplices como prompt, particularmente com línguas inglesas. Esses sistemas podem generalizar a partir de dados de treinamento para extrair tríplices de novos textos. Abordagens recentes têm aumentado a utilização de redes neurais, particularmente modelos de transformador, que melhoraram significativamente o desempenho dos sistemas de OpenIE ao capturar nuances contextuais e semânticas de forma mais eficaz. Abordagens gerativas, por outro lado, modelam a OpenIE como um problema de geração de sequências [7]. Esses modelos frequentemente empregam arquiteturas encoder-decoder para produzir uma sequência de extrações. Estudos recentes integraram embeddings de BERT em modelos gerativos, como visto em OpenIE6 e IMoJIE [17, 18], que abordam o problema de extrações redundantes em modelos gerativos de OpenIE. Embora a maioria da pesquisa em OpenIE tenha se concentrado em inglês, há crescente interesse em desenvolver sistemas multilíngues e translinguais. Zhang e colaboradores [38] propuseram uma abordagem semi-supervisionada translingual, enquanto Multi2OIE [27] utilizou M-BERT para embedding e extração de predicados em múltiplas línguas, incluindo português e espanhol.
Para o português do Brasil:

Os sistemas de OpenIE para o português têm evoluído desde a análise de dependência baseada em regras [23] e padrões orientados linguisticamente [28, 29] até aprendizado supervisionado com redes neurais profundas. Trabalhos recentes como Multi2OIE [27] e PortNOIE [5] têm mostrado melhoras significativas em scores F1 em comparação com métodos anteriores, destacando o potencial de abordagens baseadas em redes neurais para OpenIE em português. Este método português habilita a abordagem OpenIE-PT para camadas gerativas [4]. A aplicação de LLMs à OpenIE é uma tendência emergente, embora não seja ainda amplamente adotada. Existem casos de uso de LLM em campos relacionados, como Question Answering, Relation Extraction e Information Extraction. Xu e outros [36] exploraram a aplicação de um LLM para extração de relações com poucas amostras, enquanto Oppenlaender et al. [24] investigaram o uso de LLM para resposta a perguntas sobre corpora de texto de grande escala com resultados promissores. Wei e colaboradores [35] examinaram o uso de sistemas de LLM para extração de informações sem treinamento, propondo frameá-lo como um problema de resposta a perguntas multi-passo. Kolluru et al. [19] investigaram o uso de Modelos de Linguagem, nomeadamente BERT e mT5 [37], para um modelo gerativo de OpenIE em duas etapas que identifica relações e então assembleia extrações para cada relação. Escalando e Adaptando LLMs para Open IE em PT: Fine-Tuning X LoRA 5 Pesquisas recentes exploraram várias estratégias de fine-tuning para LLMs para adaptá-las a tarefas específicas de forma eficiente. O Fine-Tuning Completo (FFT) envolve atualizar todos os parâmetros do modelo, o que pode ser computacionalmente caro para modelos grandes [13]. Em contraste, os métodos de Fine-Tuning Eficiente em Parâmetros (PEFT) buscam reduzir o número de parâmetros treináveis enquanto mantém o desempenho [12]. A Adaptação de Baixo Rango (LoRA), introduzida por Hu e colaboradores [14], é um método de PEFT que ganhou popularidade devido à sua eficiência e eficácia.
A LoRA adiciona matrizes de baixa-rank treináveis às camadas de atenção de modelos pré-treinados, reduzindo significativamente o número de parâmetros treináveis enquanto alcança desempenho comparável ao treinamento fino completo em muitas tarefas [14]. A transferência de aprendizado interlinguística mostrou promessa em adaptar modelos treinados em línguas de alta recursos para línguas de baixa recursos [6]. Estudos recentes investigaram as capacidades interlinguísticas de LLMs e seu potencial para aprendizado zero-shot e few-shot em cenários multilíngues [21, 15]. Cabral et al. [4] introduziram um LLM finetunado com LoRA para o português com base em LLaMA-2, alcançando resultados bons. No contexto do OpenIE, a transferência interlinguística poderia potencialmente aproveitar a abundância de dados de treinamento em inglês para melhorar o desempenho em línguas com recursos menores, como o português. No entanto, a eficácia dessas abordagens de aprendizado transferido em tarefas de OpenIE, particularmente quando usando LLMs, permanece um desafio aberto. Nossa obra constrói sobre essas bases, explorando a aplicação de LLMs ao OpenIE em português por meio de treinamento fino e perspectivas de LoRA. Investigamos os trade-offs entre o tamanho do modelo, a técnica de adaptação e a transferibilidade interlinguística, contribuindo para o discurso em andamento sobre estratégias de treinamento fino eficientes para modelos pré-treinados grandes em cenários multilíngues.

Fundamentos de Modelos de Linguagem Grande e Técnicas de Adaptação

Essa seção explora as bases técnicas e teóricas que formam a base dos sistemas de processamento de linguagem natural (NLP) modernos. Com um foco especial em Modelos de Linguagem Grande (LLMs), essa parte do artigo detalha as inovações técnicas e teóricas que impulsionaram avanços no campo, bem como adaptações estratégicas como Adaptação de Baixa-Rank (LoRA) e treinamento fino, que melhoram a funcionalidade desses modelos em aplicações específicas.
A evolução dos modelos de linguagem larga (LLMs), desde os modelos baseados em n-gramas iniciais até as arquiteturas de transformadores atuais, reflete um progresso significativo na capacidade de simular a compreensão e produção de linguagem humana, levando a melhorias notáveis na eficiência e eficácia em uma variedade de tarefas de Processamento de Linguagem Natural (NLP) [3].

4.1 Modelos de Linguagem Larga

Os Modelos de Linguagem Larga são algoritmos de aprendizado de máquina avançados projetados para simular a capacidade humana de compreender e produzir linguagem natural. Esses modelos são predominantemente construídos usando técnicas de aprendizado profundo e são treinados em corpora textuais extensas para capturar as complexidades gramaticais, léxicas e semânticas da linguagem. Historicamente, eles evoluíram desde modelos simples baseados em n-gramas até arquiteturas de rede neural sofisticadas, como redes neurais recorrentes (RNNs) e, mais recentemente, transformadores, que utilizam mecanismos de atenção para melhorar a compreensão contextual e geração de texto [34]. Os transformadores, em particular, revolucionaram o modelamento de linguagem com sua capacidade de processar sequências de texto em paralelo, resultando em ganhos significativos na eficiência e eficácia em tarefas de NLP. Esses modelos formam a base para aplicações como sistemas de diálogo automatizados, extração de informações aberta, tradução de máquina e outras que requerem compreensão profunda e manipulação de linguagem humana [9]. A pesquisa e o desenvolvimento em andamento nessa área visam não apenas melhorar a precisão desses modelos, mas também torná-los mais acessíveis e éticos em seu uso. Recentes avanços incluem o desenvolvimento de modelos multilíngues capazes de processar e gerar texto em múltiplas línguas, o que é particularmente relevante para nosso estudo sobre Extração de Informações Abertas em Português [6].
2 Adaptação de Baixo Rango (LoRA) LoRA é uma técnica de adaptação de modelo que aproveita o princípio da decomposição de matrizes para modificar eficientemente os pesos de um modelo treinado previamente, permitindo adaptação com custo computacional significativamente reduzido. Diferente da fine-tuning tradicional, que ajusta todos os parâmetros do modelo, a LoRA se concentra em adaptar uma fração desses parâmetros através da adição de projetos de baixo rango [14]. No centro da LoRA está a ideia de que matrizes de transformação em modelos de linguagem, como aquelas encontradas nas camadas de atenção e feed-forward de transformadores, podem ser aproximadas por produtos de matrizes de menor dimensão. Matematicamente, essa abordagem envolve a introdução de dois matrizes menores, A e B, onde o produto AB serve como uma aproximação de baixo rango para atualizar a matriz de pesos originais, W. Esse produto não substitui W, mas é adicionado a ela, permitindo que o modelo original seja estendido com novas capacidades de aprendizado sem alterar diretamente sua estrutura pré-existente [14]. O principal vantagem de usar LoRA em modelos de linguagem é dupla: primeiro, reduz o número de parâmetros que precisam ser ajustados durante a adaptação, diminuindo a demanda por recursos computacionais e dados de treinamento. Segundo, mantendo a estrutura do modelo original intacta, a LoRA preserva o conhecimento prévio embutido no modelo, minimizando o risco de esquecimento catastrófico, um problema comum em adaptações mais invasivas [16]. A LoRA provou ser particularmente útil em cenários onde os recursos computacionais são limitados ou quando é necessário adaptar modelos em dispositivos com capacidade de processamento restrita. Por essas razões, a LoRA é uma escolha estratégica para adaptar modelos de linguagem a tarefas específicas, mantendo um equilíbrio entre eficácia e eficiência [14]. Escalando e Adaptando LLMs para PT Open IE: Fine-Tuning X LoRA 7 4.
3 Ajuste fino Ajuste fino, no contexto de modelos de linguagem grandes (MLMs), refere-se ao processo de ajustar os pesos pré-treinados de um modelo em um conjunto de dados ou tarefa específica, com o objetivo de adaptar o modelo para melhor desempenho em cenários específicos. Este método envolve continuar o treinamento do modelo a partir de sua configuração pré-treinada inicial, geralmente usando uma taxa de aprendizado mais baixa, para refinar seus parâmetros sem perder as generalizações aprendidas durante o treinamento extensivo [13]. O processo de ajuste fino envolve vários passos críticos: seleção de um conjunto de dados de treinamento que seja representativo da tarefa final, escolha de uma taxa de aprendizado suficientemente baixa para evitar perder informações úteis já adquiridas e ajuste cuidadoso do número de épocas de treinamento para prevenir sobreajuste. O sucesso do ajuste fino depende não apenas da qualidade e tamanho do conjunto de dados, mas também de uma boa estratégia de regularização e monitoramento da generalização do modelo [25]. A principal vantagem do ajuste fino é sua capacidade de produzir modelos altamente especializados para tarefas específicas, o que pode resultar em um aumento significativo no desempenho em comparação com modelos pré-treinados genéricos. No entanto, este método também apresenta desafios, como a necessidade de grandes volumes de dados específicos da tarefa e o risco de sobreajuste, especialmente em tarefas com conjuntos de dados limitados [11]. Por essas razões, o ajuste fino é uma técnica poderosa e amplamente utilizada na adaptação de modelos de linguagem para aplicações de processamento de linguagem natural (NLP), oferecendo uma abordagem flexível para personalizar modelos de inteligência artificial em uma variedade de domínios e tarefas [31]. No contexto do nosso estudo sobre Extração de Informação Aberta em Português, exploramos tanto as técnicas LoRA quanto o ajuste fino para adaptar modelos de linguagem grandes, comparando sua eficácia e eficiência nessa tarefa específica de processamento de linguagem cruzada.
Este análise comparativa contribui para o discurso em andamento sobre estratégias de adaptação eficientes para modelos pré-treinados grandes em cenários multilíngues [1]. 5 Metodologia 5.1 Corpus Nossa pesquisa empregou o corpus TransAlign [26], projetado especificamente para melhorar a disponibilidade de dados de treinamento de alta qualidade para Extração de Informação Aberta (OpenIE) em línguas sub-representadas. O corpus, desenvolvido por meio de alinhamento cross-lingual de dados de línguas ricas em recursos, como o inglês para o português brasileiro, compreende 96.067 triples de alta qualidade. Esses triples estão alinhados para refletir estruturas gramaticais do português brasileiro, utilizando modelos de tradução avançados e regras manualmente construídas. 5.2 Pre-processamento No processamento inicial dos dados do corpus TransAlign, cada triple e sentença foi transformado em um formato próximo à linguagem natural, utilizando um sistema de comando específico. Essa transformação teve como objetivo preparar os dados para manipulação mais eficiente em modelos de linguagem. ChatML, um formato de modelo de template utilizando sintaxe Jinja1, foi empregado para transformar uma lista de mensagens de chat em uma string formatada que pode ser utilizada diretamente por modelos de linguagem para treinamento ou inferência. No contexto do TransAlign, esse template foi adaptado para organizar a informação dos triples (ARG0, V, ARG1) em um formato simulando conversas, facilitando o treinamento de modelos em tarefas de OpenIE [33]. Axolotl2, uma plataforma de treinamento de modelos de linguagem, foi utilizada para treinar os modelos utilizando os dados processados. A escolha de Axolotl foi devido à sua eficiência em gerenciar e otimizar o treinamento de modelos de linguagem de grande escala, especialmente em configurações envolvendo adaptações de dados complexas, como é o caso da OpenIE multilíngue [14]. Após aplicar o template ChatML, os dados foram convertidos para o formato JSONL.
Este formato é particularmente útil para o treinamento de modelos de inteligência artificial, pois permite que cada linha do arquivo contenha um objeto JSON completo, representando uma entrada de dados única. Isso simplifica a carga de dados e o processamento em lotes durante o treinamento, contribuindo para a eficiência geral do processo.

5.3 Parâmetros Hipermétricos

Exploramos as configurações de treinamento de modelos de linguagem apenas de decodificação, especificamente focando no Fine-Tuning Completo para o modelo Qwen2 0,5 (0,5B de parâmetros) e LoRA no mesmo modelo. O modelo Qwen2 0,5B foi selecionado devido à sua compatibilidade com o conjunto de dados de treinamento, que inclui dados em português, e seu tamanho menor permite treinamento estável com recursos computacionais limitados. O processo de treinamento foi padronizado para garantir a comparabilidade entre diferentes escalas de modelo e técnicas. Em nossos experimentos, padronizamos as configurações para cada modelo, estabelecendo o tamanho de lote para 64. Ajustamos as configurações de acumulação de gradientes de acordo para manter a eficiência de tamanho de lote equivalente entre os dois modelos. Especificamente, utilizamos quatro acumulações para o modelo LoRA e oito acumulações para o modelo de Fine-Tuning Completo. Essa diferença no número de micro-lotes, com acumulações mais frequentes no modelo de Fine-Tuning Completo, foi necessária devido à menor variabilidade de gradientes observada durante o treinamento com LoRA. A taxa de aprendizado foi inicializada em 5 × 10−5 e ajustada de acordo com um agendador de taxa de aprendizado cosino. Nossos modelos passaram por treinamento durante três épocas com uma fase de aquecimento breve de 10 passos para estabilizar a taxa de aprendizado no início do treinamento. A função de perda utilizada foi a perda de entropia não-sloth, que melhora a cálculo de entropia tradicional, oferecendo gerenciamento de perda refinado crucial para a estabilidade e desempenho do treinamento do modelo. O otimizador escolhido foi o Paged AdamW 8bit3, incorporando quantização de 8 bits para o otimizador 1 disponível em https://jinja.palletsprojects.
Com 2 disponível em https://github.com/OpenAccess-AI-Collective/axolotl 3 https://github.com/TimDettmers/bitsandbytes Escalando e Adaptando LLMs para PT Open IE: Treinamento Finamente-Tunado com X LoRA 9 afirma que é possível reduzir significativamente os requisitos de memória enquanto preserva a eficácia do treinamento. Configurações adicionais incluíram checkpointing de gradientes, uma técnica crítica de otimização de memória que salva apenas estados intermediários selecionados durante a passagem em frente, reduzindo o consumo de memória global. Os modelos foram treinados usando pesos com precisão BF16, que alcança um equilíbrio ótimo entre desempenho computacional e precisão numérica. Para a configuração LoRA, um rank de 64 foi introduzido, resultando em 35.192.832 parâmetros treináveis. Além disso, os seguintes módulos de destino foram selecionados: q_proj, k_proj, v_proj, o_proj, gate_proj, down_proj, up_proj.

5.4 Configuração Experimental Nossos experimentos foram realizados em uma plataforma de hardware de cliente composta por um GPU NVIDIA RTX 3060 com 12GB de VRAM, suportado por 32GB de RAM do sistema. O ambiente de software foi baseado em Ubuntu, usando a ferramenta de treinamento Axolotl. Os modelos foram manipulados no formato GGUF F16, otimizado para tarefas de computação de alto desempenho, o que facilitou operações de carregamento e inferência eficientes. Para interação e manipulação de modelos, utilizamos a biblioteca llama-cpp-python 4, que se integra sem problemas com Python, permitindo processamento de dados sofisticado e ajustes de modelo. Além disso, foi usado Weights & Biases (wandb) para coletar métricas do sistema, como uso de potência do GPU, e rastrear parâmetros de treinamento, incluindo perda, durante o processo de treinamento, permitindo a geração de gráficos e coleta de dados esclarecedores. Essa configuração integral assegurou que nossos procedimentos experimentais fossem não apenas eficientes, mas também reprodutíveis.

5.5 Experimentos Os modelos foram treinados usando o corpus TransAlign completo, com 1% reservado para validação.
A avaliação da performance foi realizada utilizando métricas de precisão, recall e F1-score no conjunto de dados PUD 100, que é altamente anotado e confiável [22]. O código de avaliação desenvolvido por Stanovsky et al. (2018) foi empregado, com base em um métrica de correspondência lexical para avaliar a precisão das tríplices extraídas [30]. Os resultados serão apresentados na Tabela 1, incluindo precisão, recall e métricas F1 para cada modelo avaliado, permitindo uma comparação clara da performance do modelo sob diferentes configurações e adaptações.

6 Avaliação

Essa seção descreve a metodologia utilizada para comparar os dois modelos. Avaliamos as diferenças quantitativas empregando métricas como F1-score, precisão e recall. Para análise qualitativa, investigamos as tríplices geradas por ambos os modelos para avaliar sua eficácia em capturar informações pertinentes. Todos os outputs de modelo foram produzidos sob configurações de geração consistente: uma temperatura de 0,2, um top-p de 0,95, um min-p de 0,05 e um top-k de 40. Esses parâmetros foram escolhidos para garantir a estabilidade e relevância do conteúdo gerado.

6.1 Análise Quantitativa

Durante a fase de inferência, o consumo de memória de vídeo foi de 1,2 GB para ambos os modelos, e as taxas de geração de tokens foram semelhantes, a 2500 tokens por segundo. Tabela 1. Escores de Avaliação

Modelo F1 precisão recall
Qwen2 0,5b LoRA 0,2797 0,33 0,2427
Qwen2 0,5b 0,2712 0,32 0,2353

Durante a fase de treinamento, o modelo LoRA consumiu menos energia do que o modelo Full Fine-Tuning (FFT). Isso ocorre porque o LoRA reduz o número de parâmetros treináveis ao se concentrar apenas em matrizes de baixa rank, deixando a maioria dos pesos pré-treinados intactos. Essa abordagem reduz significativamente a carga computacional, resultando em menor uso de memória e consumo de energia, em comparação com o FFT, que atualiza todos os parâmetros do modelo, como ilustrado na Figura 1.

Fig. 1.
Gráfico ilustrando o consumo de energia comparativo dos modelos LoRA e FFT durante a fase de treinamento. Observou-se que o modelo treinado com LoRA alcançou resultados melhores em todos os métricos em comparação ao modelo que sofreu fine-tuning completo para tarefa de OpenIE, como pode ser observado na Figura 2.

Estatísticas de tempo de inferência na sentença "Alan, que estuda na UFBA, é membro da Formas," revelam diferenças mínimas entre Fine-Tuning Completo (FFT) e LoRA. FFT levou 228,14 ms, enquanto LoRA melhorou ligeiramente para 226,55 ms. Os tempos de avaliação foram de 4,89 ms por token para FFT e 4,67 ms para LoRA. Os tempos de avaliação de prompt e amostra foram quase idênticos para ambos os métodos. Essas diferenças mínimas, variando de 0,17 ms a 5,36 ms, sugerem que, embora LoRA seja marginalmente mais rápido, o impacto sobre o desempenho prático é negável.

0,00 0,10 0,20 0,30 0,40 QWEN2-0,5B QWEN2-0,5B-LoRA f1 precisão recall Fig. 2. Gráfico que representa as diferenças entre os modelos validados.

6.2 Análise Qualitativa Ao analisar alguns dos tríplices gerados por ambos os modelos, uma degradação significativa na qualidade de geração foi notada no modelo que sofreu fine-tuning completo, enquanto o modelo treinado com LoRA demonstrou maior resistência às halucinações. As seguintes extrações exemplificam isso:

Sentença: “Ela começou no RSC no meio dos anos sessenta, atuando como internada de asilo no Marat/Sade.”

FFT: arg1: Ani - rel: começou em - arg2: o RSC

Análise do FFT: O modelo FFT halucina ao identificar incorretamente "Ani" como o sujeito da ação "começou em". "Ani" não aparece na sentença, mostrando um erro claro na reconhecimento de entidade e mapeamento de relação.
Aqui está a tradução do texto científico para português do Brasil:

LoRA: arg1: Ela - rel: começou em - arg2: o RSC
Análise da LoRA: O modelo LoRA identifica corretamente "Ela" como o sujeito que "começou em o RSC", alinhando perfeitamente com o conteúdo da sentença original sem adicionar ou distorcer informações. Sentença: “O ano passado foi um ano incrível e estou pronto para que possamos voltar ainda melhor em 2017.”
FFT: arg1: O ano passado - rel: estou pronto para que possamos voltar - arg2: ainda melhor em 2017
12 Melo A., Cabral B.S., Claro D. B. Análise da FFT: Nesta extração, o modelo FFT cria uma relação hallucinada conectando "O ano passado" com "estou pronto para que possamos voltar", o que não é uma relação lógica ou suportada no contexto da sentença.
LoRA: arg1: O ano - rel: passado foi - arg2: um ano incrível
Análise da LoRA: O modelo LoRA realiza uma extração mais precisa e textualmente fundamentada, identificando que "O ano passado foi um ano incrível", o que é uma interpretação direta e fiel do que é explicitamente mencionado na sentença.
Sentença: “Após algumas cirurgias geniais e muito trabalho duro de reabilitação, eu tive uma recuperação completa.”
Extração FFT: arg1: eu tive uma recuperação completa - rel: inicialmente - arg2: uma recuperação completa
Análise da FFT: O modelo FFT incorre em redundância e uma relação não suportada "inicialmente", o que distorce o sentido da sentença. Além disso, repete "uma recuperação completa" para ambos o sujeito e o objeto, mostrando um erro de segmentação.
Extração LoRA: arg1: eu - rel: tive - arg2: uma recuperação completa
Análise da LoRA: A extração do modelo LoRA é precisa e alinhada com o conteúdo da sentença, identificando corretamente "eu" como o sujeito, "tive" como o verbo indicando ação e "uma recuperação completa" como o objeto, capturando eficientemente a estrutura de informação da sentença.
Esses exemplos ilustram como o modelo LoRA tende a produzir extrações mais fiéis ao texto original, evitando a adição de informações inexistentes ou a distorção das relações entre elementos de sentença, ao contrário do modelo FFT, que mostrou uma propensão para alucinações possivelmente devido ao overfitting ou à insuficiente generalização durante o treinamento. Agradecimentos Este material é parcialmente baseado em trabalho apoiado pela FAPESB sob a bolsa TIC002/2015 e TO CCE 0022/2023. 7 Conclusões e Direções Futuras Neste trabalho, avaliamos o desempenho de modelos de linguagem grandes (LLMs) usando adaptações de Fine-tuning Completa e LoRA, analisando tanto medidas quantitativas quanto qualitativas. Utilizamos métricas como F1, precisão e recall para avaliar resultados quantitativos. A análise qualitativa é realizada por meio de conjuntos de exemplos. Nossos achados indicam que as adaptações LoRA são tão eficazes quanto a Fine-tuning Completa, com o benefício adicional de consumo de energia mais baixo e resultados competitivos. Planejamos realizar experimentos adicionais com um modelo de linguagem mais amplo para aprofundar nossa compreensão do comportamento dos LLMs sob essas adaptações.