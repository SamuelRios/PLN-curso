Resposta ao Questionamento com Textos e Tabelas por meio de Aprendizado por Reforço Profundo

Marcos M. José1,†[0000-0003-4663-4386], Flávio N. Cação1,2[0000-0003-4771-6009], Maria F. Ribeiro, Rafael M. Cheang1[0000-0003-2434-1304], Paulo Pirozelli1[0000-0002-4714-287X], e Fabio G. Cozman1[0000-0003-4077-4935]

1 Escola Politécnica, Universidade de São Paulo, São Paulo, Brasil
2 Novelo Data, São Paulo, Brasil
† Correspondência para: marcos.jose@alumni.usp.br

Resumo. Este artigo propõe uma arquitetura inovadora para gerar respostas multi-passo a perguntas de domínio aberto que requerem informações de textos e tabelas, utilizando o conjunto de dados Open Table-and-Text Question Answering para treinamento e validação. Uma das formas mais comuns de gerar respostas nesse contexto é recuperar informações sequencialmente, onde um pedaço de dados selecionado ajuda a procurar o próximo pedaço. Como diferentes modelos podem ter comportamentos distintos quando chamados nessa busca sequencial de informações, um desafio é como selecionar modelos em cada passo. Nossa arquitetura emprega o aprendizado por reforço para escolher entre ferramentas de ponta de estado em sequência até que, no final, uma resposta desejada seja gerada. Este sistema alcançou uma taxa de F1 de 19,03, comparável a sistemas iterativos da literatura.

Palavras-chave: Aprendizado por Reforço · Modelos de Linguagem Grande · Question Answering · Multi-passo.
Uma mistura de dados textuais e tabulares oferece, portanto, um potencial significativo para melhorias em sistemas de QA. Este trabalho aborda problemas de QA que requerem a recuperação e a agregação de informações de ambos os dados textuais e tabulares. Nossa base de testes é o conjunto de dados de Question Answering (QA) Open Table- and-Text (OTT-QA) [5], um conjunto de dados de QA aberto e multi-passo. Nesse conjunto de dados, as perguntas são fornecidas sem textos de suporte, e para responder a elas, é comum ser necessário navegar por múltiplos trechos, tanto textuais quanto tabulares. O desempenho nesse conjunto de dados permanece limitado em comparação com conjuntos de dados textuais puramente, como SQuAD, onde os modelos já superam os humanos. Atualmente, as estratégias mais adotadas para QA combinam módulos de recuperação e de leitura: um módulo de recuperação recupera trechos relevantes, enquanto um módulo de leitura transforma esses trechos em uma resposta significativa [4, 7, 12]. Em conjuntos de dados multi-passo como OTT-QA, esse processo de recuperação precisa ser feito sequencialmente, movendo-se de uma peça de informação para outra, para obter os dados necessários (textuais ou tabulares). Embora sistemas não sequenciais recentes tenham atingido desempenhos maiores, uma formulação de tomada de decisão sequencial parece ser essencial para coordenar esquemas de recuperação e leitura. Neste papel, desejamos explorar os limites do desempenho obtido com decisões sequenciais aplicadas a módulos padrão, onde recorremos ao Aprendizado por Reforço (RL) para aprender estratégias. O RL é adequado para este tarefa: enquanto o aprendizado supervisionado depende de dados rotulados e estáticos para as sequências de ação, no RL a sequência de decisões é construída através da evolução da informação. Propomos, portanto, um novo abordagem de Aprendizado por Reforço Profundo (DRL) para esta tarefa, que decide iterativamente quais ações tomar (recuperar texto, recuperar tabela, gerar resposta), usando módulos padrão para conduzir essas ações. O papel é organizado da seguinte maneira. A Seção 2 fornece algum background necessário. A Seção 3 apresenta nossa arquitetura de RL proposta.
Seção 4 descreve nossa configuração experimental, enquanto a seção 5 apresenta os resultados e sua discussão associada. Finalmente, a seção 6 apresenta nossos pensamentos conclusivos.

2. Fundo

2.1. Open Table-and-Text Question Answering

Muitos conjuntos de dados e arquiteturas de QA (Question Answering) foram projetados para abordar questões que requerem entender informações textuais, como SQuAD, ou dados tabulares, como MIMICSQL [20]. No entanto, há poucos recursos que lidem com ambos textos e tabelas. Para preencher essa lacuna, o HybridQA foi introduzido como um conjunto de dados de questionamento multimodal e multi-hop fechado [6]. No HybridQA, cada pergunta carrega uma tabela e múltiplos trechos associados, e para responder a uma pergunta, é necessário combinar várias dessas fontes juntas. O OTT-QA [5] é um conjunto de dados em inglês com 45841 pares de QA construído sobre o HybridQA. O objetivo principal é servir como um conjunto de dados aberto que combina informações textuais e tabulares ao mesmo tempo que aumenta a complexidade da tarefa. O conjunto de dados contém tabelas e textos com respostas para cada pergunta no conjunto de dados. Um exemplo de par de QA do OTT-QA é mostrado na Figura 1. Para avaliar as respostas, utilizamos dois métricas de avaliação bem conhecidos (popularizadas pelo conjunto de dados SQuAD [17] e utilizadas pelo OTT-QA): Match Exato, a porcentagem de respostas preditas que são exatamente iguais às respostas ouro, e Macro Average F1-score, o sobreposição entre respostas preditas e respostas ouro. Nesse contexto, a precisão é a razão da quantidade de palavras compartilhadas entre as respostas ouro e as respostas preditas.

3. Dados e código disponíveis em: https://github.com/MMenonJ/DRL_QA_TT

3. Figura 1. Par de QA do OTT-QA [5]. A pergunta pergunta quem é o autor da série em que Nonso Aznozie interpretou o personagem Robert.
Para responder essa pergunta com precisão, é necessário localizar a tabela rotulada "Nonso Anozie" (destacada em rosa) e identificar a célula com "Principal Suspeito" (destacada em amarelo), referenciando o papel "Robert" (destacado em verde). É então possível recuperar o texto associado, que revela que o criador do filme foi "Lynda La Plante" (destacada em azul).

2.2 Questões Abertas e Resposta a Perguntas Em QA aberta, as perguntas podem ser sobre qualquer tema e um sistema deve encontrar respostas com base em uma dada pergunta e geralmente dentro de um corpus de informações. Sistemas de QA costumam consistir em dois componentes principais: um recuperador e um leitor. O papel do recuperador é buscar o corpus por textos e passagens relevantes, enquanto o leitor processa essa informação recuperada para gerar uma resposta apropriada. Algumas abordagens utilizam transformadores neurais para ambos os tarefas do recuperador e do leitor [7, 12], enquanto outras usam modelos baseados em palavras-chave especificamente para o recuperador [2, 4]. Recuperador: Um modelo de recuperador baseado em palavras-chave amplamente adotado em QA é o BM25 [18]. O BM25 se baseia na TF-IDF (Frequência de Termos - Frequência de Documento Inversa) ao incorporar a normalização da longitude do documento; portanto, documentos mais curtos que coincidem com a consulta são priorizados sobre documentos mais longos com matches de palavras-chave semelhantes. Em contraste, a Recuperação de Passagens Densa (DPR) [9] supera as limitações do matching de palavras-chave ao classificar passagens com base na similaridade semântica. A DPR utiliza dois modelos BERT treinados conjuntamente para produzir representações vetoriais densas de perguntas e passagens — a similaridade entre essas representações é estimada pelo produto de ponto, permitindo uma recuperação mais refinada com base no significado.
Ler: Os melhores leitores atuais são baseados em redes neurais de transformadores, que geram respostas a partir de entradas concatenadas de perguntas e trechos recuperados. Por exemplo, modelos baseados em BERT, como vistos em Guu et al. [7], se concentram na previsão de subtrings, enquanto arquiteturas encoder-decoder como BART, como em Lewis et al. [12], excel em tarefas de texto para texto. Outro leitor notável é o Fusion-in-Decoder (FiD) [8]. Diferentemente da concatenação simples de perguntas e trechos seguida do uso direto em uma rede encoder-decoder, o FiD processa independentemente cada trecho no encoder e combina vetores resultantes como entrada para o decoder. Este método manipula eficientemente um número maior de documentos. Similarmente, o Fusion-in-Encoder (FiE) [10] integra informações de múltiplos trechos dentro do encoder em vez do decoder. O FiE cria uma representação unificada e incorpora atenção transversal entre todos os tokens em diferentes amostras. 2.3 Arquiteturas testadas no OTT-QA Os autores do OTT-QA [5] apresentaram várias soluções, cada uma abordando diferentes estratégias de recuperação e leitura. A primeira solução, BM25-HYBRIDER, estabelece um baseline para o OTT-QA. Ela recupera 1 a 4 textos ou tabelas do corpus usando o algoritmo BM25. Esses trechos recuperados são então input para um leitor, que seleciona a resposta com o intervalo de confiança mais alto. A segunda solução, "Iterative-Retrieval + Cross-Block Reader", emprega dois variantes de recuperador iterativo. Esparsa utiliza o BM25 para recuperar 10 trechos de texto e 10 segmentos de tabela. Para cada trecho de texto, ela recupera 5 segmentos de tabela, e vice-versa. Densa utiliza um modelo semelhante ao DPR para recuperação. Começa com 8 trechos ou passagens de tabela e expande iterativamente a busca. Ambas as variantes alimentam os trechos recuperados para o Cross-Block Reader, um modelo de transformador ETC fine-tunado, para gerar a resposta.
A última solução apresentada pelos autores, o "Fusion Retriever + Cross-Block Reader", demonstrou superar os métodos anteriores. Essa abordagem combina tabelas e texto em blocos usando BM25 e melhora consultas com GPT-2. O Fusion Retriever é um setup de codificador duplo, baseado em DPR, que re retrieve k blocos com base na similaridade com a pergunta, enquanto mantém o mesmo Cross-Block Reader para gerar a resposta. O sistema QA CORE (Chain Of REasoning) emprega uma arquitetura de retriever-reader, mas inclui dois componentes intermediários, o Linker e o Chainer [14]. O retriever é construído sobre DPR e é responsável por recuperar 100 tabelas do corpus. Em seguida, um sistema de Linker baseado em modelos BERT recupera trechos do corpus com base nas linhas obtidas das tabelas recuperadas. Como o linker pode fornecer uma quantidade excessiva de informações para o leitor processar, o papel do Chainer é selecionar as 50 cadeias mais relevantes (linha de tabela e trecho de texto), que consistem em uma linha de tabela e um trecho de texto correspondente. O Chainer realiza essa tarefa empregando uma rede neural T0 que, de forma zero-shot, avalia as probabilidades do modelo gerar uma pergunta com base em textos e linhas de tabela específicas. Finalmente, para responder à pergunta, o sistema utiliza um modelo FiD, que se baseia nas informações curadas pelo Chainer. A arquitetura Chain-of-Skills (COS) [15] atualmente entrega o desempenho mais alto na OTT-QA. Os autores introduziram uma abordagem de modularização não sequencial que facilita o treinamento multi-tarefa em vários conjuntos de dados de QA abertos. As tarefas abordadas incluem recuperação única, recuperação de consultas expandidas, QA com Textos e Tabelas por meio de Aprendizado por Reforço Profundo, proposta de span de entidade, linkagem de entidade e re-ranking. Na fase de inferência, o sistema primeiro identifica 100 tabelas por meio de recuperação única. Essas tabelas são subsequentemente divididas em linhas, e o processo de re-ranking seleciona as 200 linhas mais relevantes.
Para cada linha, o sistema coleta 10 passagens da recuperação de consulta expandida e uma passagem de entidades ligadas. Em seguida, o sistema utiliza o mesmo Chainer utilizado no CORE para selecionar as 100 passagens mais relevantes, que são subsequentemente inputadas em um leitor FiE aprimorado para QA-OTT. Outra contribuição significativa é a "Recuperação Multi-modal de Tabelas e Textos com Modelos de Tri-encoder" [11], que estende as capacidades do DPR incorporando três redes de pequeno BERT para codificar perguntas, textos e tabelas individualmente.

2.4 Aprendizado por Reforço em Question Answering

Uma das abordagens pioneras para integrar Aprendizado Profundo por Reforço (DRL) à QA é a arquitetura do Ranker-Reader Reinforced (R3) [21], introduzida em 2017. Esse framework se concentra no treinamento conjunto do recuperador (referred to as o ranker) e do leitor, usando o algoritmo REINFORCE para o ranker e uma abordagem supervisionada para o leitor. Um dos principais vantagens dessa abordagem é a natureza não supervisionada do treinamento do recuperador, que fornece recompensas com base apenas na similaridade da saída do leitor para a resposta dourada. Da mesma forma, MSCQA [22] emprega DRL para treinar um seletor de ação que decide entre três componentes: invocar o leitor, selecionar passagens adicionais via o recuperador e remover respostas incorretas do memória. Em [3], um agente DRL é projetado com ações para funções de recuperador, leitor e limpeza, com o objetivo de lidar com um conjunto de QA multi-hop complexo, onde o recuperador iterativamente utiliza passagens anteriormente recuperadas para encontrar informações relevantes novas. O Aprendizado por Reforço com Feedback Humano (RLHF) é uma abordagem que tem ganhado popularidade no desenvolvimento de modelos de linguagem natural. O RLHF envolve treinar modelos por um processo que combina Aprendizado por Reforço, onde os modelos aprendem com suas ações e consequências, com Feedback Humano, que envolve revisores humanos fornecendo feedback e classificações sobre conteúdo gerado por modelos.
Um exemplo é o ChatGPT (ou GPT3.5), um agente conversacional construído sobre a base do GPT3. Embora os detalhes do processo de treinamento do ChatGPT não sejam completamente divulgados, informações disponíveis indicam que o treinamento com RLHF foi executado através do algoritmo PPO para melhorar a alinhamento do ChatGPT com os usuários e minimizar problemas relacionados à desinformação, toxicidade e sentimentos prejudiciais.

3 Arquitetura Proposta

A natureza multi-hop e multi-model da OTT-QA apresenta um desafio significativo devido ao processo de recuperação sequencial que requer. Para abordar esses desafios dentro de uma formulação sequencial explícita, propomos uma arquitetura novel. Nossa abordagem envolve treinar um agente de Aprendizado por Reinforço Profundo (DRL) que seleciona entre um conjunto de módulos treinados previamente na literatura existente.

6 M. José et al. Fig. 2. Arquitetura proposta. Em cada passo de tempo, o agente seleciona uma das três ações: Recuperar Textos, Recuperar Tabelas ou Gerar a Resposta, com base na pergunta e na informação coletada até então. Como feito para MSCQA [22], empregamos um agente DRL como seleção de ação para determinar qual componente treinado previamente deve ser ativado. Como mostrado na Figura 2, temos as seguintes ações: Recuperar Textos, Recuperar Tabelas e Gerar a Resposta. Dado que a OTT-QA não especifica o caminho correto para a recuperação de informações, incluindo a sequência de passagens a recuperar, adotamos uma abordagem de Aprendizado por Reinforço. Incorporamos um mecanismo de recompensa atrasada, que compara as respostas preditas com as respostas padrão de ouro ao final de cada episódio.

Uma vantagem notável da nossa arquitetura proposta é sua flexibilidade. Todos os componentes, incluindo o leitor e os recuperadores, podem ser facilmente substituídos por modelos mais novos e superiores quando eles se tornarem disponíveis para uso público. Além disso, o sistema pode ser melhorado ao incorporar componentes adicionais, como um modelo de recuperação de grafo, ou empregar múltiplos leitores otimizados para cenários específicos.
O agente RL aprende quando utilizar cada componente em situações diferentes através de treinamento experiencial. 3.1 Conceitos Iniciais Nossa abordagem se inspira na arquitetura de Leitura + Recuperação Iterativa + Bloco Transversal descrita na Seção 2.3, sendo também um sistema iterativo. Para uma dada pergunta, o passo de recuperação inicial envolve procurar 10 passagens, que podem ser tanto textuais quanto tabulares em natureza. Essas passagens são usadas para criar 10 blocos diferentes. Se o agente optar por realizar um passo de recuperação adicional, concatena os itens dentro de cada bloco para recuperar quatro novas passagens, seja elas textuais ou tabulares. O agente pode realizar um passo de recuperação adicional, levando à seleção de 4 novas passagens para cada bloco. Para manter a qualidade da informação recuperada e prevenir ruído excessivo, limitamos o número máximo de passos de recuperação a três. Essa limitação é destinada a evitar a acumulação de incerteza, pois a recuperação de passagens adicionais introduz mais informações, tornando mais desafiador para o processo de recuperação distinguir entre informações valiosas e ruído. Suponha que o agente decida gerar a resposta ou tenha alcançado o número máximo de passos. Nesse caso, o leitor é apresentado à pergunta e todas as passagens não repetidas (observe que blocos diferentes podem conter a mesma passagem). O leitor utiliza a Entrada de Leitura para gerar uma resposta predita ap, que é subsequentemente comparada à resposta ouro ag. Em casos onde o leitor for invocado antes de qualquer passagem ter sido recuperada, uma recuperação de Tabela Preliminar é executada para garantir que pelo menos 10 documentos estejam disponíveis para processamento. 3.2 Recompensa Como não possuímos conhecimento prévio da sequência correta de ações para o agente, não podemos utilizar aprendizado supervisionado e devemos nos basear na comparação entre a resposta esperada (ouro) e a resposta predita do agente para construir a função de recompensa.
Começamos com a função de recompensa R3, que treina o recuperador indiretamente, usando a resposta final. No entanto, devido à complexidade do problema, o pen- altivo de -1 para uma resposta gerada sem intersecção com a resposta ouro parece ex- cessivo, especialmente considerando que a Iterative-Retrieval + Cross-Block Reader, nossa inspiração, atinge uma taxa de F1 de apenas 20,9 no conjunto de teste. Portanto, ajustamos o pen- altivo para -0,5. A seguinte equação representa a recompensa no final do episódio: R(ag, arc) = { 2, se ag = arc, F1(ag, arc), senão, se ag ∩arc ≠ ∅, -0,5, senão, (1) onde a recompensa é atribuída um valor de 2 quando a resposta ouro ag precisamente coincide com a resposta predita ap. Nos casos de match parcial, a recompensa é determinada pelo score F1. Por fim, quando não ocorre match, a recompensa padrão é -0,5, como mencionado anteriormente. Também introduzimos um pequeno pen- altivo de -0,02 para cada ação de recuperação realizada. Este pen- altivo incentiva o agente a minimizar a recuperação de textos e tabelas desnecessárias.

3.3 Ações

Como mencionado anteriormente, nossa abordagem envolve três ações distin- tas: Recuperar Textos (A1), onde um modelo de recuperador busca informações textuais relevantes; Recuperar Tabelas (A2), onde um modelo de recuperador busca tabelas relevantes; e Gerar Resposta (A3), onde um modelo de leitor produz a resposta usando passagens textuais extraídas e tabelas. Em seguida, discutiremos nossas escolhas para os modelos de leitor e recuperador. Optamos por não reentrenar modelos da literatura, limitando-nos a aqueles disponíveis publicamente.

O módulo de recuperador coleta informações para o leitor. Avaliamos dois tipos: BM25 [18] e Tri-encoder [11]. Esta abordagem permitiu que avaliássemos métodos de recuperação esparsa clássica e recuperação neural densa, semelhantes à arquitetura Iterative-Retrieval + Cross-Block Reader.

O leitor: componente crítico de qualquer sistema de QA, o leitor gera respostas.
Para o OTT-QA, o modelo de Fusion-in-Encoder (FiE) público disponível do sistema COS [15] demonstrou desempenho superior. Notavelmente, esse modelo manipula passagens individualmente, tornando-o versátil para lidar com múltiplos inputs. Seguindo as recomendações dos autores, limitamos o número de passagens de entrada a 50, o que resultou em resultados ótimos em nossos experimentos de validação.

3.4 Estado/Observação

A observação é a informação que o agente utiliza para o processo de tomada de decisão. No nosso caso, essa informação compreende a pergunta e as passagens recuperadas até aquele ponto. Em vez de trabalhar diretamente com texto puro, convertimos esses elementos em dados numéricos usando embeddings. Empregamos dois encoders distintos para realizar essa transformação em embeddings. Quando o recuperador é o Tri-encoder, utilizamos suas embeddings para representar a pergunta, as passagens recuperadas e as tabelas recuperadas. No entanto, no caso do BM25, aproveitamos as representações fornecidas por um modelo MPNET treinado para busca semântica em QA4. Cada embedding é um vetor com comprimento de 512 para o Tri-encoder (mesmo que o small-BERT) e 768 para o modelo MPNET. Essa informação é então traduzida em uma sequência de 11 vetores, com um vetor para a pergunta e um para cada um dos 10 blocos. Para cada bloco, calculamos a média dos valores de representação para cada passagem, considerando a pergunta isoladamente, gerando a sequência de embeddings.

3.5 Agente

Em aplicações de RL, a função primária do agente é processar observações e gerar ações correspondentes. Nossas observações específicas consistem em uma sequência composta por 11 vetores. Em nossa exploração, experimentamos com quatro arquiteturas de rede neural distintas: MultiLayer Perceptron (MLP), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU) e Transformer.
Para o MLP, nivelamos a sequência inteira em um único vetor, o que significou adicionar zeros no primeiro passo para compensar a falta de passagens recuperadas nesse ponto. Em contraste, as outras redes processam essa informação como uma sequência. Para treinar o agente, exploramos dois algoritmos amplamente utilizados: Redes de Q Profundas (DQN) [16] e Otimização de Política Proxima (PPO) [19]. Nossa implementação é baseada na biblioteca Python Stable-Baselines3, selecionada por sua robustez e confiabilidade. O fluxo de treinamento do agente é apresentado na Figura 3. O modelo MPNET pode ser encontrado em https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1.

Fig. 3. Fluxo de treinamento da arquitetura proposta.

4 Experimentos

4.1 Bases

Antes de treinar o agente de aprendizado por reforço, exploramos cenários de base utilizando nossa arquitetura. Essas bases envolveram conjuntos fixos de ações escolhidos independentemente da pergunta e da informação recuperada. Cada combinação concebível de ações foi avaliada sistematicamente em um conjunto de dados de validação. Por exemplo, uma base envolveu o agente escolhendo consistentemente recuperar textos (Ação A1) duas vezes e em seguida invocar o leitor para gerar a resposta. Em total, exploramos 14 configurações diferentes de ações, abrangendo dois tipos de recuperadores—BM25 e Tri-encoder—resultando em 28 bases. O principal propósito desses testes de base foi estabelecer um benchmark para comparação, determinando se o agente treinado de aprendizado por reforço poderia superar ou pelo menos igualar o desempenho de caminhos de ação fixos sem compreensão contextual.

4.2 Treinamento

O treinamento do agente de aprendizado por reforço envolveu amostrar uma pergunta aleatória do conjunto de treinamento OTT-QA para cada episódio. Avaliamos o impacto do treinamento de soluções de recuperadores e comparamos seu desempenho. Para o Tri-encoder, treinamos durante um total de um milhão de passos, um número razoável considerando o tamanho do conjunto de dados de 41.469 perguntas de treinamento.
Cada pergunta poderia demorar até quatro passos (entre o recuperador e o leitor), permitindo revisitar a mesma pergunta várias vezes. Devido ao tempo de inferência mais alto do BM25 (até 20 vezes mais que o Tri-encoder), optamos por treinar apenas por 100 mil passos. Implementamos os algoritmos DQN e PPO, empregando várias arquiteturas de rede neural, incluindo MLP, LSTM, GRU e transformador. Subseções subsequentes fornecem explicações detalhadas dessas implementações. DQN: Para treinar o DQN, fizemos algumas ajustes nos parâmetros de hiper-âmplitude padrão do Stable-Baselines3, principalmente para considerar o número de passos de treinamento. Definimos o tamanho do buffer para 500K (número máximo de transições armazenadas na memória de replay) e o início do treinamento (número de passos realizados com ações aleatórias apenas para armazenar algumas transições no buffer antes do treinamento) para 50.000 para o sistema com Tri-encoder como recuperador. Para o BM25, diminuímos o início do treinamento em um fator de 5, considerando que o treinamento tem apenas 100.000 passos. PPO: Similarmente ao DQN, nos baseamos principalmente nos parâmetros de hiper-âmplitude padrão para treinar usando a implementação PPO do Stable-Baselines3. Definimos o número de passos a ser executado antes do treinamento como 128, com um tamanho de lote de 32 e 60 como o número de épocas para otimizar a perda de substituição. Como o PPO é um algoritmo Actor-Critic, ele emprega duas redes diferentes: o ator e o crítico. Em vez de usar redes separadas, uma abordagem comum é usar um extractor de características para evitar computações redundantes. Isso envolve usar camadas compartilhadas entre as redes para pré-processar a entrada, com essas camadas compartilhadas sendo treinadas conjuntamente. Redes: A seguir estão as configurações para os extractores de características que usamos, mantendo a consistência para ambos os algoritmos de treinamento.
Para o PPO, utilizamos dois camadas ocultas com 64 e 32 neuronios para as redes de ator e crítico após o extrator de características: – MLP: para o DQN, a rede MLP é uma rede simples com três camadas ocultas com 512, 128 e 64 neuronios. Para o PPO, o extrator de características é uma rede com dois camadas ocultas com tamanhos de 512 e 128. – LSTM: utilizamos duas camadas LSTM com tamanho de 512 para o Tri-encoder e 768 para a arquitetura BM25 para corresponder ao tamanho dos vetores da sequência de embedding E, com dropout de 0,1. Além disso, há uma camada oculta com 128 nós. – GRU: semelhante ao LSTM, mas com as camadas LSTM substituídas por camadas GRU. – Transformer: para o extrator de características do PPO, optamos por duas camadas de encoder do Transformer com dois heads de atenção, dimensão de características igual ao tamanho da entrada da embedding e a mesma camada oculta com 128 neuronios. Para o DQN, usamos a mesma arquitetura com uma camada adicional de 64 neuronios.

5 Resultados e Discussão

5.1 Bases

Os resultados completos das nossas bases estão fornecidos na Tabela 1. Para o retificador BM25, é evidente que recuperar tabelas é menos eficaz do que recuperar textos. Essa desigualdade resulta na sequência ótima de ações sempre ser a recuperação de textos e nunca tabelas, o que resulta em uma pontuação F1 de 19,03. Para o Tri-encoder, os resultados são mais balanceados. A sequência ótima de ações envolve recuperar textos, seguida de tabelas e, em seguida, recuperar textos novamente, resultando em uma pontuação F1 total de 8,24. É notável, no entanto, que o desempenho do Tri-encoder ao recuperar textos é consideravelmente inferior em comparação ao BM25.
59 10.52 BM25 A2 A1 A1 A3 5,15 8,08 BM25 A2 A1 A2 A3 3,61 6,17 BM25 A2 A2 A1 A3 3,21 5,45 BM25 A2 A2 A2 A3 2,08 4,15 Tri-encoder A1 A3 - - 3,52 6,32 Tri-encoder A2 A3 - - 2,94 4,41 Tri-encoder A1 A1 A3 - 4,34 7,11 Tri-encoder A1 A2 A3 - 4,83 7,54 Tri-encoder A2 A1 A3 - 5,42 7,89 Tri-encoder A2 A2 A3 - 2,66 4,40 Tri-encoder A1 A1 A1 A3 4,65 7,35 Tri-encoder A1 A1 A2 A3 4,79 7,56 Tri-encoder A1 A2 A1 A3 5,55 8,24 Tri-encoder A1 A2 A2 A3 3,66 5,81 Tri-encoder A2 A1 A1 A3 5,51 8,06 Tri-encoder A2 A1 A2 A3 4,29 6,45 Tri-encoder A2 A2 A1 A3 4,07 6,24 Tri-encoder A2 A2 A2 A3 2,89 4,52 Tabela 1. Resultados de base para o conjunto de dados de validação OTT-QA utilizando nosso framework com BM25 e Tri-encoder como recuperador. Nesse experimento, supomos que o agente sempre toma a mesma sequência de ações, independentemente da pergunta. A1 corresponde a recuperar textos, A2 é recuperar tabelas, e A3 chama o leitor para gerar a resposta.

5,2 Resultados para Agente de Aprendizado por Reforço Profundo

Presentamos resultados detalhados para os agentes de aprendizado por reforço treinados: BM25

O desempenho dos algoritmos treinados utilizando o recuperador BM25 é detalhado na Tabela 2. Entre os vários algoritmos de treinamento, o agente PPO equipado com uma rede neural transformadora demonstrou resultados notáveis. Superando outras configurações, esse agente alcançou o mesmo nível de desempenho do melhor baseline, que envolvia selecionar consistentemente a ação A1, independentemente da pergunta ou contexto.

12 M. José et al.

Resultados com o Algoritmo de Tri-encoder

Os resultados obtidos com os algoritmos que utilizam o recuperador Tri-encoder são apresentados na Tabela 2. Notavelmente, o melhor agente emergiu do algoritmo PPO, empregando a rede neural MLP, demonstrando um score F1 de 6,44%.
01 Tri-encoder DQN LSTM 3,52 5,68 Tri-encoder DQN GRU 3,79 6,13 Tri-encoder DQN Transformer 3,34 5,09 Tri-encoder PPO MLP 3,38 5,47 Tri-encoder PPO LSTM 3,75 5,53 Tri-encoder PPO GRU 2,94 4,93 Tri-encoder PPO Transformer 2,80 4,34 Tabela 2. Resultados para redes neurais e algoritmos de treinamento no conjunto de dados OTT-QA. 5.3 Discussão Os resultados de base mostram uma vantagem significativa para a recuperação de texto em relação à busca em tabelas com BM25, embora algumas perguntas exijam informações que estão estritamente apenas em tabelas. Isso pode ser devido à dificuldade do BM25 em lidar com tabelas estruturadas, onde a presença de colunas e tipos de dados são cruciais. Apesar de sua popularidade em soluções de OTT-QA, o Tri-encoder alcançou pontuações F1 abaixo de 10%, possivelmente devido à falta de treinamento multi-hop e à dependência de dados de recuperação de texto de outros conjuntos de dados, pois o OTT-QA falta essa informação. Os agentes DRL mostraram desempenho comparável ou ligeiramente abaixo do melhor baseline, especialmente para BM25, frequentemente favorecendo a recuperação de texto, o que se alinha com a estratégia de baseline ótima. A recuperação de tabelas provou ser ruim, especialmente na busca iterativa, pois as tabelas frequentemente contêm dados excessivos com informações relevantes tipicamente em uma única linha. Essa questão pode ser mitigada ao quebrar as tabelas em linhas ou conjuntos de conteúdo menores, como sugerido por outros pesquisadores [14]. O algoritmo PPO geralmente mostrou desempenho ligeiramente inferior, com uma exceção: o agente PPO com uma rede neural Transformer para BM25, que alcançou o melhor desempenho convergindo para a solução trivial de apenas escolher a melhor QA com Textos e Tabelas através do Aprendizado por Reforço Profundo 13 ação em média (recuperando textos). A escolha da arquitetura de rede neural teve um impacto mínimo, indicando a necessidade de experimentos adicionais. Nossa melhor agente DRL alcançou uma pontuação F1 de 19,03 usando BM25, aproximando-se da pontuação de 20,7 da solução Iterative-Retrieval (esparso) + Cross-Block, que inspirou nossa abordagem. No entanto, nossa abordagem densa alcançou uma pontuação F1 de 8.
24, em comparação com os 18,5, sugerindo que o ajuste de Tri-encoder requer refinamento adicional. Nossa solução apresenta algumas limitações. A natureza exploratória do Aprendizado por Reforço (RL) e o comportamento estocástico de redes neurais se beneficiam de múltiplos testes executados com média. No entanto, devido a restrições de tempo - cada execução de treinamento levou mais de uma semana - não foi possível realizar múltiplos testes. Além disso, utilizamos apenas 100 mil passos de treinamento com o recuperador BM25. Esse número de passos pode ser insuficiente considerando o conjunto de dados com 41.469 perguntas de treinamento, cada uma potencialmente requerendo até três passos. Finalmente, é importante ressaltar que o desempenho do Tri-encoder foi afetado por não ter sido treinado para recuperação iterativa. 6 Conclusão e Trabalho Futuro Neste trabalho, introduzimos um sistema inovador projetado para lidar com questões multi-passo em textos e tabelas. Nossa abordagem se baseia em um sistema de tomada de decisão DRL que seleciona iterativamente entre três módulos distintos: Retriever de Texto, Retriever de Tabela e Leitor. O nosso melhor sistema alcançou uma pontuação F1 de 19,03, competindo de forma próxima com o sistema Iterative-Retrieval (esparso) + Cross-Block não-DRL, que pontuou 20,7. Destacamos que o objetivo aqui é explorar estratégias sequenciais; outras estratégias (não sequenciais) também foram exploradas com sucesso - por exemplo, COS, que alcançou uma pontuação F1 de 63,2 no conjunto de desenvolvimento. Apesar do desempenho relativamente inferior, acreditamos que há valor significativo em soluções que explorem decisões sequenciais, pois elas têm potencial para melhorias modulares ao refinar módulos de sistema variados. Soluções que combina decisões sequenciais com módulos não sequenciais parecem ser a rota mais promissora para explorar em trabalhos futuros; aqui exploramos o limite de desempenho das decisões sequenciais usando aprendizado por reforço.
O trabalho futuro também deve incluir a implementação da abordagem Joint-Reranking [13], que utiliza uma arquitetura baseada em BERT para avaliar e selecionar as principais passagens, melhorando a capacidade do sistema para eliminar excessos de passagens e tabelas da memória. Estamos também explorando métodos alternativos para gerar observações para o agente. Atualmente, estamos calculando a média das embeddings de cada bloco, mas estamos considerando codificar todas as passagens recuperadas como uma sequência. Além disso, integrar nosso agente DRL com a arquitetura COS, que se destaca em QA OTT ao empregar estratégias de recuperação diversificadas, pode melhorar o desempenho. O agente DRL determinaria os caminhos de recuperação ótimos com base na pergunta e nas passagens recuperadas. Agradecimentos. Esta pesquisa foi realizada com o apoio da Itaú Unibanco S.A. através do programa de bolsas Programa de Bolsas Itaú (PBI); também é apoiada em parte pela Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES), Código de Financiamento 001, Brasil. Os autores gostariam de agradecer ao Centro de Inteligência Artificial (C4AI-USP), com apoio da Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP) sob o número de grant 2019/07665-4 e pela IBM Corporation. Paulo Pirozelli é apoiado pela FAPESP grant 2019/26762-0. Qualquer opinião, achado e conclusão expressa nesse manuscrito são da responsabilidade dos autores e não necessariamente refletem as vistas, política oficial ou posição da Itaú-Unibanco, FAPESP, IBM e CAPES.