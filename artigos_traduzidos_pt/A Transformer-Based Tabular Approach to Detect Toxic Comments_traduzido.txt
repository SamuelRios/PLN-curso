A Abordagem com Base em Transformer para Detectar Comentários Tóxicos Ghivvago Damas1[0000−0001−5466−6607], Rafael Torres Anchiêta2[0000−0003−4209−9013], Raimundo Santos Moura1[0000−0002−1558−3830], e Vinicius Ponte Machado1[0000−0003−3391−8443] 1 Universidade Federal do Piauí, Brasil {ghivvagodamas.ufpi,rsm,vinicius}@ufpi.edu.br 2 Instituto Federal do Piauí, Brasil rta@ifpi.edu.br Resumo. Nos últimos anos, houve um aumento significativo de linguagem tóxica e odiosa em plataformas de mídias sociais, tornando-se profundamente enraizada nas interações online. Este problema atraiu a atenção de pesquisadores de várias áreas acadêmicas, levando-os a estender seu foco para incluir disciplinas como Processamento de Linguagem Natural, Aprendizado de Máquina e Linguística, além de áreas tradicionais como Direito, Sociologia, Psicologia e Política. Este artigo apresenta uma abordagem para detectar linguagem tóxica e odiosa em mídias sociais usando Aprendizado Profundo Tabular. O objetivo é aplicar e avaliar o desempenho do modelo FT-Transformer em detectar conteúdo odioso e tóxico em comentários textuais em português brasileiro. Um aspecto importante desta pesquisa envolve o uso de modelos de embedding modernos como embebedores de linguagem e modelos de linguagem, avaliando seu desempenho com o FT-Transformer, um modelo tabular com base em transformer. O cenário experimental utiliza uma versão binária do conjunto de dados ToLD-Br. Nossa abordagem alcançou uma taxa de acurácia de 76% e uma taxa de F1-macro de 75% usando o modelo de text-embedding-3-large da OpenAI. Palavras-chave: Linguagem Tóxica e Odiosa · Aprendizado Profundo · FT-Transformer · Modelos de Embedding · Classificação de Texto. 1 Introdução Com o aumento das plataformas de mídias sociais online, níveis crescentes de toxicidade e comportamento odioso tornaram-se uma preocupação crescente. Embora as mídias sociais tenham revolucionizado a comunicação em muitos aspectos, também levaram a problemas sociais além da linguagem odiosa, incluindo cyberbullying e desinformação.
A anonimidade das plataformas online criou espaços públicos virtuais onde a intolerância prospera, realçando a necessidade de estratégias para abordar o discurso odioso e o conteúdo tóxico, bem como gerenciar os impactos negativos do uso de mídias sociais [39]. Detectar ódio e toxicidade em comentários em plataformas de mídias sociais apresenta um desafio complexo. Apesar de serem identificados como danosos, ilegais ou mesmo criminosos, esses comentários costumam atrair uma significativa interação e são frequentemente ignorados pelos algoritmos das redes sociais [41], o que significa que as empresas de mídias sociais estão plenamente cientes da natureza do discurso odioso e suas implicações, no entanto, os mecanismos aplicados para garantir as políticas de moderação são muito limitados ou ineficazes [43]. Pesquisadores estão desenvolvendo algoritmos e estratégias avançadas para detectar e filtrar o discurso odioso e a toxicidade em mídias sociais para superar essas limitações nas políticas de moderação atuais. Além disso, é essencial compreender as complexidades desse problema, incluindo o reconhecimento de que não todos os linguagens ofensivas necessariamente constituem discurso odioso ou toxicidade [18]. Fatores como contexto, a conexão entre o falante e o público-alvo, dinâmicas dentro de grupos sociais, a plataforma de mídias sociais utilizada e o tempo jogam um papel importante na identificação precisa de linguagem danosa, incluindo suas subformas, como discurso odioso [27]. Desafios incluem distinguir entre discurso contra o ódio, linguagem depreciativa usada para enfatizar e formas mais sutis de ódio e toxicidade, como ironia e metáforas [23]. A estrutura da comunicação linguística e sua relação com a percepção humana são fatores chave para desvendar ódio e toxicidade online. Devido às dificuldades apresentadas nessa tarefa, pesquisadores estão investigando diferentes estratégias computacionais para abordá-la e desenvolveram abordagens além da supervisão plena.
Essas estratégias abrangem uma variedade de técnicas, desde métodos clássicos como Frequência de Termos - Frequência Inversa de Documentos (TF-IDF) e Sacos de Palavras (BoW) até representações vetoriais, arquiteturas de aprendizado profundo e modelos baseados em Transformadores [27]. Métodos modernos avançaram para incorporar técnicas híbridas e abordagens multimodais [19], Modelos de Linguagem (LMs), Aprendizado de Máquina Grafica [37] e o uso de Grandes Modelos de Linguagem (LLMs) [24], mostrando resultados promissores. Apesar das várias abordagens existentes, há uma falta de investigação sobre abordagens para dados estruturados, como dados tabulares. Este trabalho olha para o Aprendizado Profundo Tabular (TDL) como uma ferramenta viável e valiosa para trazer diversidade e alternativas inconvencionais para tarefas de classificação de texto. Aplicar modelos de TDL para detecção de discurso de ódio pode fornecer vários benefícios, incluindo aqueles encontrados nos modelos mais comumente usados (SVM, XGBoost, LightGBM e Redes Neurais) e aqueles que superam suas limitações ao lidar com características heterogêneas brutas, como textos, sequências, imagens, áudio e embeddings. Integrar com outros modelos de aprendizado profundo ou métodos de representação complexos também é uma limitação em muitos métodos tradicionais, principalmente em Árvores de Decisão de Boosting de Gradientes (GBDTs), além de baixa flexibilidade e incapacidade de trabalhar com complexidade e dimensionalidade de bancos de dados relacionais modernos [15]. Este artigo apresenta uma abordagem para detectar discurso de ódio e comentários tóxicos no português brasileiro, empregando o modelo FTT (FT-Transformer) [13] como classificador binário. Encoders de texto externos, ou Text Embedders (TE) [15], são aplicados ao modelo FTT. Esses TE podem ser alimentados por modelos de embeddings modernos e Modelos de Linguagem Treinados com Antecedência (PTLMs) para transformar texto bruto ou tokenizado em representações embedded significativas conhecidas como embeddings. Essa abordagem tabular baseada em Transformador foi avaliada usando o conjunto de dados ToLD-Br [17], onde vários modelos foram usados para gerar embeddings.
Apesar de os resultados do Aproacho Tabular Baseado em Transformadores para Detecção de Comentários Tóxicos não terem superado todos os métodos de detecção de discurso de ódio, em comparação com outros métodos, esse abordagem mostrou resultados competitivos, pois requer menos recursos computacionais, permite um treinamento mais rápido e não utiliza técnicas de otimização de saída. A estrutura desse artigo é a seguinte: Seção 2 fornece uma visão geral concisa do trabalho relacionado. A Seção 3 descreve nosso abordagem desenvolvida em detalhes. Na Seção 4, apresentamos os cenários experimentais e conjuntos de dados, seguidos de uma análise dos resultados. Finalmente, a Seção 5 conclui o artigo e destaca direções de pesquisa futuras potenciais.

2 Trabalho Relacionado

A pesquisa inicial sobre discurso de ódio e linguagem ofensiva abriu caminho para explorações subsequentes. Trabalhos pioneiros de Chen et al. [8], Burnap e Williams [6], Waseem e Hovy [42] e Nobata et al. [22] definiram e analisaram discurso ofensivo e tóxico, comportamento de usuários e moderação de mídias sociais. Para o português brasileiro, contribuições significativas para a detecção de discurso de ódio, tóxico e prejudicial, e análise de texto foram feitas por Almeida et al. [1], Pelle et al. [25], Bispo [4], Silva e Serapiao [33], Leite et al. [17] e Fortuna et al. [12]. Esses estudos aplicaram técnicas de aprendizado de máquina, métodos de representação de características textuais e análise de discurso para detectar e analisar comentários tóxicos e ofensivos em mídias sociais, avançando o campo da detecção automática de discurso de ódio. O trabalho de Almeida et al. [1] e Pelle et al. [25] expandiu o trabalho sobre discurso ofensivo e prejudicial online. O estudo de Almeida et al. [1] propôs uma estratégia de identificação de discurso de ódio usando quantificadores de Teoria da Informação, alcançando uma taxa de precisão F1 de 86%, 84% e 96% para classificar discurso de ódio, ofensivo e regular. O estudo não se concentrou em discurso de ódio na língua portuguesa.
No entanto, ainda fez contribuições valiosas para a comunidade científica e foi útil para pesquisas futuras de conteúdo em português. Pelle et al. [25] introduziram o Hate2Vec, um classificador baseado em ensemble para detectar comentários ofensivos em plataformas da web, desempenhando bem com conjuntos de dados em inglês e português, comparado ao classificador BoW tradicional, alcançando uma taxa de F acima de 90%. Avanços subsequentes na detecção de discurso de ódio foram feitos por Bispo [4] e Silva e Serapiao [33]. Eles desenvolveram classificadores usando arquiteturas LSTM e CNN, alcançando precisão significativa com embeddings como Wang2Vec e GloVe. Bispo [4] criou especificamente um classificador translingual de inglês para português usando GBDTs, com precisão de F1-score variando entre 72% e 91%. Em contraste, Silva e Serapiao [33] utilizaram uma arquitetura CNN para identificar discurso de ódio, alcançando taxas de F1-score e precisão entre 82,64% e 96,74%. Diferentes técnicas de otimização foram empregadas para conjuntos de dados variados, como Adam para OffComBR [10] e RMSprop para HLPHSD [11]. O trabalho de Leite et al. [17] introduziu o conjunto de dados ToLD-br para comentários tóxicos em português brasileiro. Ele usou BERTimbau fine-tuned e BERT multilíngue para classificação, alcançando uma taxa de F1-macro de 76%. Fortuna et al. [4] G. Damas et al. [12] estudaram as capacidades de generalização de classificadores para discurso de ódio, toxicidade, linguagem abusiva e ofensiva, encontrando pouca generalização com conjuntos de dados multilíngues e BERT, melhor generalização de inglês para português com uma taxa de F1-macro de 67% e a melhor generalização entre conjuntos de dados em inglês com uma taxa de F1-macro de 70%. Saraiva et al. [32] introduziram uma abordagem semi-supervisionada baseada em grafo de nós para detectar comentários tóxicos com o conjunto de dados ToLD-Br. Seu método utiliza uma rede de grafo heterogêneo não direcionado e pesado (HGN) com embeddings de 100 dimensões GloVe para a língua portuguesa e alcança uma taxa de F-macro de 73% usando apenas 10% do ToLD-Br.
Eles utilizaram um Classificador de Boost de Gradient com Consistência de Generalização de Tamanho (LGC) como método de transdução. Pesquisas recentes exploraram várias modelos de aprendizado de máquina e modernos modelos de embedimento, como embeddings de Linguagem Grande (LLMs) e SBERT [28], para detectar discurso tóxico e odioso em mídias sociais. Esses modelos provaram ser eficazes em tarefas como classificação, agrupamento e reordenação em NLP. Estudos enfatizaram o valor dos modelos de embedimento modernos em melhorar tarefas com base em texto e seu impacto geral em NLP [20]. Além disso, pesquisas mostraram a eficácia do modelo de Embedimento de Sentença de BERT Linguística-Independente em detecção de discurso odioso translingual e multilingue [31]. Outros métodos usaram técnicas diferentes com LLMs, incluindo prompting e classificação end-to-end. Um estudo de Oliveira et al. [24] explorou o módulo ChatCompletion e 2 prompts do OpenAI ChatGPT (GPT-3.5-turbo) [5] para avaliar o desempenho do GPT em comparação com modelos baseados em BERT em vários conjuntos de dados. O GPT alcançou pontuações F1 de 73% como classificador zero-shot no conjunto de dados ToLD-Br e 74% quando promptado no cenário experimental de cross-dataset com o conjunto de dados balanceado HLPHSD. Em outra investigação de da Rocha Junqueira et al. [29], uma comparação entre modelos baseados em BERT - BERTimbau e Albertina PT-BR foi realizada. Apesar de esforços de fine-tuning, o Albertina PT-BR não conseguiu igualar o desempenho do BERTimbau. O modelo base do BERTimbau alcançou uma pontuação F1 de 88%, enquanto o modelo grande alcançou 89%. Em contraste, o modelo base do Albertina PT-BR apenas alcançou uma pontuação de 74%. O estudo de da Silva Oliveira et al. [34] examinou o desempenho de LLMs como GPT-3.5-turbo e Maritaca AI Sabiá [26] em abordagens de aprendizado zero-shot e few-shot, comparando-os ao modelo BERTimbau.
A Sabiá demonstrou uma capacidade aprimorada e precisa para classificar textos contendo expressões coloquiais e slang, e pode identificar palavras agressivas e obscenas com base em um design específico de prompt. A análise do ToLD-Br revelou diferenças no desempenho entre o ChatGPT e o MariTalk, com o MariTalk apresentando uma precisão melhorada devido à sua compreensão mais profunda das sutilezas do português. Enquanto isso, Assis et al. [2] conduziram um estudo avaliando a capacidade de modelos de linguagem para distinguir discurso neutro, ofensivo e odioso em posts de mídias sociais. Os classificadores baseados em BERT-PT-BR superaram os chatbots no conjunto de dados HateBR [38], mas nos posts neutros, os chatbots superaram os classificadores baseados em BERT no conjunto de dados ToLD-Br. O ChatGPT e o MariTalk obtiveram escores F1 de 71% e 70%, respectivamente, que foram menores que os classificadores baseados em BERT, que variavam de 77% a 86%. Outras iniciativas exploram modelos TDL como alternativas para abordagens de classificação de texto binário não convencionais e têm ganhado atracção, com frameworks notáveis desenvolvidos por Younus e Qureshi [44] e Chopra et al. [9]. O trabalho de Chopra et al. [9] propõe um método automático para detectar discurso odioso em texto misto Hindi-ingles e texto Hindi em Devanagari, onde a arquitetura do framework emprega um modelo de classificador TabNet treinado em características extraídas usando um modelo baseado em BERT para línguas indiana (MuRIL) [16] a partir de dados transliterados mistos. Este estudo demonstrou que o TabNet com embeddings MuRIL foi eficaz para características de texto Devanagari, mesmo tendo sido treinado em dados transliterados. O framework de Younus e Qureshi [44] destacou os desafios da detecção de sexismo e enfatizou a importância do número de épocas de treinamento para melhorar o desempenho do modelo, pois o ByT5 aprende representações mais limpas e finas.
Desde que os modelos de TDL são eficazes em lidar com dependências complexas e heterogeneidade em dados tabulares, e os modelos de embedding modernos melhoraram a qualidade das representações vetoriais, a introdução de novos métodos para gerar embeddings contextuais de alta qualidade pode trazer descontinuidade ao cenário de detecção de discurso de ódio e outras tarefas de processamento de linguagem natural. Nossa pesquisa se destaca em relação a métodos anteriores ao abordar inicialmente a falta de estudos que integrem modelos de embedding modernos, técnicas de geração de embeddings e TDL para detectar comentários tóxicos e odiosos em português brasileiro. Destacamos a importância de testar diferentes tipos de embeddings modernos no processo de avaliação de nossa estratégia de classificação. TDL e embeddings modernos são a base de nosso método proposto, que busca aproveitar as vantagens de ambos para alcançar resultados melhores.

3 Método Proposto

Nesta seção, propomos uma abordagem metodológica para a classificação binária de comentários tóxicos e odiosos em redes sociais utilizando o Deep Learning Tabular (TDL). O desenvolvido aborda aplica o modelo FT-Transformer (Feature Tokenizer + Transformer ou FTT) [13], ajustes leves no modelo base FTT, integração à arquitetura de frame PyTorch para processamento de dados tabulares e incorporação de embeddings específicos de língua para o português brasileiro. A figura 1 ilustra um processo de cinco etapas de desenvolvimento desse método de TDL, que inclui: 1) Preparação de dados; 2) Geração de embeddings; 3) Treinamento do modelo; 4) Avaliação; e 5) Previsão. É resumido da seguinte forma: 1) Preparação de dados: O primeiro passo envolve a coleta e a normalização dos dados para garantir que sejam adequados para treinamento e tenham um formato de entrada consistente. Essa preparação remove colunas de dados indesejadas, forma os dados e normaliza o texto com o Enelvo [3].
PREPARO DE DADOS TREINAMENTO DE MODELO AVALIAÇÃO GERENCIAMENTO DE EMBEDDING PREDIÇÃO 1 2 3 4 5 Fig. 1. Um processo passo a passo para detectar comentários tóxicos e odiosos.

2) Gerenciamento de Embedding: Este passo envolve a conversão de dados textuais em representações de vetores utilizando uma ferramenta de gerenciamento de embedding baseada em modelo de linguagem (TE), que é um ferramenta de gerenciamento de embedding baseada em LM. A representação de saída coincide com o formato e a dimensão do dados de entrada e do tamanho de embedding do modelo. Para formatar a saída do TE em um formato amigável para pipelines de TDL, é necessário o passo de materialização, onde o TE pré-encode os dados de texto antes de serem moldados em um tensor final. A forma final do tensor é determinada pelas características dos dados de texto de entrada e pelo modelo de linguagem utilizado como encoder no TE.

3) Treinamento de Modelo: Os dados processados são então alimentados para o modelo FT-Transformer, que é treinado subsequentemente. O modelo FT-Transformer tem dois principais componentes: o Tokenizador de Características (FT) e o Transformer. Como o FT original não pode garantir uma operação suave e uma codificação adequada dos tipos de dados de texto no conjunto de dados, essa abordagem depende de sua versão aprimorada reformulada por Hu et al. [15], StypeEncoder. Depois de lidar e processar corretamente cada tipo de dados, as características são concatenadas em um vetor denso e passadas para o componente de transformer como entrada de dados de treinamento.

4) Avaliação: Depois do treinamento, o modelo é avaliado utilizando métricas como a pontuação F1 e a precisão. Essa avaliação ajuda a entender a eficácia da abordagem em detectar corretamente os comentários tóxicos, enquanto minimiza falsos positivos e negativos.

5) Previsão: Finalmente, o modelo treinado gera previsões em um conjunto de teste separado. Este passo valida a capacidade do modelo em se adaptar e realizar efetivamente em dados não vistos, demonstrando sua robustez e confiabilidade para outras tarefas de classificação de texto e aplicações do mundo real.
Ao seguir este abordagem estruturada, objetivamos aproveitar as capacidades do FT-Transformer e modelos de embedding avançados para melhorar a detecção de discurso tóxico e odioso em contextos de mídias sociais. Observe-se que há um desafio intrínseco em selecionar modelos de embedding apropriados para tarefas como a detecção de toxicidade e discurso odioso em comentários em português, pois modelos de linguagem podem introduzir ruído e prejudicar a qualidade dos embeddings, levando a desempenho inferior. Portanto, essa abordagem também emprega modelos de linguagem em português confiáveis como Modelos de Embedding Geradores (ou Embedders de Linguagem). 4 Experimentos e Resultados Esta seção se concentra em detectar conteúdo tóxico e odioso em comentários de mídias sociais por classificação de texto binária. Os cenários experimentais descritos nesse estudo descrevem as abordagens utilizadas para conduzir nossa pesquisa e avaliar a eficácia da nossa solução proposta. Nossa metodologia é testada usando o conjunto de dados ToLD-Br [17], que consiste em 21 mil comentários de mídias sociais. Para garantir consistência e facilitar comparação justa com outros métodos, dividimos os dados em 80% para treinamento, 10% para validação e 10% para teste, de acordo com o experimento original. Os parâmetros do modelo selecionados para esse experimento estão listados na Tabela 1. Além disso, foi implementado um agendador para reduzir gradualmente a taxa de aprendizado desde seu valor inicial até zero durante o treinamento. Tabela 1. Parâmetros do Modelo. Parâmetro Valor Parâmetro Valor canais 256 out_channels 2 camadas 12 taxa_de_aprendizado 0,0
0001 batch_size 512 função de perda 'Cross Entropy' épocas 100 otimizador 'AdamW' O Embedder de Texto utiliza modelos diferentes em cada rodada de treinamento, que são: i) SBERT: E5-large [40] e SBERTimbau-large3; ii) baseado em BERT: BERTim-bau [35], DeBERTa-V2-XL [14], e Albertina PT-BR [30]; e embedimento LLM: VoyageAI (voyage-large-2) e OpenAI (text-embedding-3-large). A Tabela 2 detalha esses modelos, incluindo o suporte de língua e as dimensões de embedding de saída. Tabela 2. Lista de Modelos de Embedimento. Rótulo do Modelo Suporte de Língua Tipo do Modelo Dimensões de Saída BERTimbau Monolingual BERT-based 1024 AlbertinaPTBR Monolingual BERT-based 1536 SBERTimbau Monolingual SBERT 1024 ME5Large Multilingual SBERT 1024 DeBERTaV2XL Multilingual BERT-based 1536 VoyageLarge2 Multilingual LLMem 1536 OpenAI-TE3-large Multilingual LLMem 1536 3 https://huggingface.co/rufimelo/bert-large-portuguese-cased-sts 8 G. Damas et al. Os experimentos realizados no nosso estudo seguem o cenário de acordo do anotador mais laxe descrito por Leite et al. [17]. Nesse cenário de acordo, o conjunto de dados contém 11.745 comentários não tóxicos e 9.255 comentários tóxicos, com uma razão na distribuição de classes de 1:1,2, o que faz com que o conjunto de dados ToLD-Br possa ser considerado relativamente balanceado. Após completar as rodadas de treinamento para cada modelo de embedimento, uma avaliação de desempenho pode ser feita com base nos métricas obtidas. A Tabela 3 apresenta a pontuação F1 para as classes tóxicas e não tóxicas, bem como a precisão geral do TE escolhido. Tabela 3. Avaliação de Métricas em diferentes Embedders de Texto. Embedder de Texto Tóxico Não-Tóxico Precisão BERTimbau 0,7101 0,7531 0,7333 AlbertinaPTBR 0,6826 0,7306 0,6906 SBERTimbau 0,7158 0,7429 0,7300 ME5Large 0,7378 0,7258 0,7319 DeBERTaV2XL 0,6637 0,6974 0,6814 VoyageLarge2 0,7143 0,7327 0,7238 OpenAI-TE3-large 0,7398 0,7740 0,7580 Nota: Valores Tóxico e Não-Tóxico representam a pontuação F1.
Com base nos resultados, o modelo OpenAI-TE3-large apresenta a maior precisão e F1-score para a classe tóxica, tornando-se a escolha mais robusta. O BERTimbau e o SBERTIMBAU são desempenhadores fortes entre os modelos monolingues, com o BERTimbau sendo eficaz para a classe não tóxica e o SBERTIMBAU apresentando melhor desempenho para a classe tóxica. Os modelos de embeddings multilíngues, como OpenAI-TE3-large, ME5Large e VoyageLarge2, mostram desempenho excelente. A Tabela 4 compara os resultados de diferentes abordagens desenvolvidas para detectar fala tóxica e odiosa utilizando o conjunto de dados ToLD-BR e compara a pontuação F1 e a precisão global de cada método. Tabela 4. Detecção de fala tóxica e odiosa: comparação de abordagens utilizando ToLD-BR. Trabalho Abordagem Pontuação F1 Precisão [17] M-BERT + aprendizado de transferência 0,76 - [32] GloVe + HGN + XGBoost 0,73 - [24] Prompt GPT 3,5 0,73 - [29] Albertina PT-Br Base 0,74 0,78 [29] BERTimbau Base 0,88 0,88 [29] BERTimbau Large 0,89 0,89 [34] Prompt Sabiá + 10 few-shots 0,73 - [34] Prompt GPT 3,5-turbo + zero-shot 0,74 - OpenAI-TE3-large + FTT 0,75 0,76 Nossos BERTimbau + FTT 0,73 0,73 ME5Large + FTT 0,73 0,73 Uma abordagem baseada em Transformador para detectar comentários tóxicos 9 Os resultados mostram que as abordagens mais bem-sucedidas para detectar ódio e toxicidade em comentários de mídias sociais envolvem fine-tuning extensivo, aprendizado de transferência ou aprendizado zero-shot, como BERTimbau Large e M-BERT como classificadores, e prompting GPT 3,5-turbo. No entanto, métodos que utilizam representações de grafo, embeddings estáticos e modelos de embeddings modernos também podem alcançar resultados fortes mesmo sem ajustes adicionais, fornecendo um baseline para melhorias futuras ou implementações híbridas, e isso foi demonstrado com nossa abordagem no setting OpenAI-TE3-large + FTT. A matriz de confusão apresentada na Figura 2 serve como uma avaliação subsequente da abordagem proposta, fornecendo insights adicionais sobre o desempenho do modelo sobre as classes (Tóxica e Não Tóxica). Fig. 2.
Matriz de Confusão para Classificação Toxicidade vs. Não-Toxicidade. A matriz de confusão revela que o modelo apresenta uma habilidade robusta para identificar com precisão comentários não-toxicos, como indicado pelo alto número de verdades negativas. Além disso, o modelo demonstra uma capacidade moderada para detectar comentários tóxicos, com 778 verdadeiros positivos. No entanto, os resultados também indicam classificações notáveis, com 301 falsos positivos e 194 falsos negativos, sugerindo espaço para melhoria na redução das taxas de classificação errada. Em resumo, o modelo apresenta uma precisão geral de 76,43%, com um bom equilíbrio entre precisão (72,08%) e recall (80,08%) para a classe tóxica. Nossa abordagem baseada em Transformer para dados tabulares está disponível em https://github.com/GhivvagoDamas/Tabular-Transformer-Toxic2024.

5 Conclusão e Trabalho Futuro Como demonstrado anteriormente, propusemos uma abordagem inédita para classificação de texto binária, que é especificamente projetada e treinada para dados estruturados em formato tabular. Essa abordagem produziu resultados impressionantes, detectando com sucesso discursos de ódio e toxicidade sem métodos de fine-tuning ou aprendizado de transferência complexos. Ela apresenta uma taxa de acurácia intrigante de 76% e uma pontuação F1 de 75% no conjunto de teste utilizando o modelo de embedding de texto OpenAI text-embedding-3-large como Embedder de Texto.

Antes dos experimentos, o ToLD-Br foi submetido a uma revisão adicional e avaliação, destacando preocupações sobre sua validade e confiabilidade. Problemas foram identificados com o rácio de imbalance nos cenários de acordo dos anotadores, possivelmente devido a bias no processo de anotação, resignificação de termos e uso estilístico de palavrões. Modelos de Aprendizado Profundo Tabular, como o FT-Transformer, excel em aprendizado multimodal e processamento de dados heterogêneos estruturados com ajustes mínimos. Embora o uso de um GPU seja importante para essa abordagem e implementações de outros modelos de TDL, não são necessários recursos computacionais extensos.
Este estudo destaca a importância dos modelos de TDL e de Embedding na detecção de discurso tóxico e odioso. Como os modelos de embedding modernos demonstraram eficiência de processamento aprimorada e resultados melhorados em diferentes aplicações de NLP [20], incorporá-los na abordagem proposta não é apenas adequado, mas também a escolha mais inteligente e assertiva. Para trabalhos futuros, pretendemos investigar mais sobre modelos de embedding modernos como BGE M3 (FlagEmbeddings) [7], SBERT, e como converter modelos de LLM de ponta como Sabiá [26], Aya [36] e outros modelos geradores em modelos de embedding poderosos e robustos com espaço de embedding refinado e compreensão contextual profunda. Nossas intenções incluem adaptar e desenvolver outras estratégias baseadas em TDL e ampliar a aplicabilidade de NLP, investigar técnicas de detecção e mitigação de bias semelhantes ao estudo de Nascimento et al. [21], e estratégias de melhoria do desempenho do modelo, como incorporar características contextuais adicionais, Aprendizado Contraste e Geração Augmentada de Recuperação (RAG).