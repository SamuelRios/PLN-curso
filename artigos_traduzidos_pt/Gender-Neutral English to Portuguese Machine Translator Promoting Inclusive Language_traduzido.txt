Tradutor de Máquina de Inglês Não-Gênero para Português: Promovendo Linguagem Inclusiva Ricardo Trainotti Rabonato1, Evangelos Milios2 e Lilian Berton1 1 Universidade Federal de São Paulo, Avenida Cesare Mansueto Giulio Lattes, nº 1201 - Eugênio de Mello, SP - Brasil 2 Universidade Dalhousie, Avenida University, nº 6050, Halifax, NS - Canadá trainotti.ricardo@unifesp.br, lberton@unifesp.br Resumo. A tradução por máquina (TM) desempenha um papel crucial na globalização, tornando o acesso à informação mais inclusivo, embora desafios persistam para línguas menos populares, como o português. Uma das principais desafios mais complexos na tradução automática para línguas como o português é a preservação precisa do gênero gramatical masculino e feminino. Há ainda situações em que a tradução não reflete adequadamente a igualdade de gênero, frequentemente reforçando estereótipos sociais. Nossa intenção é explorar abordagens para garantir a justiça na TM de inglês para português por meio de técnicas de pós-processamento, que visam aplicar alguma transformação ao output do modelo. Para isso, utilizamos o modelo MarianMT como nossa base, e o fine-tunamos usando um conjunto de sentenças de inglês-português gerado e cuidadosamente elaborado para mitigar o viés de gênero nas sentenças. Os resultados em métricas de disparidades de gênero, com base no conjunto de testes WinoMT para TM, como ∆G, ∆S e a precisão geral (preservando o gênero da entidade original), melhoraram significativamente com alguma queda no score BLEU (Bilingual Evaluation Understudy). Nossa pesquisa se concentra em abordar o viés de gênero na língua portuguesa. No entanto, também pode ser adaptada para outras línguas, pois é crucial garantir comunicação global verdadeiramente justa e inclusiva. Palavras-chave: NLP · Tradutor de Máquina · Justiça · Português. 1 Introdução O processamento de linguagem natural (NLP) desempenha um papel fundamental em nossa sociedade atual, sendo uma tecnologia essencial em várias áreas.
Ele permite que as máquinas compreendam, interpretem e gerem texto humano de forma semelhante à humana [21]. Essa capacidade é criticamente importante em uma era de dados massivos, onde a maioria da informação é desestruturada e textual. O Processamento de Linguagem Natural (NLP) é essencial para melhorar a eficiência e a precisão dos motores de busca, permitindo que os chatbots e assistentes virtuais sejam mais inteligentes [1, 23], análise de sentimento avançada em mídias sociais [17, 26], tradução automática [20] e classificação de texto [12, 18, 13]. Além disso, desempenha um papel crucial na acessibilidade, tornando a tecnologia mais inclusiva para pessoas com deficiências de comunicação. Em resumo, o NLP é uma tecnologia que impulsiona a inovação em muitos setores, facilitando a interação entre humanos e máquinas de forma mais natural e eficaz.

2 Rabonato et al.

Com base em avanços em aprendizado de máquina (ML) e NLP, os sistemas de tradução por máquina (MT) têm evoluído constantemente, fornecendo traduções cada vez mais precisas e contextuais [20, 8]. Eles são amplamente utilizados em várias áreas, desde traduções de documentos empresariais até comunicação internacional online, ajudando a superar barreiras linguísticas e promover a compreensão global. À medida que aumenta a demanda por comunicação global, a capacidade de traduzir eficientemente entre línguas menos amplamente utilizadas se torna cada vez mais importante. Embora as tecnologias de tradução por máquina tenham feito avanços notáveis em línguas amplamente faladas, traduzir para línguas menos comuns envolve vários obstáculos, como falta de dados de treinamento suficientes e diversidade linguística. Neste trabalho, objetivamos explorar a tradução automática para o português, que é a língua materna de mais de 230 milhões de pessoas ao redor do mundo1. Sua importância ultrapassa os limites de Portugal e Brasil e também é falada em vários outros países, como Moçambique, Angola, Cabo Verde e Timor-Leste.
Além disso, é uma das línguas oficiais de organizações internacionais, como a Comunidade dos Países de Língua Portuguesa (CPLP)2 e a União Europeia3, o que a torna um veículo crucial para diplomacia e comércio global. As regras de gênero gramatical presentes em algumas línguas são um desafio significativo para a tradução por máquina. É possível identificar três agrupamentos linguísticos [27]. O primeiro são línguas sem gênero (por exemplo, finlandês e turco), em que as distinções de gênero são mínimas e frequentemente limitadas a pares lexicais essenciais, como termos de parentesco ou de endereçamento. As línguas de gênero notional (por exemplo, dinamarquês, inglês) incluem línguas que distinguem entre gênero lexical (por exemplo, "mãe"/"pai") e gênero pronominal (por exemplo, "ela"/"ele", "ela"/"ele"). Finalmente, línguas de gênero gramatical (por exemplo, francês, grego, alemão e espanhol) possuem um sistema de classificação que atribui gênero masculino, feminino e, às vezes, neutro a substantivos, frequentemente com ligações semânticas a referentes humanos. A atribuição de gênero pode ser formal para objetos inanimados, mas é frequentemente dependente do sentido para referentes humanos. Essas línguas utilizam um sistema de acordo morfosintático, com inflexões de gênero se estendendo a diferentes componentes da fala, como verbos, determinantes e adjetivos [27]. Isso representa um desafio adicional para a tarefa de tradução automática, especialmente quando se traduz de línguas que não possuem gênero gramatical e línguas de gênero gramatical. Assim, traduzir profissões e termos relacionados ao gênero pode ser desafiador em muitas línguas, incluindo a tradução do inglês para o português [10]. Muitos problemas de gênero podem surgir nesse contexto: – Gênero Masculino Genérico: frequentemente, em inglês, termos como "engenheiro" ou "médico" são usados no gênero masculino genérico, sem distinguir o gênero. No entanto, ao traduzir para o português, é necessário decidir se o termo deve ser traduzido como "engenheira" ou "médica".
com/en/magazine/como-muitas-pessoas-falam-português-e-onde-ele-é-falado 2 https://www.cplp.org/ 3 https://european-union.europa.eu/principles-countries-history/languages_pt

Título Suprimido devido à Extensão Excessiva

3 será traduzido para masculino ou feminino, o que pode perpetuar estereótipos de gênero. – Ambiguidade de Gênero: Alguns termos em inglês, como "ator" ou "garçonete", não fazem distinção de gênero na língua original. No entanto, ao traduzi-los para o português, é necessário decidir se eles serão traduzidos de forma neutra ou com uma forma feminina correspondente, como "atriz" ou "garçom". – Termos de Endereçamento e Gênero: Em algumas profissões, termos de endereçamento ou títulos podem variar com base no gênero de uma pessoa. Por exemplo, em inglês, usamos "Sr." e "Sra." (Senhor e Senhora) para fazer essa distinção. No entanto, no português, essa distinção é mais complexa, com variações como "Sr." e "Sra." ou "Dr." e "Dra." dependendo do contexto, a tradução precisa ser cuidadosamente escolhida. – Profissões Tradicionalmente de Gênero Único: Algumas profissões têm sido historicamente associadas a um gênero único. Por exemplo, "enfermeira" costumava ser predominantemente feminina, enquanto "piloto" era considerada uma profissão masculina. Quando traduzindo esses termos, é importante considerar como as normas culturais e de gênero estão evoluindo e se adaptar a essas mudanças. A Tabela 1 mostra dois exemplos de traduções contendo viés de gênero. Na primeira sentença, a ocupação "mecânico" deve ter sido traduzida como "a mecânica", que é a forma feminina em português, pois está relacionada ao pronome "ela" (ela). O mesmo ocorre na segunda sentença, onde "agricultor" também deve ser traduzido na forma feminina como "a fazendeira" por motivos semelhantes. Sentença fonte Tradução portuguesa O mecânico deu um presente ao funcionário porque ela ganhou na loteria.
O CEO ajudou a enfermeira porque precisava de ajuda.

Tabela 1. Exemplo de viés de gênero em tradução por máquina utilizando modelo pre-treinado MarianMT. As entidades de gênero gramatical são distinguidas por cores: azul representa entidades e pronomes masculinos, vermelho signiﬁca entidades e pronomes femininos, e laranja denota entidades e pronomes neutros. Neste trabalho, objetivamos reduzir esse tipo de viés de gênero existente na tradução automática inglesa-portuguesa com foco na justiça. A justiça é a propriedade que os algoritmos e sistemas não perpetuam preconceitos ou discriminação desigual [3, 14, 5, 25]. Isso envolve criar e implementar métricas e estratégias para mitigar viés e garantir que as decisões automatizadas sejam imparciais e equitativas para todas as pessoas, independentemente de sua origem étnica, gênero, idade ou outras características protegidas. A justiça é essencial para promover ética e igualdade na era digital. Trabalhos anteriores exploraram preocupações de justiça em tradução, como [24, 30, 16]. No entanto, nenhum delas abordou métodos de pós-processamento ou explorou o idioma português. Portanto, desenvolver sistemas de tradução por máquina capazes de lidar com línguas desafiadoras é uma área promissora de pesquisa, com implicações significativas para promover diversidade cultural e comunicação global inclusiva. As principais contribuições do nosso trabalho são resumidas abaixo:

– Propomos um abordagem de fine-tuning de pós-processamento baseada no modelo MarianMT para tradutores de máquina ingleses-portugueses neutros de gênero. O pós-processamento fornece a oportunidade de corrigir erros ou viés que podem ter surgido durante a coleta de dados ou modelagem.
– Conseguiu-se melhorias nos métricas de disparidades de gênero utilizadas em MT, como ∆G, ∆S e a precisão geral, em comparação ao modelo de base, embora tenha ocorrido um declínio no BLEU (Bilingual Evaluation Understudy), o que é esperado dado o trade-off entre justiça e precisão.
– Criamos um conjunto de dados composto por 10.400 frases, inspirado nos utilizados no teste WinoMT, mas ampliado com modificações personalizadas. Esse conjunto de dados será tornado público.

– Destacamos a importância do estudo da justiça em sistemas de processamento de linguagem natural, especialmente para línguas com recursos mais limitados, como o português. É crucial garantir acesso equitativo e representação em tecnologias de inteligência artificial para comunidades linguísticas subrepresentadas. Este trabalho está organizado da seguinte maneira. A Seção 2 menciona outros trabalhos que anteriormente exploraram a tradução automática. A Seção 3 apresenta conceitos importantes relacionados à justiça em aprendizado de máquina. A Seção 4 apresenta a metodologia adotada no trabalho, os conjuntos de dados e os algoritmos utilizados. A Seção 5 mostra os resultados alcançados na tradução automática. A Seção 6 apresenta as observações finais.

2 Trabalhos Relacionados Alguns trabalhos sugerem que ferramentas de tradução automática podem ser utilizadas para explorar a questão do viés de gênero em inteligência artificial mediante o uso de línguas sem gênero. Foi utilizado uma lista exaustiva de títulos de ocupações fonte do Bureau of Labor Statistics (BLS) dos EUA para construir frases como "Eles são um Engenheiro" (com "Engenheiro" substituído pelo título de ocupação específico de interesse) em doze línguas sem gênero distintas, incluindo húngaro, chinês, iorubá e outras [24]. Essas frases são subsequentemente traduzidas para o inglês utilizando a API do Google Translate, e coletam dados sobre a frequência de pronomes femininos, masculinos e sem gênero nos resultados das traduções. Seus achados revelam um viés pronunciado em direção a padrões masculinos dentro do Google Translate, especialmente em campos caracterizados por representação de gênero imbalançada ou estereótipos, como profissões de Ciência, Tecnologia, Engenharia e Matemática (STEM).
Comparamos esses achados com dados da BLS sobre a distribuição real de gênero em cada título de emprego, ilustrando que o Google Translate falha em replicar a demografia de gênero do mundo real. Os autores melhoram os sistemas de tradução de máquina neural (NMT) introduzindo informações de gênero em seu trabalho [30]. Eles criaram conjuntos de dados extensivos contendo informações do falante para 20 pares de línguas e realizaram experimentos que integraram dados de gênero ao NMT para múltiplos pares de línguas. Eles empregaram o toolkit OpenNMT-py, estruturado como um codificador-decodificador de sequência para sequência, utilizando unidades recorrentes LSTM. Eles demonstram que a inclusão de uma característica de gênero em um sistema NMT resulta em uma melhoria significativa na qualidade da tradução para seletores pares de línguas. Outros trabalhos propuseram uma abordagem desbiassada por gênero para MT. Uma abordagem de aprendizado adversário foi usada para reduzir o viés de gênero em um modelo de tradução de máquina seq2seq em [15]. Nessa abordagem, um modelo de previsão M com pesos W aprende a prever uma saída Y a partir de uma entrada X, enquanto permanece neutro em relação à variável protegida Z. O adversário A tenta prever Z a partir das previsões de saída do modelo ˆY . A abordagem do estudo para reduzir o viés de gênero se baseou em uma adaptação nuances do modelo Transformer, alvoando as embeddings de palavras nos componentes do codificador e do decodificador [16]. Para fortalecer a neutralidade de gênero, embeddings de palavras pré-treinadas foram introduzidas, resultando em uma variedade de modelos, cada um utilizando embeddings pré-treinados distintos, principalmente provenientes de GloVe. O estudo explorou múltiplos cenários experimentais, incluindo treinar modelos sem embeddings pré-treinados para permitir o aprendizado autônomo e incorporar embeddings pré-treinados, como GloVe padrão, GloVe HardDebiased e GloVe Neutro por Gênero (GN-GloVe), derivados do mesmo corpus.
Além disso, a investigação abordou a utilização específica de embeddings pré-treinados, examinando três casos: apenas encoder, apenas decoder e ambos encoder e decoder, para avaliar de forma abrangente seu impacto na mitigação da bias de gênero durante a tradução. Os autores avaliam o sistema proposto no Workshop on Machine Translation4 (WMT) tarefa de benchmarking inglês-espanhol. Este quadro holístico permitiu uma avaliação abrangente das estratégias de embeddings, esclarecendo meios efeitos de melhorar a justiça da tradução. Em seu trabalho sobre mitigação da sensibilidade a atributos protegidos como gênero e idade em classificação de sentimento, os autores avaliaram a tradução em sentido único como técnica [9]. Eles demonstram, em particular, que traduzir resenhas de produtos dinamarqueses para o inglês e de volta minimiza a disparidade de grupo em três estruturas de classificação distintas. Eles usaram dois modelos de linguagem pré-treinados diferentes, nomeadamente o modelo multilingue LASER e um modelo monolíngue BERT treinado para dinamarquês. Além disso, alguns classificadores foram empregados, incluindo vizinho mais próximo, regressão logística e (kernel gaussiano) máquinas de vetores de suporte (SVMs). Os autores descobrem que a tradução em sentido único no tempo de teste reduz a lacuna de justiça (até 47%), mas que o efeito desaparece para os melhores modelos (SVMs empilhados em representações BERT) quando tanto os dados de treinamento quanto os dados de teste são traduzidos para um idioma estrangeiro e de volta. 4 http://www.statmt.org/wmt13/
Definição de justiça

Os autores destacam que, ao abordar o tema de bias, pesquisadores em Inteligência Artificial, Engenharia de Software e comunidades de Direito propuseram mais de vinte diferentes noções de justiça nos últimos anos [31]. No entanto, não há consenso sobre a definição apropriada para situações específicas. Além disso, compreender as distinções intricadas entre numerosas definições é um desafio considerável. Um dos aspectos fundamentais da definição de justiça é o conceito de justiça por grupo e justiça individual. A justiça por grupo se concentra em garantir que os resultados sejam equitativos para diferentes grupos predeterminados (por exemplo, homens e mulheres). Por outro lado, a justiça individual busca tratar indivíduos semelhantes de forma semelhante, independentemente de sua filiação a um grupo. Apesar de parecerem estar em conflito, as medidas de justiça individual e por grupo não representam necessariamente princípios normativos distintos [2]. Além disso, as definições de justiça frequentemente giram em torno dos conceitos de anti-discriminação e igual oportunidade. A anti-discriminação visa prevenir o tratamento discriminatório contra qualquer grupo específico. A igual oportunidade, por outro lado, visa fornecer indivíduos de diferentes grupos com uma chance igual de um resultado positivo. Impacto Desigual e Tratamento Desigual são dois conceitos no campo da discriminação e do bias que ajudam a abordar e identificar diferentes formas de discriminação e desempenham um papel crucial na garantia de justiça e oportunidades iguais. O impacto desigual se refere a situações em que os resultados de um sistema afetam desproporcionadamente um grupo, mesmo se não houver bias explícito. O tratamento desigual se refere a situações em que indivíduos de diferentes grupos são tratados de forma diferente devido a bias ou discriminação.
Métricas de justiça

Muitas métricas de justiça se baseiam nos conceitos de uma matriz de confusão (mostrada na Tabela 2), uma representação tabular utilizada para avaliar a eﬀicácia de um algoritmo de classificação em termos de resultados positivos e negativos corretamente ou incorretamente preditos pelo modelo. Algumas métricas de justiça comuns incluem:

– Paridade Demográfica: Mede se as taxas de predição positivas (TP + FP/TP + TN + FP + FN) são iguais em diferentes grupos.
– Oportunidade Igual: Garante que a Taxa de Verdadeira Positiva (TP/TP + FN) seja a mesma em grupos.
– Oportunidades Igualizadas: Requer que as Taxas de Verdadeira Positiva (TP/TP + FN) e de Falsa Positiva (FP/FP + TN) sejam iguais em grupos.
– Paridade Preditiva: Verifica se a Precisão (FP/FP + TN) é a mesma em grupos.
– Equilíbrio de Taxa de Falsa Positiva: Garante que a Taxa de Falsa Positiva (FP/FP + TN) seja igual em grupos.
– Equilíbrio de Taxa de Falsa Negativa: Garante que a Taxa de Falsa Negativa (FN/FN + TP) seja igual em grupos.

Essas métricas ajudam a avaliar se um modelo é justo em diferentes grupos demográficos ou categorias. Abordagens

Garantir que os modelos tratem todos os grupos demográficos de forma equitativa exige consideração cuidadosa em todo o pipeline de aprendizado de máquina. Geralmente, três abordagens têm sido exploradas: pré-processamento, processamento em tempo real e processamento pós-processamento [6].

1. Pré-Processamento: No aprendizado de máquina consciente de justiça, o pré-processamento envolve modificar os dados de treinamento para mitigar bias antes do processo de aprendizado comece. Técnicas incluem reponderar amostras, alterar características para remover informações sensíveis e gerar dados sintéticos para balancear classes em diferentes grupos.
2. Processamento em tempo real: Esta etapa incorpora restrições de justiça diretamente no algoritmo de aprendizado.
Modificações podem incluir alterar a função objetivo para penalizar previsões injustas ou adicionar restrições que garantam tratamento equitativo em diferentes grupos demográficos. 3. Processamento Pós: Após o modelo ter sido treinado, técnicas de processamento pós ajustam as previsões do modelo para reduzir resultados injustos. Essas metodologias podem envolver recalibrar limiares de previsão ou aplicar transformações às probabilidades de saída para garantir equidade entre grupos. Quando se trata de combater bias em tarefas de Processamento de Linguagem Natural (NLP), o principal enfoque é abordar essas questões durante a fase de pré-processamento, como destacado no trabalho de [6]. Isso inclui ações como remover ou substituir palavras específicas, ajustar dicionários e aplicar técnicas não supervisionadas para balancear o conjunto de dados de treinamento. Bias e equidade em MT A avaliação de bias em traduções é frequentemente difícil de realizar porque não há um verdadeiro padrão de referência. No entanto, estudos como o de Rabonato et al. [4], intitulado "O homem é ao programador de computador como a mulher é ao dona de casa?", mostram que embeddings de palavras exibem bias que reproduzem estereótipos de gênero presentes na sociedade. No seu estudo, os pesquisadores começaram por selecionar ocupações estreitamente associadas às palavras "ela" e "ele" nos embeddings de palavras que estavam analisando. Para avaliar se essas ocupações selecionadas realmente refletiam estereótipos de gênero, os pesquisadores engajaram indivíduos para avaliar cada ocupação e determinar se ela transmitia estereótipos femininos, masculinos ou se era neutra em termos de gênero. Para quantificar o grau de estereotipicidade, eles empregaram uma escala de avaliação que variava de 0 a 10, onde pontuações mais altas indicavam uma associação mais forte com estereótipos de gênero. Ao analisar os resultados, o estudo encontrou uma correlação substancial entre as posições das palavras de ocupação selecionadas ao longo do eixo "ela-ele" nos embeddings de palavras e as pontuações atribuídas.
Em outras palavras, as propriedades geométricas das embeddings refletiam de forma próxima as julgamentos humanos sobre estereótipos de gênero. O WinoMT, um desafio apresentado para avaliar o viés de gênero em tradução por máquina utilizando a concatenação de Winogender e WinoBias, foi apresentado em [29]. O conjunto de dados de avaliação compreende 3.888 sentenças projetadas para investigar potenciais viés de gênero nas traduções. Em cada sentença, uma entidade primária, coreferente com um pronome, interage com uma entidade secundária e visa revelar tendências de viés de gênero no sistema de tradução. O conjunto tem um equilíbrio igual entre ambos os gêneros masculino e feminino, assim como entre atribuições de papéis de gênero estereotipadas (por exemplo, um médico homem) e não estereotipadas (por exemplo, uma engenheira mulher). A metodologia para avaliar sistemas de tradução por máquina começa calculando a precisão global, que é determinada pela avaliação do percentual de instâncias em que a tradução mantém com sucesso o gênero da entidade a partir da sentença original em inglês. Seus achados revelam que, em oito línguas diferentes, a maioria dos sistemas de tradução por máquina testados apresenta desempenho pobre em preservar a precisão de gênero. Mesmo o melhor modelo para cada língua apresenta desempenho que não é significativamente melhor do que adivinhação aleatória quando se trata de infletar corretamente o gênero nas traduções. ∆G representa a disparidade nos escores F1 entre sentenças que contêm entidades masculinas e aquelas que apresentam entidades femininas. Eles apontam que todos os sistemas avaliados, exceto um, apresentam melhor desempenho em papéis masculinos, o que pode indicar que esses são mais frequentes no conjunto utilizado para treinamento. Finalmente, ∆S quantifica a discordância na precisão ao traduzir o antecedente em sentenças com atribuições de papéis de gênero pró-stereotipadas e anti-stereotipadas. De acordo com os autores, os resultados desse métrica mostram que todos os sistemas testados apresentam um desempenho significativamente melhor quando apresentados com atribuições pró-stereotipadas (por exemplo,
A empregada doméstica feminina), sua performance piora quando traduzindo papéis anti-stereotípicos (por exemplo, um bibliotecário masculino). Embora o teste WinoMT inclua um conjunto diverso de oito línguas (espanhol, francês, italiano, russo, ucraniano, hebreu, árabe e alemão), é importante notar que o português não fazia parte da seleção.

Título Suprimido devido à Excessiva Longevidade

9 4 Metodologia

Essa seção detalhará a metodologia adotada. Assim, a Seção 4.1 detalhará as técnicas de justiça pós-processamento utilizadas, a Seção 4.2 detalhará os conjuntos de dados utilizados e a Seção 4.3 detalhará os passos para executar os experimentos.

4.1 Técnicas de Justiça Pós-Processamento

As técnicas de justiça pós-processamento são um método bem conhecido para abordar potenciais bias em saídas de modelo relacionados a variáveis ou subconjuntos protegidos no campo da justiça de aprendizado de máquina [6]. Essas técnicas se destacam por sua flexibilidade notável, pois não precisam de acesso aos modelos internos ou algoritmos; em vez disso, dependem apenas das previsões do modelo e dos dados de atributo sensível [6]. Essas técnicas são especialmente bem adaptadas a cenários "black-box", que são aqueles em que a pipeline de ML não é completamente transparente. Como abordagem pós-processamento, a fine-tuning apresenta promessa significativa em reduzir o bias de gênero na tradução de máquina. No contexto desse estudo, a fine-tuning envolve adaptar o modelo de tradução de máquina MarianMT pré-treinado utilizando um conjunto de dados balanceado por gênero. Ao incorporar um conjunto diverso de exemplos específicos de gênero na processo de fine-tuning, o modelo se torna mais sensível às nuances de gênero sem comprometer a fluência ou precisão da tradução.

4.2 Conjuntos de Dados

Em nosso esforço para reduzir o bias de gênero na tradução de máquina, adotamos uma abordagem estratégica que envolveu a criação de um conjunto de dados especializado para fine-tuning o modelo MarianMT.
Este conjunto de dados, composto por 100.400 sentenças paralelas em inglês e português, foi elaborado para equilibrar a redução do viés de gênero ao manter uma alta qualidade de tradução. Para alcançar isso, estrategicamente combinamos 90.000 sentenças do corpus CAPES TDC [28] e 10.400 sentenças artificiais, geradas especificamente para essa tarefa. O corpus CAPES é uma fonte confiável que inicialmente foi empregada no treinamento original do modelo MarianMT. Este corpus foi compilado a partir dos resumos de todas as teses e dissertações produzidas no Brasil entre 2013 e 2016. Essa escolha foi motivada pela intenção de preservar a qualidade de tradução e prevenir o overfitting que poderia ocorrer ao se depender exclusivamente de dados artificiais. Essas sentenças artificiais foram geradas com cuidado, inspiradas nas sentenças utilizadas no teste WinoMT, mas com modificações personalizadas. Ao combinar sentenças autênticas do corpus CAPES com exemplos artificiais projetados, buscamos equilibrar a manutenção da qualidade de tradução e o aprimoramento do modelo para abordar o viés de gênero. Essa abordagem reconhece a importância dinâmica da tradução no mundo real, garantindo que nosso processo de fine-tuning seja efeito na redução do viés de gênero ao manter os padrões de qualidade de tradução estabelecidos pelo modelo original.

4.3 Configuração Experimental A configuração experimental para essa pesquisa foi realizada no cluster "Béluga", um recurso de computação de alto desempenho fornecido pela Aliança de Pesquisa Digital do Canadá, equipado com recursos de hardware substanciais. A configuração de hardware incluiu quatro GPUs NVidia V100SXM2, cada uma equipada com 16GB de RAM. Em termos de software, o experimento foi realizado usando um conjunto estabelecido de ferramentas e bibliotecas. O idioma de programação primário foi o Python 3.10.1. O framework de aprendizado profundo PyTorch 2.0.1 foi empregado para o desenvolvimento e treinamento do modelo. A ferramenta Natural Language Toolkit (NLTK) 3.8 foi utilizada.
1 O pré-processamento de texto facilitado e análise linguística foram realizados, enquanto a biblioteca Pandas 2.0.3 ofereceu capacidades de manipulação de dados eﬁcientes. A biblioteca Transformers, versão 4.31.0, desempenhou um papel fundamental na facilitação do processo de fine-tuning do modelo Mar-iánMT, otimizando a integração de arquiteturas baseadas em transformers no fluxo de trabalho de pesquisa. Os parâmetros foram configurados para adaptar o processo de fine-tuning. A taxa de dropout da camada de decoder do modelo foi definida como 0,2, contribuindo para a regularização do modelo durante o treinamento. Os parâmetros de fine-tuning também foram definidos, abrangendo uma taxa de aprendizado de 1e-5, um tamanho de batch de 8 e um total de 10 épocas de treinamento. O otimizador selecionado para essa tarefa foi AdamW.

4.4 Avaliação de Tradução e Bias de Gênero

Para o processo de avaliação, desenvolvemos scripts personalizados inspirados na metodologia original WinoMT [29], adaptados ao nosso contexto de tradução inglês-português. Esses scripts extraem automaticamente o gênero gramatical atribuído à entidade primária em cada tradução. Seguindo essa extração, uma comparação é feita entre o gênero da entidade primária traduzida e o gênero anotado nos dados de padrão ouro. O objetivo é avaliar o grau até que nossos modelos de tradução sejam alinhados com o gênero da entidade primária, de acordo com as anotações de ouro. Os principais métricas de desempenho para avaliar o conjunto de dados WinoMT incluem ∆G, ∆S e a precisão geral de preservação do gênero de entidades durante a tradução, referida como "acc.". O estudo comparou a qualidade das traduções antes e após o fine-tuning usando o métrica BLEU (Bilingual Evaluation Understudy) [22] para determinar se isso teve um efeito negativo na qualidade da tradução. O BLEU é uma métrica bem estabelecida para avaliação de tradução por máquina que mede o sobreposição entre a tradução por máquina e a tradução de referência.
É calculado com base na precisão, que mede o percentual de n-gramas na tradução por máquina que também aparecem nas traduções de referência. O score BLEU varia de 0 a 1, com 1 sendo um match perfeito com as traduções de referência [22]. Utilizando o modelo MarianMT como nossa base, realizamos ajustes finos com um conjunto de dados composto por sentenças em inglês-português. Esse conjunto de dados foi auto-gerado para incluir sentenças que mitiguem o viés de gênero. No final, ao avaliar o modelo, observamos uma diminuição na precisão da tradução simultânea com uma redução no viés de gênero.

Resultados e Discussão

Os resultados experimentais destacam o sucesso em atingir o objetivo principal de redução do viés de gênero na tradução por máquina por meio de ajustes finos. Os três métricas de justiça, ∆G, ∆S e "acc." do teste WinoMT, exibiram resultados melhorados após ajustes finos, como mostrado na Tabela 3. Essas métricas, projetadas para avaliar a capacidade do modelo de produzir traduções justas e equitativas, demonstraram progresso claro na mitigação do viés de gênero, alinhado com o objetivo da pesquisa. Esses resultados positivos indicam que o processo de ajuste fine melhorou significativamente a capacidade do modelo de gerar traduções que respeitem a neutralidade e equilíbrio de gênero, um passo crucial para criar sistemas de tradução por máquina mais inclusivos e imparciais. No entanto, é importante notar que essas melhoras promissoras nas métricas de justiça foram acompanhadas por um trade-off, como previsto, em termos do score BLEU. A redução do score BLEU destaca o trade-off bem estabelecido entre justiça e precisão que frequentemente acompanha esforços para reduzir o viés em tradução por máquina. No entanto, o score de 0,38 do modelo ajustado indica que ele ainda produz traduções boas.
Enquanto o foco primário da pesquisa foi melhorar a justiça e mitigar o viés de gênero, essa troca de qualidade de tradução serve como um lembrete da delicada balança que existe entre esses dois objetivos e indica a necessidade de continuar o processo de pesquisa para alcançar resultados melhores. Pre-trained Fine-tuned ∆G 0.1367 0.0148 ∆S 0.1319 0.0871 Acc. 0.6021 0.7086 BLEU 0.54 0.38 Tabela 3. Resultados para métricas de viés e qualidade de tradução no modelo pre-treinado MarianMT e modelo MarianMT fine-tuned.

5.1 Discussão O modelo MarianMT fine-tuned traduz corretamente as ocupações "o mecânico" e "o agricultor" para as formas femininas em português, como mostra a Tabela 4. Isso sugere que o modelo é capaz de produzir traduções neutras ou sensíveis ao gênero quando lidar com termos ocupacionais gênero.

5 A maioria dos algoritmos de redução de viés danificam o desempenho do modelo de previsão no processo de torná-lo justo, o que é comumente referido como o trade-off entre justiça e precisão [7].

6 https://cloud.google.com/translate/automl/docs/evaluate 12 Rabonato et al. Sentença fonte Tradução portuguesa O mecânico deu um presente à funcionária porque ela ganhou a loteria. A mecânica deu um presente à funcionária porque ela venceu a lotaria. O CEO ajudou o enfermeiro porque ele precisava de ajuda. O CEO ajudou o enfermeiro porque ele necessitava de ajuda. Tabela 4. Exemplos de traduções geradas usando o modelo MarianMT fine-tuned. Os gêneros gramaticais das entidades são distinguíveis por cores: azul representa entidades e pronomes masculinos, vermelho denota entidades e pronomes femininos e laranja denota entidades e pronomes neutros. No entanto, apesar de ter reduzido o métrica ∆S em comparação ao modelo original, o modelo fine-tuned, como todos os sistemas testados em [29], também comete mais erros de tradução em sentenças com atribuição de gênero anti-stereotipada (por exemplo, uma agricultora feminina) do que em papéis estereotipados (por exemplo, um advogado masculino), embora em grau menor do que antes da fine-tuning.
Embora o foco do teste esteja relacionado à atribuição estereotipada de gênero em ocupações, o teste inclui uma pequena amostra (240 sentenças) em que o agente principal tem um papel de gênero neutro. Nessas situações, a capacidade do modelo finamente ajustado para preservar o gênero (acc.) é apenas de 0,2292, caíndo mesmo abaixo do modelo antes da fine-tuning, que é de 0,3125. Essa limitação é primariamente encontrada em sentenças como "O técnico informou ao cliente que eles poderiam pagar em dinheiro" ou "O funcionário deu feedback ao supervisor sobre o estilo de gerenciamento deles". Nessas situações, "eles" e "seus" são empregados para se referir a uma pessoa em um contexto genérico ou neutro de gênero. De acordo com [11], esse uso fornece uma referência geral a essa pessoa sem especificar seu gênero (como ela ou ele). Embora haja propostas para o uso de pronomes neutros de gênero em português, como "elu"7, o modelo de tradutor MarianMT e, consequentemente, a versão fine-tuned, não incluem essa variação.

5.2 Limitações e trabalho futuro

Enquanto os resultados dessa pesquisa demonstraram a eﬀicácia da fine-tuning como técnica para reduzir o viés de gênero na MT, é importante reconhecer algumas limitações nesse trabalho. O conjunto de dados utilizado nessa pesquisa foi especialmente projetado com base nas sentenças do teste WinoMT. Embora tenha sido projetado para ser bem adequado para esse teste, pode não representar plenamente as complexidades do viés de gênero em cenários de tradução "reais", o que pode requerer investigação adicional. Além disso, o teste se concentra principalmente no viés de gênero relacionado às ocupações, o que é indubitavelmente significativo. No entanto, o viés de gênero na MT ultrapassa os contextos ocupacionais, o que pode ser explorado em trabalhos futuros.

7 De acordo com [19], o pronome neutro de gênero mais comumente usado em português é "elu".
Título Suprimido devido à Extensão Excessiva

13 Com base nos resultados desse estudo, há várias vias para trabalhos futuros sobre redução de bias de gênero na tradução por máquina. É essencial continuar a pesquisa para melhorar os métodos de avaliação de equidade em traduções, garantindo que essas melhorias não reduzam significativamente a qualidade da tradução. Explorar conjuntos de dados alternativos ou ajustar parâmetros de fine-tuning pode ser uma via promissora. Futura pesquisa pode se concentrar em melhorar sistemas de tradução por máquina para incluir traduções que incorporem o português neutro. Isso ajudaria a preencher a lacuna em fornecer traduções que incluem escolhas de linguagem diversificadas por gênero e se alinhem com as normas sociais evoluindo. 6 Conclusões Ferramentas de Tradução por Máquina têm contribuído significativamente para a comunicação, permitindo interações globais sem problemas e pontes culturais através da Internet. No entanto, à medida que o campo da equidade em Aprendizado de Máquina cresceu, preocupações sobre a equidade se estenderam às ferramentas de MT, especialmente em relação ao seu potencial para propagar bias de gênero presente nos dados de treinamento. Neste trabalho, exploramos o tema do bias de gênero na tradução por máquina inglês-portugues e propusemos um método de redução de bias usando fine-tuning em um modelo pré-treinado MarianMT. A importância do estudo da equidade na tradução por máquina inglês-portugues está principalmente ligada à falta de foco de pesquisa sobre o idioma português. Portanto, a pesquisa desse tipo contribui para o avanço da pesquisa de equidade e o desenvolvimento de ferramentas de tradução com um grau reduzido de bias, beneficiando uma comunidade de mais de 230 milhões de falantes de português em todo o mundo. A linguagem é um poderoso instrumento para moldar percepções, reforçar estereótipos e influenciar atitudes. Bias de gênero presentes nos textos utilizados como dados de treinamento podem perpetuar representações danosas de indivíduos e comunidades, levando a discriminação e percepções distorcidas.
Ao compreender e abordar essas bias, podemos trabalhar em direção a criar plataformas de comunicação inclusivas e equitativas que respeitem a dignidade e diversidade de todos os usuários. Nesse sentido, nossa pesquisa buscou contribuir para esse objetivo investigando a bias de gênero na MT e desenvolvendo técnicas para mitigar seus efeitos. Os experimentos realizados nesse estudo demonstraram a eﬀicácia da ﬁne-tuning como técnica para reduzir a bias de gênero em modelos de tradução por máquina. Durante esse processo de ﬁne-tuning, o modelo adapta seu conhecimento existente ao novo conjunto de dados (neste caso, um corpus paralelo balanceado por gênero), ajustando seus parâmetros para produzir traduções menos biasadas, enquanto busca preservar a qualidade da tradução original. Uma das principais preocupações ao aplicar técnicas de redução de bias é o trade-oﬀ entre justiça e precisão. Nossos experimentos mostraram que a ﬁne-tuning pode reduzir a bias sem sacrificar significativamente a precisão da tradução. Comparações de scores BLEU antes e após a ﬁne-tuning indicam que, apesar de alguma perda, as traduções geradas permanecem de boa qualidade. Finalmente, é importante notar que o trabalho apresentado aqui representa apenas uma pequena contribuição para a equidade na Tradução por Máquina. É necessário expandir a pesquisa e explorar novas vias para melhorar a detecção de bias, compreender as implicações de vários tipos de bias e refinar técnicas de mitigação, garantindo que as traduções sejam justas e de alta qualidade. Agradecimentos Os autores agradecem à Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES) e ao Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq).