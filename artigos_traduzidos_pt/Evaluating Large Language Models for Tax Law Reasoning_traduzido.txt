Avaliando Modelos de Linguagem Grande para Razão em Direito Tributário

João Paulo Cavalcante Presa[0009−0004−4160−6495], Celso Gonçalves Camilo Junior[0000−0003−2553−8790], e Sávio Salvarino Teles de Oliveira[0009−0002−1203−5246] Universidade Federal de Goiás (UFG) joaopaulop@discente.ufg.br celsocamilo@ufg.br savioteles@ufg.br Resumo. A capacidade de razão sobre leis é essencial para profissionais jurídicos, facilitando a interpretação e a aplicação de princípios legais em situações do mundo real complexas. As leis tributárias são fundamentais para financiar funções governamentais e moldar o comportamento econômico, embora sua interpretação apresente desafios devido à sua complexidade, evolução constante e suscetibilidade a diferentes pontos de vista. Os Modelos de Linguagem Grande (LLMs) demonstram considerável potencial em apoiar esse processo de razão, processando extensos textos legais e gerando informações relevantes. Este estudo avalia o desempenho dos LLMs na razão legal dentro do domínio do direito tributário para entidades jurídicas, utilizando um conjunto de dados de perguntas do mundo real e respostas de especialistas em português brasileiro. Empregamos métricas quantitativas (BLEU, ROUGE) e avaliação qualitativa utilizando um modelo LLM sólido para garantir a precisão factual e relevância. Um conjunto de dados inédito foi curado, compreendendo perguntas genuínas de entidades jurídicas em direito tributário, respondidas por especialistas jurídicos com textos legais correspondentes. A avaliação inclui tanto modelos LLM de código aberto quanto proprietários, fornecendo uma avaliação da eficácia desses modelos em tarefas de razão legal. A forte correlação entre o métrico avaliador de modelo LLM robusto e o Bert Score F1 sugere que esses métricos capturam eficazmente aspectos semânticos pertinentes à qualidade percebida pelo ser humano. Palavras-chave: Razão Legal · Modelos de Linguagem Grande (LLMs) · Resposta a Perguntas Legais · Direito Tributário. 1 Introdução A capacidade de razão sobre leis é essencial para profissionais jurídicos, permitindo que eles interpretem e apliquem princípios legais em situações do mundo real complexas.
Questões legais frequentemente carecem de respostas diretas, requerendo análise minuciosa, pesquisa abrangente e síntese de múltiplos recursos para desenvolver argumentos ou soluções bem fundamentados. A lei tributária, em particular, é crucial porque influencia como os governos financiam serviços públicos e afeta a atividade econômica ao moldar decisões de investimento e gastos individuais. No entanto, a interpretação da lei tributária apresenta desafios significativos para o Processamento de Linguagem Natural (NLP) devido à complexidade e ambiguidade inerentes do linguagem jurídica, atualizações constantes, alterações e a necessidade de contextualizar regulamentações em jurisdições específicas. Modelos de Linguagem Grande (LLMs) demonstram potencial significativo em melhorar o processo de razão jurídica [23]. Esses modelos podem processar textos legais extensivos, incluindo estatutos, jurisprudência e opiniões jurídicas, para extrair informações relevantes e abordar as complexidades da lei tributária. Ao utilizar técnicas de geração avançadas, LLMs podem responder a perguntas jurídicas usando conjuntos de dados legais específicos, como casos de tribunal e precedentes jurídicos, fornecendo informações compreensivas e relevantes para profissionais jurídicos [29]. No entanto, há um vazio na compreensão de como LLMs raciocinam sobre textos legais, pois tarefas de questionamento e resposta existentes geralmente contêm respostas diretamente extraíveis dos textos fornecidos, enquanto a razão jurídica frequentemente requer compreensão mais profunda e aplicação de princípios jurídicos a cenários nuances [2]. Para abordar este vazio, desenvolvemos um conjunto de dados inédito compreendendo perguntas reais formuladas por entidades jurídicas no domínio da lei tributária, respondidas por especialistas jurídicos com textos legais de suporte (passagens de ouro). Esse conjunto de dados permite que avaliemos as habilidades de razão jurídica dos LLMs, focalizando sua capacidade de entender questões legais complexas, utilizar artigos de lei relevantes e gerar respostas precisas e coerentes.
A avaliação compara respostas geradas por LLM com respostas de especialistas utilizando métricas como ROUGE [19], BLEU [27] e similaridade semântica [40], ao lado de avaliações por um LLM forte [41], contribuindo para uma compreensão das capacidades de razão jurídica dos LLM. Esta pesquisa avalia tanto LLMs de código aberto quanto proprietários em cenários que requerem compreensão e aplicação integral da lei, distinta das abordagens extractivas utilizadas em conjuntos de dados como SQuAD [28] e TriviaQA [14]. Nossa base de dados requer que os LLM compreendam e apliquem a lei para gerar respostas apropriadas, frequentemente envolvendo vocabulário complexo e contextos não diretamente refletidos nos textos [2]. Este artigo apresenta dois contribuições significativas para o campo de NLP jurídico, particularmente no domínio desafiador da lei tributária. Primeiramente, é introduzido um conjunto de dados inédito composto por perguntas de lei tributária real-world, respostas elaboradas por especialistas e textos legais de suporte, ultrapassando tarefas de questionamento e resposta extractivas e requerendo que os modelos demonstrem habilidades de razão jurídica. Utilizando este conjunto de dados, avaliamos bem LLMs compreendem perguntas complexas de lei tributária e geram respostas precisas e bem fundamentadas, fornecendo uma compreensão melhor das capacidades e limitações atuais dos LLM em lidar com tarefas de razão jurídica.
1 Aplicação de Lei Tributária em LLMs Avaliando Q&A LLMs com Geração Ampliada de Recuperação O conjunto de dados LLeQA [21] inclui 1.868 perguntas jurídicas anotadas por especialistas, contendo respostas e referências legais. Este trabalho aplica a técnica de Geração Ampliada de Recuperação (RAG), recuperando artigos estatutários de um corpus extenso de legislação belga. A eficácia do modelo é avaliada utilizando o métrica ME-TEOR, demonstrando a viabilidade de integrar a recuperação de informações com LLMs para melhorar a precisão das respostas jurídicas. ChatLaw [5] aborda a criação de um modelo de linguagem de grande escala para o domínio jurídico, especificamente no contexto chinês. Este trabalho combina métodos de recuperação de banco de dados de vetores com recuperação baseada em palavras-chave para aumentar a precisão das respostas. Integrando essas técnicas, o modelo é capaz de fornecer respostas mais precisas e contextualmente relevantes. Avaliando a Razão Jurídica de Q&A em LLMs LAiW [6] propõe um benchmark para avaliar as capacidades de LLMs no contexto jurídico chinês. O objetivo deste trabalho é testar bem como os modelos podem lidar com tarefas jurídicas específicas. Os resultados mostram que alguns LLMs específicos jurídicos desempenham melhor do que seus counterparts gerais, embora reste um grande gap em relação ao GPT-4 [26]. LawBench [8] oferece uma avaliação abrangente das capacidades de LLMs em tarefas jurídicas, incluindo Q&A. Este trabalho testou extensivamente 51 LLMs populares, incluindo 20 multilíngues, 22 focados no chinês e 9 específicos para a lei. A conclusão é que, embora o fine-tuning de LLMs em textos jurídicos específicos traga melhorias, os modelos ainda precisam ser usáveis e confiáveis para tarefas jurídicas complexas. Fine-tuning e Avaliação de Q&A Modelos de Linguagem Grande FedJudge [37] utiliza Aprendizado Federado (FL) para superar desafios de privacidade de dados. Este framework otimiza LLMs jurídicas federadas, permitindo que os modelos sejam treinados localmente em clientes, com seus parâmetros agregados e distribuídos em um servidor central.
O modelo FedJudge é avaliado em tarefas de perguntas e respostas utilizando métricas como ROUGE, BLEU e BertScore para comparar a qualidade das respostas geradas. Este trabalho demonstra que o modelo fornece respostas mais precisas e relevantes em diferentes contextos jurídicos. O DISC-LawLLM [38] emprega modelos de linguagem treinados em conjuntos de dados supervisionados no domínio jurídico e incorpora um módulo de recuperação para acessar e utilizar conhecimento jurídico externo. Este sistema avalia perspectivas objetivas e subjetivas utilizando o DISC-Law-Eval, um benchmark que inclui perguntas de resposta jurídica. Além disso, a avaliação subjetiva é realizada usando o modelo GPT-3.5 como juiz.

3 Metodologia

Esta seção destaca a metodologia utilizada no nosso estudo, com um enfoque particular no processo de seleção do modelo. Também detalhamos o processo de coleta de dados, a criação de um corpus relevante e os setups experimentais dos modelos selecionados, incluindo as prompts e parâmetros específicos utilizados. Além disso, detalhamos a abordagem de avaliação, discutindo tanto as métricas empregadas quanto a estratégia de avaliação subjetiva.

3.1 Coleta de Dados

Nosso conjunto de dados consiste em uma série de perguntas sobre direito tributário relacionadas a entidades jurídicas. As perguntas foram selecionadas de uma coleção anualmente atualizada pela Coordenação-Geral da Tributação (Cosit) [9] do Serviço de Receita Federal do Brasil. O conjunto de dados inclui mais de mil pares de perguntas-respostas, com a maioria das respostas apoiadas por uma base normativa ou jurídica relevante. A granularidade das referências nas respostas é o mais detalhada possível, citando especificamente os artigos da lei ou outras regulamentações utilizadas para formular as respostas. As perguntas representam dúvidas reais de contribuintes, e especialistas no campo tributário brasileiro elaboram as respostas. A seguir, discutiremos como o conjunto de dados foi criado. Seleção de Perguntas

Extraímos uma subamostra do conjunto de perguntas e respostas fornecido pela Cosit.
Nessa seleção, nos concentramos em questões que incluíam respostas com referências legais em vez da regulamentação integral. Embora a maioria das respostas tenha incluído referências legais, elas foram frequentemente elaboradas por especialistas de maneira que extrapolassem o escopo da pergunta ou incluissem detalhes excessivos, como tabelas e exemplos numerosos. Essa complexidade as tornou inadequadas para uso em contextos como Geração com Recuperação de Informação (RAG). Excluímos essas respostas detalhadas demais para garantir uma avaliação justa com os LLMs. Os resultados iniciais desse processo de seleção estão representados nas primeiras três colunas da Tabela 1. Coleta de Regulamentações (Passagens de Ouro) Após selecionar as questões e suas correspondentes referências legais, leis e artigos, reunimos cada documento regulamentar referenciado pelas especialistas em suas respostas às questões formuladas por entidades legais. Embora essa tarefa tenha sido intensiva em tempo, foi essencial para avaliar as capacidades de raciocínio dos LLMs em relação a textos legais. Ao completar essa etapa, o conjunto de dados compreendeu a pergunta, a resposta, a referência à regulamentação e a regulamentação em si (passagens de ouro). A Tabela 1 apresenta o conjunto de dados final. Corpus Legislativo Nessa etapa, coletamos mais de 30 documentos, que incluíram leis, instruções, decretos e opiniões. Cada documento contém até milhares de artigos compostos por múltiplas disposições. Esses documentos representam...
O crédito relacionado à administração tributária... Dependem da isenção do IRPJ de reconhecimento prévio? Não. O benefício da isenção do IRPJ não depende... RIR/2018, art. 192. Art. 192. As isenções referidas nesta Seção... Em quais circunstâncias um indivíduo é considerado equivalente a uma pessoa jurídica? Para fins de imposto de renda, os indivíduos são considerados... RIR/2018, art. 162, § 1, incisos I a III. Art. 162. As empresas individuais são consideradas... São os condôminos na propriedade imobiliária sujeitos ao imposto de renda? Condomínios na propriedade imobiliária não estão sujeitos... RIR/2018, art. 167. Art. 167. Os condomínios na propriedade imobiliária devem... uma fração da legislação tributária brasileira e incluem as normas que subjazem às respostas dos especialistas no conjunto de dados. É importante notar que essas normas estão constantemente sendo alteradas, e muitas disposições foram revogadas. Todas as disposições revogadas foram excluídas até a data de criação do conjunto de dados para garantir um corpus de alta qualidade. Além disso, qualquer pergunta que tivesse sua base regulamentar revogada foi eliminada durante a fase de seleção de perguntas. A figura 1 mostra os documentos do corpus. 3.2 Configuração Experimental Neste estudo, realizamos uma avaliação abrangente de modelos de linguagem de grande escala (LLMs) em termos de sua capacidade de raciocínio sobre leis, especialmente se concentrando na tributação corporativa para pessoas jurídicas. Avaliamos os LLMs usando os conjuntos de dados criados neste artigo, que foram criados a partir de perguntas e respostas sobre a tributação de pessoas jurídicas, fornecidas por especialistas em matéria. Selecionamos mais de 20 LLMs para avaliação, abrangendo modelos proprietários e de código aberto. Os modelos escolhidos incluem exemplos notáveis como Mistral AI, Llama, Gemma, Qwen, várias versões fine-tuned pela comunidade desses modelos e um modelo proprietário.
Cada modelo apresenta características e capacidades únicas, fornecendo uma ampla gama de perspectivas para nossa avaliação. 6 J. Presa et al. ADI SRF nº 5, de 2001 ADN Cosit nº 4, de 1996 Decreto-Lei nº 1.381, de 1974 Decreto-Lei nº 1.510, de 1976 Decreto-Lei nº 1.598, de 1977 IN DPRF 21, de 1992 IN RFB nº 1.252, de 2012 IN RFB nº 1.520, de 2014 IN RFB nº 1.700, de 2017 IN RFB nº 2.004, de 2021 IN RFB nº 2.055, de 2021 IN SRF nº 213, de 2002 IN SRF nº 51, de 1978 IN nº 122, de 1989 Lei nº 6.404, de 1976 Lei nº 6.766, de 1979 Lei nº 9.249, de 1995 Lei nº 9.316, de 1996 Lei nº 9.430, de 1996 Lei nº 9.532, de 1997 Lei nº 9.718, de 1998 Lei nº 11.051, de 2004 PN CST nº 1, de 1983 PN CST nº 2, de 1983 PN CST nº 4, de 1981 PN CST nº 58, de 1977 PN CST nº 72, de 1975 PN CST nº 146, de 1975 Portaria MF nº 356, de 1988 RIR 2018 Fig. 1: Documentos do corpus legislativo Para manter a consistência em nossas avaliações, padronizamos o parâmetro de temperatura em 0,1 para todos os modelos escolhidos. Esse ajuste de temperatura baixa foi escolhido para reduzir a aleatoriedade no output, incentivando respostas mais determinísticas. Além disso, não impostimos um limite máximo de tokens, permitindo que os modelos gerem respostas sem nenhuma restrição em sua extensão. Um prompt específico (ver Prompt Question Answer no Apêndice A) foi elaborado para guiar os modelos a razoar sobre a lei e gerar respostas apropriadas. O prompt instrui explicitamente os modelos a razoar sobre o contexto jurídico fornecido e formular uma resposta. Se um modelo não for capaz de gerar uma resposta satisfatória, é instruído a declarar que não sabe a resposta. A informação jurídica necessária para responder a cada pergunta, como um artigo de uma lei ou um documento jurídico, está incluída no prompt. Essa informação é a mesma utilizada pelos especialistas para criar as respostas de referência, garantindo uma base justa para comparação.
Ao utilizar prompts padronizados e incorporar provisões legais relevantes, garante-se que os modelos tenham acesso à mesma informação que os especialistas humanos. Isso permite uma avaliação minuciosa de suas capacidades de razão. É importante notar que as perguntas e as respostas de referência são apresentadas em português brasileiro. Este aspecto do estudo testa as habilidades de razão dos modelos e avalia sua competência em gerar respostas precisas e apropriadas ao contexto na língua portuguesa. Dado que muitos LLMs são treinados primariamente em conjuntos de dados de língua inglesa, avaliar seu desempenho em textos legais em português brasileiro para entender a aplicabilidade e limitações desses modelos em jurisdições que não são de língua inglesa. Embora o conjunto de dados utilizado nesse experimento contenha um corpus adequado para Geração-Augmentada de Recuperação (RAG), nossa avaliação se concentrou exclusivamente nas tarefas de geração e razão. Esta decisão foi inspirada por outros conjuntos de dados prominentes, como SQuAD 2.0 e HotpotQA, que também fornecem as passagens esperadas ao lado das respostas de verdade, permitindo uma avaliação direta das habilidades de geração do modelo sem a etapa de recuperação. Ao se concentrar nestes aspectos, objetivamos isolá-los e avaliar minuciosamente a habilidade dos LLMs de gerar respostas precisas e razoáveis com base apenas no contexto legal fornecido.

3.3 Métricas de Avaliação
Nossa estudo avaliou modelos de linguagem grande (LLMs) utilizando uma abordagem integral que integra métodos quantitativos e qualitativos. Para a avaliação quantitativa, utilizamos os métricas BLEU (Bilingual Evaluation Understudy) e ROUGE (Recall-Oriented Understudy for Gisting Evaluation) [19,27]. No campo do processamento de linguagem natural, essas métricas são cruciais para avaliar a qualidade da geração de texto ao comparar as respostas dos modelos com um conjunto de respostas de referência pré-definido.
Especificamente, no domínio de perguntas e respostas relacionadas à tributação empresarial, esses métodos fornecem uma medida quantitativa de como as respostas geradas se alinham com as respostas ideais. Apesar de sua ampla utilização, métodos como BLEU, ROUGE [37] e METEOR [21], amplamente utilizados para avaliar modelos de linguagem, primariamente fornecem uma perspectiva quantitativa e podem não capturar plenamente a precisão das respostas em cenários de perguntas e respostas [20]. Essa limitação surge porque esses métodos não avaliam adequadamente a precisão factual ou a relevância das respostas geradas, o que é crítico para determinar se as perguntas foram respondidas corretamente. Para abordar essa lacuna, adotamos uma abordagem qualitativa mais refinada, utilizando as capacidades de um modelo de linguagem poderoso como um substituto para o julgamento humano. Especificamente, empregamos o GPT-4 para avaliar o desempenho de outros modelos. Essa abordagem se baseia na noção de que um modelo de linguagem robusto, como o GPT-4, pode emular eficazmente o julgamento humano ao avaliar respostas [7,10,17,20,31,35,41] a perguntas abertas, fornecendo assim uma aproximação mais próxima dos critérios de avaliação humanos. Para a avaliação qualitativa, utilizamos um prompt cuidadosamente projetado para avaliar a precisão factual das respostas dos modelos. A precisão de cada modelo foi então calculada com base nessa avaliação. O prompt específico utilizado para essa avaliação pode ser encontrado na Seção de Avaliação do Prompt no Apêndice A para mais detalhes.
Tabela 2: Métricas de Desempenho do Modelo

Modelo ROUGE-L BLEU Bert Score F1 Acc. GPT-4 Mistral-7B-Instruct-v0,2 0,35 0,20 0,67 0,54 Mistral-7B-Instruct-v0,3 0,40 0,26 0,71 0,55 Mixtral-8x7B-Instruct-v0,1 0,38 0,24 0,70 0,53 Mixtral-8x22B-Instruct-v0,1 0,44 0,30 0,73 0,59 Llama-2-70b-chat-hf 0,38 0,19 0,69 0,49 Llama-2-13b-chat-hf 0,37 0,20 0,68 0,43 Llama-2-7b-chat-hf 0,32 0,14 0,65 0,34 Llama-3-70b-chat-hf 0,34 0,16 0,65 0,60 Llama-3-8b-chat-hf 0,35 0,15 0,65 0,54 Qwen1,5-110B-Chat 0,39 0,21 0,71 0,60 Qwen1,5-72B-Chat 0,41 0,24 0,71 0,62 Qwen1,5-14B-Chat 0,34 0,16 0,68 0,48 Qwen2-72B-Instruct 0,43 0,29 0,73 0,64 Gema-7b-it 0,40 0,22 0,70 0,45 Yi-34B-Chat 0,38 0,26 0,70 0,52 GPT-3,5-turbo 0,38 0,15 0,69 0,56 Platypus2-70B-instruct 0,41 0,29 0,70 0,57 Vicuna-13b-v1,5 0,41 0,27 0,71 0,50 Vicuna-7b-v1,5 0,37 0,23 0,69 0,39 Openchat-3,5-1210 0,42 0,28 0,72 0,55 WizardLM-13B-V1,2 0,36 0,25 0,68 0,49 SOLAR-10,7B-Instruct-v1,0 0,36 0,23 0,70 0,51 OpenHermes-2p5-Mistral-7B 0,41 0,25 0,71 0,55

4.1 Análise do Desempenho do Modelo

As versões mais recentes das famílias Llama, Qwen e Mistral apresentam avanços significativos em relação às suas predecessoras. Esses modelos incorporam várias melhorias arquiteturais, incluindo a ativação SwiGLU [30] e a atenção de consulta agrupada (GQA) [4]. Os modelos Qwen2-72B-Instruct [1] e Llama-3-70b-chat-hf [3] se beneficiaram dessas melhorias, especialmente as modificações no tokenizador e a inclusão da GQA, o que levou a ganhos de desempenho notáveis. Como resultado, o modelo Qwen2-72B-Instruct [1] alcançou a maior precisão. Resultados semelhantes foram observados em outras avaliações de benchmark de LLM [1], destacando o desempenho superior dos modelos que incorporam essas técnicas. A análise do desempenho dos modelos revelou que o tamanho do modelo tem um impacto significativo nos resultados, embora esse impacto não seja sempre direto. Modelos maiores, como Qwen2-72B-Instruct [1] e Mixtral-8x22B-Instruct-v0,1, apresentaram melhor desempenho.
1 [13], alcançou desempenho superior, exibindo os maiores índices de ROUGE-L, BLEU, Bert Score F1 e precisão avaliada em métricas GPT-4. No entanto, observamos que modelos menores, como Mistral-7B-Instruct-v0.3 [12] e OpenHermes-2p5-Mistral-7B [32], superaram alguns modelos maiores em métricas específicas. Por exemplo, Mistral-7B-Instruct-v0.3 atingiu um Bert Score F1 de 0,71, superando vários modelos maiores. OpenHermes-2p5-Mistral-7B demonstrou desempenho notável com precisão comparável a modelos significativamente maiores. Esses achados sugerem que, embora modelos maiores geralmente apresentem resultados melhores devido à sua capacidade de capturar informações mais complexas, modelos menores bem treinados e fine-tunados podem oferecer desempenho competitivo em contextos específicos. Esta tendência indica que a qualidade do treinamento e a adequação do modelo ao conjunto de dados específico são fatores críticos que podem mitigar a disparidade de tamanho entre modelos. Embora o volume de dados portugues utilizado no treinamento desses modelos ainda não tenha sido verificado, as melhorias arquitetônicas e de treinamento sugerem o desempenho aprimorado dos LLMs em tarefas de Q&A em direito tributário corporativo. No família Mistral, o modelo Mixtral-8x22B-Instruct-v0.1 [24] destacou-se com os maiores índices em ROUGE-L, BLEU e Bert Score F1, indicando o potencial da arquitetura de mistura de especialistas [13] para textos legais em português. A análise de modelos open-source fine-tunados revela melhorias significativas em relação aos modelos base. Os modelos openchat-3.5-1210 [34] e OpenHermes-2p5-Mistral-7B [32], ambos derivados do Mistral-7B-v0.1 [12], mostraram aumentos notáveis em precisão após fine-tuning. Da mesma forma, os modelos vicuna-13b-v1.5 e vicuna-7b-v1.5 [41], fine-tunados a partir do Llama 2 [33], também demonstraram avanços na precisão de resposta. Além disso, modelos como WizardLM-13B-V1.2 [36], SOLAR-10.7B-Instruct-v1.
0 [15,16], e Platypus2-70B-instruíram [18], derivados de Llama 2, melhoraram os resultados de seus modelos base. Notadamente, esses processos de fine-tuning foram conduzidos em conjuntos de dados diversificados, não no conjunto de dados experimental em si, e ainda levaram a melhorias nos métricos no conjunto de dados experimental. Essas melhorias sugerem que o fine-tuning pode ser eficaz para melhorar as capacidades de tarefas de Q&A e geração de texto jurídico quando aplicado a conjuntos de dados específicos.

4.2 Análise de Métricas de Avaliação

As métricas tradicionais como BLEU e ROUGE podem não capturar plenamente as nuances necessárias para uma resposta precisa em tarefas de Q&A. O métrica de Pontuação Bert F1 é amplamente reconhecida por sua alinhamento com a avaliação humana devido à sua capacidade de capturar semelhanças semânticas profundas entre textos [40], superando as capacidades de matching lexical de métricas tradicionais como ROUGE-L e BLEU [40]. Embora este estudo não tenha como objetivo provar que o LLM é um juiz para avaliação alinhado com a avaliação humana, recentes estudos têm explorado essa alinhamento [20,31,35,41]. Nossa pesquisa avalia a qualidade das respostas geradas por LLMs em tarefas de Q&A do domínio jurídico. A forte correlação entre a Avaliação de Precisão do LLM (GPT-4) e a Pontuação Bert F1, como evidenciado pelas correlações de Pearson (0,657) e Kendall (0,491) (ver Tabela 3), sugere que ambos os métricas capturam aspectos semânticos relevantes para a qualidade percebida humanamente. Os resultados estão em linha com estudos [40] que recomendam usar correlações de Pearson e Kendall para avaliar a qualidade de métricas. Além disso, a análise de Bland-Altman, que é particularmente adequada para comparar métodos de medição [11], confirma que a Pontuação Bert F1 e a Avaliação de Precisão do LLM (GPT-4) capturam aspectos semânticos relevantes.
670 (GPT-4) Accuracy Evaluation são mais concordantes, como demonstrado pela menor variação na dispersão de pontos e pela largura mais estreita dos limites de acordo (ver Figura 2). Em contraste, ROUGE-L e BLEU demonstraram diferenças médias maiores e limites de acordo mais amplos. O Bert Score F1 apresentou uma diferença média próxima de zero e limites de acordo mais estreitos, indicando melhor concordância com as medições de Accuracy Evaluation do LLM. Fig. 2: Plots de Bland-Altman para Métricas vs Accuracy Evaluation do LLM (GPT-4) Avaliando Modelos de Linguagem Grande para Razão de Direito Tributário 11 Essas descobertas sugerem que a Accuracy Evaluation do LLM (GPT-4), similar ao Bert Score F1, poderia ser uma métrica valiosa e representativa para avaliar o desempenho real de modelos de linguagem. Embora ROUGE-L e BLEU mostrem correlações mais altas com o Bert Score F1, a correlação mais forte e concordância da Accuracy Evaluation do LLM com o Bert Score F1 indicam seu potencial de alinhamento com a avaliação humana. Isso apoia o desenvolvimento de métricas de avaliação que refletam mais precisamente a qualidade percebida pelos humanos, alinhando-se com a direção da pesquisa atual que investiga o potencial dos LLMs para se alinhar com a avaliação humana [20,31,35,41].

5 Conclusão Este estudo destaca a importância do direito tributário na sociedade e o potencial dos modelos de linguagem para ajudar na compreensão e aplicação dele. Desenvolvemos um conjunto de dados inédito de perguntas e respostas de direito tributário do mundo real em português brasileiro e conduzimos uma avaliação rigorosa de vários modelos de linguagem. Embora nossos achados sugiram que esses modelos mostram promessa em compreender e razoar sobre textos legais complexos, é necessário mais pesquisa para demonstrar plenamente sua eficácia em razão legal em um espectro mais amplo de cenários e tarefas.
A nossa avaliação mostrou que avanços na arquitetura de modelo têm um impacto notável no desempenho, e fine-tuning de modelos open-source, mesmo quando feito em conjuntos de dados diversificados, em vez de específicos para o domínio jurídico, ainda podem melhorar sua capacidade de gerar respostas relevantes e precisas. Isso sugere que melhorias contínuas e adaptações são valiosas para melhorar as capacidades de modelos de linguagem em tarefas jurídicas. Para avaliar o desempenho do modelo, utilizamos o Bert Score F1, conhecido por sua forte correlação com avaliações humanas em tarefas que envolvem compreensão descritiva e estrutural, e um novo métrico, LLM Accuracy Evaluation. Embora o Bert Score F1 já seja estabelecido como uma medida eficaz alinhada com a avaliação humana, especialmente em tarefas descritivas, nossos resultados mostraram que a LLM Accuracy Evaluation também demonstrou uma forte correlação com o Bert Score F1 através de correlações de Pearson e Kendall. A análise de Bland-Altman confirmou ainda mais que o métrico LLM se alinha estreitamente com o Bert Score, sugerindo seu potencial como uma alternativa confiável em avaliações. No entanto, é importante notar que, embora esses achados sejam encorajadores, o uso desses métricos em tarefas baseadas em razão, como essa estudo, ainda requer mais validação. O métrico LLM é uma ferramenta promissora, mas mais pesquisa é necessária para estabelecer sua eficácia plena, especialmente para capturar as nuances da razão jurídica. Limitações e Trabalho Futuro Uma limitação do nosso estudo é que, enquanto ele se concentrou em avaliar as capacidades de geração e razão de LLMs, não exigiu que os modelos identificassem provisões jurídicas específicas como parte de suas respostas. Nossa base de dados inclui um corpus abrangente que contém as leis necessárias, permitindo a aplicação de técnicas de Geração-Augmentada de Recuperação (RAG).
Isso permite que os modelos recuperem disposições legais relevantes e as incorporem em suas respostas, o que é essencial para um processo de razão estatutária mais completo. Ao fornecer os artigos legais relevantes, que não refletem diretamente as respostas às perguntas, nosso estudo avaliou uma parte da razão estatutária ao testar a capacidade dos modelos de aplicar a lei para gerar respostas precisas. O trabalho futuro poderia explorar a integração do RAG, visando melhorar a capacidade dos modelos não apenas de gerar respostas corretas, mas também de identificar e citar as disposições legais apropriadas, alcançando assim uma razão estatutária mais robusta e abrangente. Disponibilidade do Conjunto de Dados e Código O conjunto de dados utilizado nesse estudo, bem como o código para reproduzir os experimentos e análises, estão disponíveis ao público. O conjunto de dados pode ser acessado pelo link: https://github.com/joaopaulopresa/dataset. O código pode ser encontrado aqui: https://github.com/joaopaulopresa/code.