Aprimorando Parâmetros da Arquitetura CleanUNet para Melhorar a Desenvolvimento de Fala

Matheus Vieira da Silva1[0000-0002-1673-8187], João Fernando Mari2[0000-0001-8271-3802] e André Ricardo Backes3[0000-0002-7486-4253]

1 Escola de Ciência da Computação, Universidade Federal de Uberlândia, Uberlândia, Brasil - matheus.silva007@ufu.br
2 Instituto de Ciências Exatas e Tecnológicas, Universidade Federal de Viçosa, Rio Paranaíba, Brasil - joaof.mari@ufv.br
3 Departamento de Computação, Universidade Federal de São Carlos, São Carlos, Brasil - arbackes@yahoo.com.br

Resumo. A melhoria de fala refere-se a um conjunto de técnicas que visam recuperar fala limpa a partir de um sinal corrompido. Uma forma de corromper um sinal é adicionando ruído. O ruído surge de várias maneiras. Condições acústicas subótimas podem causar ruído de fundo e eco, comprometendo a clareza da fala e tornando técnicas de desenho necessário para melhorar a fala. Neste trabalho, otimizamos a arquitetura CleanUNet, uma rede neural convolucional (CNN) proposta especificamente para tarefas de desenho de fala causal. Exploramos alternativas para o bottleneck do transformador, como a arquitetura Mamba, capaz de lidar com saídas do encoder de forma mais eficiente com complexidade linear, também reduzimos o número de canais ocultos nas camadas convolucionais. Isso diminui o número de parâmetros do modelo e melhora a velocidade de treinamento e inferência em um GPU único, oferecendo uma abordagem mais otimizada para desempenho melhorado. Até onde sabemos, essa é a primeira tentativa de incorporar a Mamba como substituto para o transformador padrão na arquitetura CleanUnet.

Palavras-chave: CleanUNet · Processamento de áudio · Arquitetura Mamba · Rede neural convolucional.

1 Introdução A pandemia de COVID-19 e os lockdowns levaram a significativos mudanças em como as pessoas trabalham e interagem com a tecnologia, exemplificadas pelo crescimento exponencial do Zoom de 10 milhões de participantes diários em dezembro de 2019 para mais de 300 milhões em abril de 2020 [8].
A pandemia levou a um súbito shift para o trabalho remoto e reuniões virtuais, revelando um problema significativo: os pobre acústicos em casa negativamente afetam a videoconferência. Condições acústicas subótimas causaram mais ruído de fundo e eco, prejudicando a clareza da fala e aumentando o esforço necessário para chamadas de vídeo. Ambientes acústicos inadequados em casa também prejudicaram a qualidade do áudio e a produtividade do trabalho. É importante notar que o problema de acústicas insuficientes não se limita às salas de casa durante a videoconferência. Desafios de ruído de fundo interrompendo a comunicação têm sido uma preocupação desde o início da revolução digital [20]. Técnicas avançadas como Subtração Espectral [2] e Filtros de Wiener [15] foram desenvolvidas para abordar os desafios na melhoria da fala. Esses métodos utilizam algoritmos e filtros avançados para reduzir ruído de fundo, amplificar vozes de falantes e melhorar a qualidade do áudio. O principal desvantagem dessas técnicas é que elas supõem ruído estacionário, ou seja, que as características do ruído de fundo sejam esperadas para permanecer relativamente constantes durante toda a gravação de áudio. Em cenários reais, como reuniões do Zoom, o ruído pode frequentemente ser não estacionário, variando em intensidade e conteúdo espectral ao longo do tempo. Nos últimos anos, graças a avanços em aprendizado profundo, a remoção efiiente de ruído não estacionário da fala foi significativamente melhorada [26]. Existem dois métodos primários para melhoria da fala: o domínio tempo-frequência e o domínio de onda. Algumas estudos de cruzamento exploram abordagens híbridas que combinam ambos os métodos [21, 25]. Avanços recentes em aprendizado profundo, como redes convolucionais [10], redes gerais adversárias [5], U-Net para segmentação [19] e arquitetura de transformador [24], facilitaram a obtenção de desempenho de ponta ao estrategicamente aproveitar os méritos e limitações inerentes a cada paradigma arquitetural.
Uma arquitetura que atinge resultados de ponta é conhecida como CleanUNet [9]. Este modelo se inspira em [17] e combina transformadores e U-Net para melhorar a fala ruim dentro do domínio de onda. No entanto, a arquitetura CleanUNet apresenta um desafio formidável: o tamanho do modelo e sua vulnerabilidade à complexidade quadrada no gargalo do transformador. Esses desafios levam a três implicações principais: 1) Recursos Computacionais Confinados: os processos de treinamento e inferência necessitam de recursos computacionais substanciais devido à intensidade computacional do modelo e sua dependência de amostras de áudio que variam com a taxa de amostragem; 2) Preocupações Ambientais: a primeira implicação levanta preocupações ambientais, incluindo questões como pegada de água e carbono, que são exacerbadas pelas demandas computacionais extensas quando operando a escala. 3) Desempenho em Tempo Real: a adaptação do modelo a hardware limitado por recursos é inadequada para aplicações de denoising em tempo real. Neste trabalho, abordamos os três principais desafios do CleanUNet otimizando sua arquitetura. Nossas contribuições incluem a introdução de alternativas ao gargalo do transformador para reduzir o número de parâmetros do modelo para eficiência e melhorar a velocidade de inferência para facilitar aplicações em tempo real. Nossa revisão do modelo melhora a eficiência substituindo o gargalo do transformador por alternativas que manipulam saídas do encoder com complexidade linear O(n). Além disso, otimizamos empiricamente o número de canais ocultos nas camadas convolucionais. Essas modificações reduzem o número de parâmetros do modelo e melhoram significativamente tanto a velocidade de treinamento quanto a velocidade de inferência em um único GPU. O restante do presente artigo está organizado da seguinte maneira: Seção 2 apresenta o trabalho relacionado. Seção 3 descreve os materiais e métodos utilizados em nosso trabalho.
Aqui está a tradução do texto científico para português do Brasil:

Optimizar CleanUNet para Denoising de Fala 3 A Seção 4 apresenta os experimentos e resultados, enquanto a Seção 5 conclui o artigo. 2 Trabalhos Relacionados A subtração espectral é um método simples e efeito para reduzir ruído de fundo [23]. Esse método reduz imediatamente a amplitude dos componentes de ruído enquanto preserva sinais de fala, estimando o espectro de ruído médio durante períodos silenciosos e subtraindo-o do espectro do sinal de entrada. No entanto, a aleatoriedade do ruído pode resultar em valores negativos e distorções. Com base em estimativas estatísticas do sinal subjacente e do ruído, o filtro de Wiener cria dinamicamente um filtro linear capaz de suprimir ruído existente [18]. O sinal e o ruído são modelados como processos estocásticos com propriedades espectrais bem conhecidas. Seu design autoajustável e baseado em estatística tornam o filtro de Wiener muito efeito para remover ruído de fala real, especialmente em ambientes de escritório residencial. As características estatísticas de ruídos estacionários são estáveis e não mudam significativamente com o tempo. Exemplos incluem o som constante de um motor, conversas de fundo contínuas e ruído de fundo consistente. Ruído de fundo constante pode ser aproximado atualizando períodos silenciosos, técnicas como subtração espectral funcionam melhor. Ruídos não estacionários que mudam rapidamente são difíceis de estimar e suprimir. Ruídos não estacionários têm estatísticas variáveis no tempo, o que significa que suas características mudam rapidamente. Exemplos incluem sons abruptos como quebrar vidro, ruídos de construção esporádicos e sinos repentinos. Um dos métodos de aprendizado profundo mais antigos utilizados para denoising de fala foi as Redes Neurais Recorrentes (RNNs) [14]. As RNNs processam sequências de entrada incrementalmente, preservando um estado interno para capturar contexto temporal, tornando-as ideais para processamento de fala. O treinamento de RNNs mapeia gradualmente inputs de fala ruidosa para saídas limpas alvo e visa reduzir ruído de áudio.
No entanto, RNNs têm limitações, como inferência desafiadora e alto custo computacional. Como alternativa, Redes Neurais Convolucionais (CNNs) [12] tornaram-se populares. CNNs de 1D utilizam padrões locais e trabalham diretamente com áudio bruto para reduzir ruído. CNNs de 2D operam utilizando representações de espectrograma. Em ambos os casos, a rede captura características de fala local usando pequenos filtros convolucionais para separá-las do ruído de fundo. Agrupamento e downsampling agregam informações ao longo do tempo. Múltiplos camadas convolucionais aprendem características de nível superior, reconhecendo componentes de fala mesmo em condições ruidosas. Um dos principais vantagens das CNNs é a compartilhamento de pesos, reduzindo significativamente os parâmetros em comparação com RNNs, o que resulta em treinamento e inferência mais efcientes. Enquanto RNNs aproveitam o contexto temporal, CNNs empregam lentes localizadas para isolarem efcientemente a fala do ruído no domínio tempo-frequência para reduzir ruído de forma efciente. Pesquisas em andamento melhoram o desempenho de denoising de fala com base em CNNs [26]. Redes Geradoras Adversárias (GANs) [5] empregam um gerador para melhorar a fala ruidosa e um discriminador que distingue entre fala melhorada e fala limpa. Treinamento adversário melhora a qualidade do output. CNNs são comumente usadas nos modelos de gerador e discriminador em arquiteturas de GAN. Recentemente, avanços em aprendizado profundo incorporaram soluções híbridas envolvendo arquiteturas de U-Net [19] e Transformer [24]. Os Transformers podem capturar contextos de fala a longo prazo e interações precisas entre entradas ruidosas para denoising. Camadas de atenção priorizam componentes importantes de fala sobre ruído de fundo. Embora o uso de Transformers para processamento de fala ainda esteja sendo investigado, os resultados iniciais são promissores [11].
4 horas de pares de fala limpa e ruidosa, projetados para treinamento e testes de modelos de melhoria de fala que operam em 48 kHz. O conjunto de treinamento consiste em 34.647 pares de amostras de áudio, enquanto o conjunto de testes compreende 824 pares. Devido às variações nos tamanhos das amostras de áudio dentro de cada par, um passo de pré-processamento foi realizado para normalizar seus tamanhos. Isso envolveu concatenar todas as amostras e subsequentemente recortar uma nova amostra para uma comprimento especificado, denominado L. Se a última amostra for menor que L, será paddingada com o valor válido mais recente.
Aqui está a tradução do texto científico para português do Brasil:

Otimizar o CleanUNet para Denoising de Fala 5 PESQ Este métrica se concentra na qualidade percebida da fala e envolve um algoritmo psicoacústico para simular a audição humana e comparar a fala limpa (original) e a fala aprimorada (saída do modelo) para prever qualidade percebida. A cálculo inclui etapas como alinhamento de níveis, filtragem e um processo de comparação detalhado que considera várias distorções e artefatos introduzidos pelo processamento. A saída do score varia de -0,5 a 4,5, onde os scores mais altos indicam melhor qualidade. STOI Projetado para prever a inteligibilidade de sinais de fala, o método envolve calcular a correlação temporal a curto prazo entre as fala limpa (original) e a fala aprimorada (saída do modelo) sinais em várias faixas de frequência. O processo inclui segmentação dos sinais em frames curtos, aplicação de uma transformada de Fourier para analisar o conteúdo de frequência e, em seguida, computar a correlação entre os frames correspondentes das fala limpa e aprimorada. O score STOI varia de 0 a 1, onde 0 indica pouca inteligibilidade e 1 indica excelente inteligibilidade. 3.4 Arquitetura de Transformador A arquitetura de transformador foi introduzida pela primeira vez em [24] e utiliza mecanismos de atenção auto- [1] para capturar as dependências entre diferentes palavras ou tokens em uma sequência. Isso permite que o modelo avalie a importância de cada token na sequência em relação a todos os outros tokens. O componente chave da atenção auto é o cálculo de scores de atenção com base em consultas (Q), chaves (K) e valores (V). A representação matemática da atenção auto é descrita abaixo: Atenção(Q, K, V) = softmax(QKT / √dk) V (1) onde Q, K, V representam as matrizes de consultas, chaves e valores derivadas da matriz de entrada. √dk é um fator de escala, onde dk tem a dimensionalidade de K e V. A função softmax normaliza os scores de atenção e o resultado é usado para calcular a soma ponderada dos vetores V.
Inspireada em CNNs, a arquitetura emprega uma técnica de atenção chamada multi-headed attention para processar sequências de entrada simultaneamente. A expressão a seguir ilustra o processo: MultiHead(Q, K, V ) = Concat(head1, head2, ..., headh)WO (2) headi = Atenção(QW Qi , KW Ki , VW Vi ) (3) onde headi é a pontuação de atenção e W Qi , W Ki , W Vi e WOi são matrizes de parâmetros. Durante o treinamento, a complexidade computacional de calcular as pontuações de atenção para cada par de tokens resulta em um crescimento quadrático de O(n2) com uma sequência de entrada de comprimento n. Pesquisadores propuseram otimizações e aproximações para abordar esse desafio e reduzir a sobrecarga computacional [7, 16].

A arquitetura Longnet, introduzida por [4], é um novo variante do Transformer que atinge complexidade computacional linear e dependência de token logarítmica, otimizando a eficiência para processamento de sequências extensas. A inovação central do mecanismo de atenção dilatada reside na sua capacidade de expandir o campo ativo exponencialmente com distância crescente. Este mecanismo consegue isso dividindo a entrada (Q, K, V ) em segmentos igualmente grandes com um comprimento de segmento predeterminado W. Cada segmento é então tornado esparsamente ao longo da dimensão da sequência, selecionando linhas a intervalos especificados r. Os segmentos são processados em paralelo, após o que são dispersos e concatenados para produzir a saída. Incorporando o conceito de atenção multi-head, o método inovador melhorou ainda mais o mecanismo de atenção dilatada diversificando a computação entre diferentes cabeças. Especificamente, a computação varia entre cabeças ao espaciar segmentos distintos dos pares (Q, K, V ). Esta arquitetura é compatível integralmente com Transformers com a introdução de tamanhos de segmentos r e taxas de dilatação d como parâmetros hiper.
6 Arquitetura Mamba A arquitetura Mamba [6] é uma solução inovadora projetada para mitigar os desafios de complexidade quadrática enfrentados por modelos de atenção convencionais. Este modelo aborda as restrições de escalabilidade dos modelos de Transformer tradicionais em processar sequências longas e significativamente melhora a eficiência e flexibilidade com complexidade linear em modelagem de sequências. Introduz duas inovações importantes nos modelos de Estado Espacial (SSM): um mecanismo de seleção e um algoritmo consciente do hardware. Um SSM pode ser simplificado pelos parâmetros (∆, A, B, C), onde cada parâmetro representa uma matriz diagonal. A discretização de (∆, A, B) produz (A, B). Este procedimento traduz o modelo de um domínio contínuo para um domínio discreto, permitindo cálculos dentro de um quadro digital. O processo de discretização pode ser feito através de fórmulas fixas A = Fa(∆, A) e B = Fb(∆, A, B), onde o par (Fa, Fb) é uma regra de discretização, como zero-order hold (ZOH). Uma propriedade essencial dos SSMs é a Invariância de Tempo Linear (LTI). Nesse caso, todos os passos de tempo nas dinâmicas representadas pelas matrizes (∆, A, B, C) são os mesmos. Esta característica permite a paralelização de métodos de treinamento, usando algoritmos convolucionais ou recursivos para calcular eficientemente a saída do modelo. A arquitetura Mamba introduz um mecanismo de seleção semelhante ao mecanismo de gating de RNNs que pode se concentrar ou filtrar partes diferentes de um contexto, permitindo que os SSMs adaptem ao tempo e sejam dependentes de entrada. No entanto, essa modificação compromete a propriedade LTI, resultando na incapacidade de aplicar métodos de convolução como anteriormente feito. Para superar isso, os autores implementam um método consciente do hardware para aproveitar a hierarquia de memória de GPUs modernas. O algoritmo consciente do hardware incorpora três técnicas-chave: fusão de kernel, scan paralelo e recompilação. A fusão de kernel reduz a entrada/saída de memória otimizando a combinação de operações em uma única operação de GPU.
Algoritmos de scan paralelo são aplicados para habilitar cálculo paralelo apesar da natureza não-linear das SSMs. A recomputação é utilizada durante a backpropagação para minimizar o uso de memória, recalculando estados intermediários em vez de armazená-los. 

Experimentos foram realizados em um PC com um processador AMD Ryzen 7 7800X3D, 32GB de RAM e uma GPU RTX 3090 de 24GB, rodando o sistema operacional Linux Ubuntu 20.04. Modificamos o código original CleanUNet4 para rodar em Pytorch 2.1.15, CUDA 12.3 e Python 3.10.6, juntamente com as modificações necessárias para implementar a arquitetura Mamba usando mamba-ssm7. Propusemos e avaliamos duas variações da CleanUNet, ambas projetadas para reduzir o número de parâmetros aprendíveis, reduzir complexidade, tempo de treinamento e predição, e manter a qualidade. A primeira variação consiste em aplicar mecanismos de atenção dilatada, enquanto a segunda envolve substituir a arquitetura de transformador pela arquitetura Mamba. Ambos os modelos foram comparados com a CleanUnet descrita em [9], que consideramos a Baseline. A baseline CleanUnet [9] consiste em um codificador e um decodificador, ambos com D camadas e um gargalo composto por N blocos de atenção auto-atenção (camadas de gargalo). Cada camada do codificador é composta por uma camada de convolução 1D (Conv1D) seguida por uma camada ReLu e uma camada de convolução 1x1 (Conv1x1) seguida por uma unidade linear gateada (GLU). Cada Conv1D tem tamanho de kernel K e passo S = K/2. A primeira Conv1D tem H kernels, e as outras camadas dobram o número de canais. A Conv1x1 dobra o número de canais, enquanto a GLU reduz ele pela metade. Cada decodificador é composto por uma camada de convolução 1x1 seguida por uma GLU e uma camada de convolução transposta 1D (ConvTranspose1D). Seguindo a arquitetura U-Net, cada camada do codificador está conectada a uma camada do codificador por uma conexão de saltos na ordem reversa. O gargalo é composto por K blocos de atenção auto-atenção.
Cada bloco de atenção auto-atenção compreende uma camada de atenção auto-atenção multi-cabeça (8 cabeças) e uma camada conectada por completo (tamanho de entrada/saída de 512 e tamanho de camada interna de 2048), como descrito em [9]. Para este estudo, consideramos um modelo CleanUNet com D = 8, K = 4 e N = 5 [9]. Os transformadores com base em atenção dilatada propostos seguem a estrutura UNet do CleanUNet, considerada uma base com um gargalo composto por camadas de atenção dilatada. Para essa configuração, consideramos um modelo CleanUNet com base em atenção dilatada com D = 4, K = 3 e N = 1 para treinamento e previsão no GPU. O bloco de atenção auto-atenção compreende uma camada de atenção auto-atenção multi-cabeça dilatada (32 cabeças) e uma camada conectada por completo (tamanho de entrada/saída de 1024 e tamanho de camada interna de 2048). A lista de tamanhos de segmento {10, 20, 30, 60} foi fornecida com suas respectivas taxas de dilatação {2, 4, 8, 16}. 4 https://github.com/NVIDIA/CleanUNet 5 https://pytorch.org/ 6 https://www.python.org/ 7 https://pypi.org/project/mamba-ssm/1.1.0/ 8 da Silva et al. O CleanUNet proposto com um gargalo composto pela arquitetura Mamba também segue a configuração da base CleanUNet, mas com alterações no número de camadas de codificador/descodificador D = 10, e canais ocultos começando a partir de 32 nos primeiros camadas até canais ocultos máximos de 256. Uma arquitetura Mamba substituiu o gargalo do transformador com N = 1. A dimensionalidade do vetor de entrada nesse gargalo foi definida para 512 (entrada/saída) com 16 camadas. Utilizamos uma configuração de treinamento similar a [9]. Os modelos foram treinados usando otimizador Adam com β1 = 0,9 e β2 = 0,999. A taxa de aprendizado diminui ao longo do treinamento usando um agendador de taxa de aprendizado de anelamento cosino com uma taxa de aprendizado máxima de 0,00025 e uma razão de aquecimento de 5%. Neste trabalho, empregamos a técnica de aumento de dados de shift aleatório dentro de um intervalo de 0 a S segundos e aumento de BandMask [3].
A perda de função CleanUNet descrita em [9] combina uma perda de STFT (Transformada de Fourier no Tempo) multi-resolução com uma perda baseada em onda de som para maximizar o desempenho de desenho de fala. Ela é composta por duas partes: a primeira é a perda L1 aplicada diretamente à onda de som, o que encoraja a saída desenfreada a se assemelhar ao onda de som de fala limpa no domínio do tempo, e a segunda parte é a perda de STFT multi-resolução, que modifica a magnitude do espectrograma em várias resoluções. A Tabela 1 apresenta os resultados ótimos e configurações para o modelo de melhoria de fala, destacando a eficiência da implementação da arquitetura Mamba como gargalo. Essa abordagem reduz significativamente os parâmetros do modelo, permitindo um aumento no throughput de dados e tamanhos de lote maiores. Consequentemente, facilita o treinamento do modelo em um único GPU. Dado a complexidade linear do tratamento de entrada da Mamba, o modelo realiza cálculos mais rapidamente no gargalo. Essa estrutura também suporta sequências de vetores maiores. Da mesma forma, o LongNet reivindica complexidade linear, teoricamente suportando sequências mais longas também. De acordo com os resultados da tabela, o LongNet utiliza poucos codificadores convolucionais para se adaptar às restrições de memória. Essa eficiência permite a preservação e utilização de características que poderiam ser descartadas durante o processo de convolução, melhorando assim o desempenho do modelo. Os resultados apresentados na Tabela 1 são detalhados visualmente pelas gráficos de dispersão na Figura 1, na qual mostramos o número de parâmetros no eixo x versus os índices de qualidade de áudio no eixo y. Na primeira linha, consideramos o número total de parâmetros do modelo, incluindo codificador, decodificador e gargalo, e na última linha, consideramos apenas o número de parâmetros no gargalo. O proposto CleanUNet com gargalo baseado em Mamba reduz significativamente o número de parâmetros treináveis com uma redução ligeira nos índices de qualidade de áudio.
Enquanto o número total de parâmetros é reduzido em 69,59%, a redução no STOI, PESQ-largobanda e PESQ-estreitobanda é apenas de 4,60%, 3,63% e 6,02%, respectivamente. É importante notar que, além de substituir a garganta com uma arquitetura Mamba, o número de camadas nos codificadores/descodificadores foi aumentado, embora o número de canais tenha sido reduzido para permitir que o modelo execute operações mais rápidas enquanto economiza memória. Durante o otimização do CleanUNet para Denoising de Fala, a Tabela 1 apresenta os resultados e configurações do modelo de qualidade de áudio.

Qualidade de Áudio
Tabela 1. Resultados e configurações do modelo CleanUNet

Desempenho de Treinamento
Parâmetros Totais (Milhões) 46,082177 11,882497 14,012545
Parâmetros da Garganta (Milhões) 16,550656 8,401920 6,666240
Configurações da Rede
Convolucionais (D) 8 4 10
Tamanho do Kernel (K) (4x4) (3x3) (5x5)
Passeio (S) 2 2 2
Camadas da Garganta (N) 5 1 1
Configurações de Treinamento
Taxa de Aprendizado 0,0002 0,00025 0,00025
Tamanho da Lote 64 24 32
Comprimento da Corte 1,33s e 1,5s 3s 3s
Divisão - Treinamento/Teste Treinamento/Teste 0 10 20 30 40 50 60
Parâmetros Totais (Milhões) 0,800 0,825 0,850 0,875 0,900 0,925 0,950 0,975 1,000
Métrica de Qualidade de Áudio (STOI)
Banco de Dados D. Atenção Mamba
Inferência (s) 5,13 9,66 13,45
0 10 20 30 40 50 60
Parâmetros da Garganta (Milhões) 1,0 1,5 2,0 2,5 3,0 3,5 4,0
Métrica de Qualidade de Áudio
Banco de Dados Banco de Dados D. Atenção D. Atenção Mamba Mamba
PESQ (Largobanda) PESQ (Estreitobanda)
0 5 10 15 20 25 30
Figura 1. Comparação dos modelos considerando o número de parâmetros vs métricas de qualidade de áudio. Na primeira linha, mostramos o número total de parâmetros.
No último registro, mostramos o número de parâmetros na garganta. A coluna inicial também inclui o tempo de inferência em segundos (s). 10 da Silva et al. realizaram a inferência usando todo o conjunto de teste, e o modelo proposto realizou 46,9% mais rápido que o modelo de referência. 5 Conclusão Este artigo abordou alguns desafios relacionados ao uso de CleanUNet para desenho de fala. Sua boa performance é resultado de uma rede convolucional profunda que combina transformadores e U-Net para melhorar a fala ruim dentro do domínio de onda. O treinamento requer um grande número de recursos, e sua arquitetura limita a adaptação do modelo a hardware limitado em recursos, tornando-o inadequado para aplicações de desenho em tempo real. Exploramos alternativas para a garganta do transformador, como a arquitetura Mamba, capaz de lidar com saídas do encoder de forma mais eficiente com complexidade linear, reduzindo a necessidade de múltiplos camadas convolucionais. Como resultado, drasticamente reduzimos o número de parâmetros treináveis enquanto preservamos sua performance, o que melhorou o treinamento e o tempo de inferência em um único GPU. Agradecimentos. André R. Backes agradece o apoio financeiro da CNPq (Conselho Nacional de Desenvolvimento Científico e Tecnológico, Brasil) (Bolsa #307100/2021-9). Este estudo foi financiado em parte pela Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Código de Financiamento 001.