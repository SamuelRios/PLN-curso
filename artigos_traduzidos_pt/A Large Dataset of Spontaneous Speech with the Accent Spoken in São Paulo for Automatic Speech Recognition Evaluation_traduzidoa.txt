Um grande conjunto de dados de fala espontânea com o sotaque falado em São Paulo para avaliação de reconhecimento de fala automático

Rodrigo Lima1[0009−0009−4344−1109], Sidney E. Leal1[0000−0002−8817−2063], Arnaldo Candido Junior2[0000−0002−5647−0891], e Sandra M. Aluísio1[0000−0001−5108−2630]

1 Universidade de São Paulo, São Carlos, SP 13566-590, Brasil guico21@usp.br, sidleal@gmail.com, e sandra@icmc.usp.br
2 Universidade Estadual Paulista, São José do Rio Preto, SP, 15054-000, Brasil arnaldo.candido@unesp.br

Resumo. Presentamos um corpus de fala espontânea livremente disponível para o português brasileiro e relatamos resultados preliminares de reconhecimento de fala automático (RFA), usando tanto o modelo Wav2Vec2-XLSR-53 quanto o modelo Distil-Whisper, ambos fine-tunados e treinados em nosso corpus. O Corpus de Áudio NURC-SP compreende 401 falantes diferentes (204 femininos, 197 masculinos) com um total de 239,30 horas de gravações de áudio transcritas. Ao melhor de nossa conhecimento, este é o primeiro grande corpus de fala espontânea com sotaque Paulistano dedicado à tarefa de RFA em português. Primeiramente, apresentamos os procedimentos de design e desenvolvimento do Corpus de Áudio NURC-SP, e em seguida, descrevemos quatro experimentos de RFA em detalhes. Os experimentos demonstraram resultados promissores para a aplicabilidade do corpus para RFA. Especificamente, fine-tunamos duas versões do modelo Wav2Vec2-XLSR-53, treinamos um modelo Distil-Whisper usando nosso conjunto de dados com rótulos determinados pelo modelo Whisper Large-V3, e fine-tunamos este modelo Distil-Whisper com nosso corpus. Nossos melhores resultados foram o modelo Distil-Whisper fine-tunado sobre o Corpus de Áudio NURC-SP com um WER de 24,22%, seguido por versões fine-tunadas do modelo Wav2Vec2-XLSR-53 com um WER de 33,73%, o que é quase 10 pontos percentuais pior do que o Distil-Whisper. Para habilitar a reproducibilidade dos experimentos, compartilhamos o conjunto de dados do Corpus de Áudio NURC-SP, modelos pré-treinados e receitas de treinamento nos repositórios Hugging-Face e Github.
Palavras-chave: Avaliação de reconhecimento de fala automático · Fala espontânea · Português brasileiro · Corpora de fala pública

1 Introdução

Datasets públicos ou abertos para treinamento e avaliação de reconhecedores de fala automática (ASR) em português brasileiro (PB) aumentaram em número e horas desde meados de 2020, quando havia aproximadamente 60 horas disponíveis, divididas em quatro pequenos datasets de fala lida. Em 2024, mais de 8 mil horas estão disponíveis para treinar modelos de ASR, seja a partir de datasets automaticamente rotulados ou manualmente revisados (ver Tabela 1 para uma lista de datasets). Esses recursos em PB permitem treinar modelos de ASR de ponta, como Wav2Vec2-XLSR-53 [8] e Distil-Whisper [11]. Especificamente, ambos os modelos são adequados para línguas com recursos baixos ou médios, fornecendo resultados consistentes sem a necessidade de conjuntos de treinamento com dezenas de milhares de horas. Wav2Vec2-XLSR-53 é pré-treinado em um grande conjunto de dados de fala não rotulada que abrange 53 línguas, incluindo o PB, por meio de aprendizado auto-supervisionado. O processo de pré-treinamento permite fine-tuning eficiente em datasets PB disponíveis, gerando resultados competitivos de ASR. Distil-Whisper é uma versão distilada de Whisper [19]. O último, treinado em 680.000 horas de dados multilíngues em uma abordagem multitarefa, mostrou desempenho superior em vários datasets e domínios quando comparado a reconhecedores de fala que utilizam o modelo Wav2Vec2. Uma desvantagem dos vários modelos Whisper grandes é a necessidade de um grande conjunto de dados para melhorar o desempenho e a incapacidade de utilizá-los em ambientes computacionais limitados. Por outro lado, Distil-Whisper é uma variante pequena baseada em distilação de conhecimento que se concentra na eficiência e é 6 vezes mais rápida, 49% menor e realiza dentro de 1% de taxa de erro de palavra (WER) em conjuntos de avaliação fora da distribuição. Até o melhor de nossa conhecimento, Distil-Whisper nunca foi avaliado em datasets PB.
Em relação aos dados utilizados para treinar reconhecedores de fala, [10] os caracteriza em duas dimensões: (i) estilo de produção, que descreve um continuum de espontaneidade, indo desde um lado, que é a fala planejada, até o outro, que seria a fala não planejada; (ii) modo, que caracteriza o processo de obtenção das pares de treinamento, no qual, por um lado, o texto é usado como estímulo para a fala (levando a uma fala lida) e, por outro lado, é a fala que é transcrita em texto. Assim, a fala lida é uma fala planejada que vem de um texto recitado e, ao outro extremo, a fala espontânea/conversacional é caracterizada como fala transcrita em estilo não planejado. Como compromisso, temos uma fala preparada para ser falada posteriormente (por exemplo, os Ted Talks3, nos quais expoentes de todo o mundo apresentam um discurso em 18 minutos ou menos, resultando na necessidade de usar um texto planejado para transmitir a mensagem no curto prazo). A fala espontânea apresenta fenômenos que tornam sua reconhecimento mais complexo do que a de fala lida ou preparada. Como resultado, conjuntos de dados com fala lida/preparada foram primeiramente e amplamente disseminados para treinar reconhecedores de fala ou sintetizadores de fala, principalmente para o idioma inglês (ver [24], [17]), pois as palavras nos áudios correspondem diretamente às palavras do transcript, pois foram lidas. No entanto, a fala mais natural e espontânea contém muitos fenômenos prosódicos que não estão presentes na fala lida, como graus mais altos de redução segmental5 e formas mais complexas de variação (preenchedores, auto-correções e repetições) afetadas pelo ritmo de fala. 3 https://www.ted.com/talks 5 Por exemplo, [5] comenta sobre a elisão de vogais entre as palavras que, no dialeto de São Paulo, afeta as vogais finais postônicas /a/, /o/ e /u/. Por exemplo, no exemplo (a) me’ren[da es]co’lar (lunch de escola) –> me’ren[des]co’lar, a vogal /a/ é deletada e uma sílaba nova é criada ([des]).
Um Grande Conjunto de Dados de Fala Espontânea...

3 Assim, os vários tipos de fala espontânea comuns na vida diária, como aulas, conversas entre duas ou mais pessoas e entrevistas, são materiais importantes para os conjuntos de dados utilizados para treinar ASRs, pois eles trazem intonação natural a perguntas, declarações e expressões de emoções como surpresa, admiração, indignação, raiva, assombro, medo, exaltação, entusiasmo, entre outros [22, 4]. Além disso, eles trazem fenômenos linguísticos como pausas preenchidas, geralmente escritas como “eh”, “ah”, “ahh”, “mm”, “uhn” e disfluências de edição (repetições de palavras ou partes de palavras, revisões do que se pretende dizer, com reinício de fala) [16]. Consequentemente, reconhecedores de fala são esperados para se desempenhar pior com fala espontânea do que quando aplicados à fala lida. [19] mostra a aplicação do ASR Whisper a 14 conjuntos de dados de fala lida e espontânea em língua inglesa. Os valores mais altos de WER (Taxa de Erro de Palavra) ocorrem para os conjuntos de dados de fala espontânea, por exemplo, CHiME65 tem um WER de 25,5 e AMI SDM16 tem um WER de 36,4, enquanto Common Voice7 tem um WER de 9,0 e Tedlium8 tem um WER de 4,0, usando o modelo large-v2. Para o português brasileiro, [15] relata um WER de 14,50% para o modelo Whisper large-v2 aplicado a um conjunto de dados de fala espontânea portuguesa de aproximadamente 17 horas de entrevistas sobre histórias de vida. Este artigo apresenta um novo corpus de fala espontânea BP, particularmente do sotaque paulistano (cidade de São Paulo), adequado para treinar e avaliar sistemas de reconhecimento de fala. O Corpus de Áudio NURC-SP é parte da divisão de São Paulo do projeto NURC (Norma Urbana Linguística Culta), um projeto projetado para documentar e estudar a língua portuguesa falada por pessoas com um alto grau de educação formal em cinco capitais brasileiras (veja detalhes deste corpus na Seção 3). Ele contém 239,30 horas de áudios amostrados a 16 kHz e suas respectivas transcrições, totalizando 170k áudios segmentados.
Os áudios foram transcritos automaticamente pela primeira vez e revisados manualmente com o objetivo de tarefa de reconhecimento de fala (ASR). Portanto, este novo corpus adiciona 239 horas ao total disponível para treinamento e avaliação de sistemas de reconhecimento de fala. Para comparar a qualidade do nosso corpus, quatro modelos de ASR são disponibilizados nesse trabalho: (i) uma versão fine-tuned de Wav2Vec2.0 XLSR-53 com os conjuntos de treinamento e validação do NURC-SP Audio Corpus, (ii) o mesmo que o primeiro, mas usando como ponto de partida o modelo treinado por [6] para CORAA-ASR v1.19, (iii) uma versão distilada do modelo Whisper Large-v3 [19], um modelo que tem suporte para o idioma português, treinado usando nosso conjunto de dados com rótulos determinados pelo Whisper Large-v3, e (iv) uma versão fine-tuned do terceiro modelo com os conjuntos de treinamento e validação do NURC-SP Audio Corpus. 5 https://openslr.org/150/ 6 https://groups.inf.ed.ac.uk/ami/corpus/overview.shtml 7 https://commonvoice.mozilla.org/en/datasets 8 https://www.openslr.org/51/ 9 https://github.com/nilc-nlp/CORAA 4 Rodrigo Lima et al. O corpus e os modelos treinados estão disponíveis publicamente em nosso repositório do GitHub10 sob a licença CC BY-NC-ND 4.0. As principais contribuições feitas nesse trabalho são resumidas da seguinte forma: 1. Um grande corpus de BP de pares de áudio-transcrição validados por humanos contendo 239,30 horas de fala espontânea. 2. O primeiro corpus, segundo nossa conhecimento, que aborda uma grande quantidade de fala paulistana com sotaque para ASR em BP (CORAA ASR traz 31,14 horas de fala da capital de São Paulo). 3. Quatro modelos de ASR, disponíveis publicamente, com base no corpus apresentado. 2 Trabalhos Relacionados com Datasets de Português Brasileiro para ASR A Tabela 1 apresenta grandes corpora para construir sistemas de ASR focados na língua portuguesa. Algumas recursos são multilíngues, no entanto, a Tabela 1 detalha especificamente as estatísticas para a língua portuguesa.
Entre os recursos apresentados, há uma ligeira preponderância da variante brasileira nos recursos existentes, embora a variante portuguesa europeia também seja incluída em alguns deles. Tabela 1. Estatísticas dos principais conjuntos de dados disponíveis para reconhecimento de fala em português. Alguns conjuntos de dados são multilíngues, no entanto, os números mostrados são para o idioma português. Corpora (Data de Lançamento) Estilo de Fala Horas Número de Áudios Número de Falantes Licença Multilíngue LibriSpeech (MLS) (2020) leitura 130,1 - 54 CC BY Multilíngue TEDx (2021) preparado 164 93.000 - CC BY-NC-ND 4.0 Spotify Podcast Dataset (2022) espontânea 7.600 123.000 - conjunto de dados proprietário CORAA ASR 1.1 (2022) espontânea 290 402.466 1.689 CC BY-NC-ND 4.0 Common Voice 17.0 (2024) leitura 175 - 3.453 CC-0 NURC-SP Audio Corpus (2024) espontânea 239,30 177.224 401 CC BY-NC-ND 4.0 Um dos projetos mais conhecidos que lidam com fala lida/preparada em inglês é o Librivox11, que distribui livros do domínio público em formato de áudio. Esses áudios foram usados em vários projetos para criar recursos para processamento de fala em inglês, como o LibriSpeech ASR Corpus12 e o LibriTTS13, ambos hospedados no repositório de Recursos de Fala e Linguagem Abertos. O corpus MultiLingual LibriSpeech (MLS) [18] é um grande corpus multilíngue adequado para pesquisa de fala, derivado de áudios de livros lidos do LibriVox e consiste em 8 línguas, incluindo cerca de 32K horas de inglês e um total de 4,5K horas para outras línguas. Para o português, há 131 horas e 54 áudios. Especificamente para a tarefa de reconhecimento, pode ser combinado com outros recursos, pois tem poucos falantes (falantes de áudio de livros). Esses recursos consistem em áudio limpo, geralmente de qualidade de estúdio.
Devido a isso, modelos construídos exclusivamente com esse tipo de áudio são apenas adequados para reconhecimento de fala em cenários de baixo nível de ruído. Para superar essa limitação, uma solução seria injetar ruído no áudio ou combiná-lo com áudio de outros projetos em diferentes níveis de qualidade. O Corpus MultiLingual TeDx [21] foi proposto para habilitar pesquisas nas áreas de reconhecimento de fala automático e tradução de fala para texto. Para o português, há 164 horas disponíveis em 93k áudios. O corpus é composto por palestras sobre uma ampla gama de assuntos, sendo gerenciado no âmbito do projeto TEDx, ligado ao grupo TED (Tecnologia, Entretenimento e Design). No caso do português, também há traduções das transcrições para o inglês e espanhol. Além disso, áudios em espanhol e francês também têm traduções para o português. O corpus Spotify [7] foi lançado inicialmente em inglês. Em 2022, a empresa lançou uma nova versão [14] que inclui o português [12], oferecendo vários áudios para o idioma português, principalmente de podcasts disponíveis na plataforma. Em total, 76k horas de áudio foram tornadas disponíveis a partir de 123k episódios de programas na plataforma. As transcrições foram geradas automaticamente e estão sujeitas a erros de transcrição. Possui licença gratuita para uso acadêmico, mas pesquisadores interessados em acessar os áudios devem submeter uma solicitação de acesso no site dos organizadores. O CORAA ASR [6] é um corpus para reconhecimento de fala automático que contém fala espontânea especialmente preparada. O CORAA ASR é a combinação de cinco projetos independentes que lidam com fala no interior de São Paulo (com 35,96 horas — Projeto ALIP), Minas Gerais (com 9,64 horas — Projeto C-ORAL Brasil), Recife (141,31 horas — Projeto NURC-Recife) e São Paulo capital (31,14 horas — Projeto SP2020), além da fala preparada de palestras do TeDx Talks em português e português brasileiro (72,74 horas), totalizando 290 horas e 402k áudios.
O corpus de Voz Comum [1] é um projeto de uso aberto criado pela Fundação Mozilla. O projeto é uma resposta à falta de recursos para vários idiomas, incluindo o português. No projeto, os usuários podem contribuir simultaneamente para o crescimento da base e acessar áudios de outras pessoas. Para colaborar com o projeto, os usuários podem doar áudios em suas próprias vozes e revisar doações de outros usuários. O projeto tem ferramentas para coleta, validação e internacionalização (adaptabilidade a diferentes idiomas). A licença de uso permissiva desse projeto permite a exploração do corpus, incluindo fins comerciais. Na versão 17, o subcorpus para o idioma português tem 211 horas de áudio e transcrições, das quais 175 foram validadas. O Corpus de Áudio NURC-SP, o foco desse artigo, é descrito em detalhes na Seção 3.

O Corpus NURC-SP foi a divisão de São Paulo do projeto NURC (Norma Urbana Linguística Culta). O NURC-SP coletou mais de 300 horas de falantes de São Paulo ao longo da década de 1970. O corpus NURC-SP é composto por 375 gravações de áudio de três tipos de gênero: 1. (DID) Documentador e Participante - Diálogo realizado entre um documentador e um participante diretamente; 2. (D2) Participante e Participante - Diálogo entre dois participantes mediado por um documentador; e 3. (EF) Participante - Aulas, seminários, classes, discursos em geral, dados por um participante em um contexto formal. Esses são divididos em três subcorpos: o Corpus Mínimo (21 gravações de áudio), o Corpus de Áudios e Transcrições Não-Alinhados (26 gravações de áudio) e o Corpus de Áudio.
O Centro de Documentação Cultural Alexandre Eulalio (CEDAE/UNICAMP) digitizou os áudios analógicos originais do NURC-SP em dezembro de 2020 e tornou-os disponíveis para o Projeto Tarsila15 como recurso para: (i) construir conjuntos de dados de treinamento para sistemas de reconhecimento de fala espontânea e (ii) facilitar estudos linguísticos futuros. Os três subcorpos que integram o repositório digital do NURC-SP foram disponibilizados no Portal NURC-SP Digital [20] para pesquisadores de Linguística e para o público em geral, devido ao fácil acesso e ferramentas de filtragem para utilizar o material. Aqui, neste artigo, nos concentramos no Corpus de Áudio do NURC-SP. Ele foi originalmente composto por 328 gravações de áudio sem transcrições, que foram automaticamente transcritas pelo WhisperX [3]. A revisão dos segmentos de transcrição automatizados pelo WhisperX foi realizada de junho de 2023 a dezembro de 2023 por 14 falantes nativos do BP. O processo de revisão foi baseado em um guia de anotação projetado para: (i) ajudar a tornar a revisão uniforme e (ii) remover segmentos com grande quantidade de ruído e vozes sobrepostas. O guia contém 11 regras, lidando com (i) marcas de oralidade, (ii) como transcrever pausas preenchidas, (iii) hesitações repetidas, (iv) números, (v) letras individuais, (vi) abreviaturas, (vii) termos estrangeiros, (viii) pontuação e capitalização, (ix) sons de emoção (ex. risos) que foram anotados entre parênteses, (x) compreensão de palavras ou trechos e (xi) como lidar com falhas de segmentação automática.

3.1 Processamento e filtragem do Corpus de Áudio do NURC-SP para avaliar modelos de ASR
A qualidade do áudio foi um caráter muito importante quando se preparava o conjunto de dados para avaliar modelos de ASR. Nesta seção, apresentamos os passos de processamento do Corpus de Áudio do NURC-SP para gerar a versão utilizada para avaliar os quatro modelos de ASR nesse artigo: 15 https://sites.google.com/view/tarsila-c4ai/home Um Grande Conjunto de Fala Espontânea... 7 1.
Remoção dos áudios (e de todos os seus segmentos) com qualidade ruim/baixa (com um grande número de problemas de distorção de voz, chiado, ruído de fundo e interrupções); 2. Anotação dos segmentos usando rótulos de qualidade "alta/baixa" para todos os áudios restantes, com base no que foi relatado por anotadores humanos durante a revisão da transcrição automática; 3. Remoção de parte dos segmentos de baixa qualidade, detalhada mais adiante nesta seção, com o objetivo de gerar o conjunto de dados final para treinar modelos de reconhecimento de fala automática (ASR); e 4. Geração de estatísticas do Corpus de Áudio da NURC-SP e dos conjuntos de treinamento, validação e teste. Dos 328 áudios, quatro tinham qualidade ruim, 92 tinham qualidade média, 52 tinham qualidade boa e 180 tinham qualidade excelente. Portanto, desses 328 áudios, cinco não foram usados para treinar os modelos de ASR nesse artigo: SP_EF_395, SP_DID_283, SP_DID_194, SP_D2_397, SP_D2_337, pois tinham problemas de distorção de voz, chiado, ruído de fundo e interrupções, o que os tornava responsáveis por gerar transcrições com uma taxa alta de erros. As transcrições foram processadas para remover os rótulos usados na revisão da transcrição automática: (i) “###” foi usado para indicar segmentos com ruído de fundo alto, áudio muito baixo, vozes superpostas e música; (ii) sons paralinguísticos, como risos ou tosse, foram anotados entre parênteses — (riso), (toss); (iii) compreensão de palavras ou trechos foi também marcada com parênteses “( )”; e (iv) palavras truncadas no início ou no final do áudio devido ao erro de segmentação automática foram transcritas parcialmente com a ajuda de “>” e “<” (por exemplo, se a palavra “casa” – casa) foi truncada no final do segmento, foi anotada como “ca>” e se foi no início, foi anotada como “<sa”. Segmentos marcados com “###” são removidos completamente, bem como segmentos que contenham apenas sons paralinguísticos (por exemplo, risos) ou contenham apenas palavras truncadas.
Trechos com palavras truncadas e não truncadas (marcados com “>” ou “<”) tiveram a palavra truncada removida e foram marcados como de baixa qualidade. Pares que tinham palavras ou passagens marcadas como mal compreendidas, usando as tags “( )”, tiveram as tags removidas e os pares foram marcados como “baixa qualidade”. Pares de transcrição de áudio com apenas tags de sons paralinguísticos tiveram as tags removidas e foram marcados como “alta qualidade”. O conjunto de dados disponível nesse trabalho tem 323 pares de áudio-transcrição, que foram divididos em 177.224 segmentos transcritos pelo WhisperX e revisados por falantes nativos de BP (veja mais detalhes na Tabela 2).

3.2 Estatísticas do Corpus de Áudio NURC-SP

Tabela 2 apresenta estatísticas do Corpus de Áudio NURC-SP. Em geral, quase 240 horas de fala foram geradas na exportação final, distribuídas em cerca de 177 mil segmentos com uma duração média de aproximadamente 5 segundos. No total, mais de 2 milhões de tokens foram transcritos, resultando em aproximadamente 12 tokens por segmento. É importante notar que a soma de áudios com vozes femininas é maior que o número total de áudios, pois um áudio pode ter simultaneamente ambos os tipos de vozes. A Tabela 2 apresenta as estatísticas do Corpus de Áudio NURC-SP.

Tabela 2. Estatísticas do Corpus de Áudio NURC-SP

Treinamento Validação Teste Total
Número de Áudios (único) 303 6 14 323
Vozes femininas (número)* 191 4 9 204
Vozes masculinas (número)* 186 3 8 197
Razão Masculino/Feminino 0,97 0,75 0,89 0,97
Duração (horas) 224,47 4,60 10,23 239,30
Quantidade de Áudios (segmentado) 166.971 3.142 7.111 177.224
Duração do Segmento (média segundos) 4,83 5,27 5,18 4,86
Duração do Segmento (máxima segundos) 29,87 29,70 28,98 29,87
Média Tokens 11,81 12,67 12,32 11,85
Média Tipos 10,61 11,73 12,29 10,69
Total Tokens 1.971.993 39.715 87.598 2.099.306
Total Tipos 84.767 8.005 12.218 88.004
Razão Tipo/Token 0,043 0,202 0,139 0,042
*Existem áudios com dois falantes, em várias combinações: Masculino e Masculino, Masculino e Feminino, Feminino e Feminino. E o número de vozes masculinas é maior que o número total de áudios, pois um áudio pode ter simultaneamente ambos os tipos de vozes. A Figura 1 mostra a duração dos segmentos de áudio.
É possível notar que os áudios menores que 5 segundos são predominantes, com um pico aproximadamente em 3 segundos, enquanto o intervalo entre 5 e 20 segundos também está bem representado. Existem também áudios mais longos que 20 segundos, mas sua ocorrência é menos comum. A figura 2 mostra o número de áudios por idade e gênero. O número de áudios de femininos e masculinos é semelhante, resultando em um balanceamento razoável em relação ao gênero, que também pode ser visto na razão masculina/feminina na Tabela 2. Em relação à idade, há mais falantes no grupo de idade II (entre 36 e 55 anos) do que nos outros grupos (I=25-35 e III=56 em diante). Há também um pequeno número de áudios com falantes de idade desconhecida. A figura 3 mostra o número de áudios por gênero de fala (Palestras e Discursos — (EF), Entrevistas (DID) e Diálogos (D2), respectivamente). Entrevistas e diálogos são muito mais comuns do que palestras, totalizando 90% dos áudios. Assim, o corpus NURC-SP é predominantemente composto por fala espontânea.

4 Experimentos com o Corpus de Áudio NURC-SP Realizamos experimentos sobre o Corpus de Áudio NURC-SP com o objetivo de avaliar a qualidade e limitações do conjunto de dados. Baseamos nossos experimentos em duas principais arquiteturas: Wav2Vec2 [8] e Distil-Whisper [11]. Wav2Vec2 tem o advantage de apresentar um desempenho bom em línguas de baixa e média recursos. Wa2Vec2 é um pré-treinamento auto-supervisionado em diferentes línguas, o que acelera o processo de treinamento para ASR, permitindo robustez contra ruídos comuns em fala espontânea. Distil-Whisper [11] é uma versão destilada de Whisper, o estado-da-arte em ASR em várias línguas. Optamos pela versão destilada.
Por ser o original Whisper custoso, treinado originalmente por milhares de horas de fala rotulada, enquanto o Distil-Whisper pode realizar eficientemente com menos dados de treinamento. O Distil-Whisper é menor e mais rápido, enquanto preserva muitas das fortes características do original Whisper. A arquitetura Wav2vec2 opera diretamente no sinal de onda, sem a necessidade de representações intermediárias como espectrogramas. Além disso, há dois principais tipos de camadas: camadas convolucionais processam o sinal de onda bruto; camadas Transformer [23] processam o resultado do passo anterior. O modelo pode ser treinado de forma auto-supervisionada. O sinal também é quantizado após passar pelas camadas convolucionais e o modelo de linguagem mascarada é usado nas camadas Transformer. Uma perda rica considera representações quantizadas e previsões da camada Transformer para produzir representações quantizadas diversificadas que representam janelas pequenas no áudio original. As representações aprendidas podem então ser refinadas para representar, por exemplo, fones no processo de reconhecimento de fala. Nesse caso, é usado uma perda seq2seq clássica para geração de ASR, nomeadamente a perda CTC [13]. O Whisper propõe uma abordagem off-the-shelf para sua arquitetura, consistindo em um transformador tradicional, com blocos de codificação e decodificação, e se concentrando mais nos dados de treinamento do que nas ajustes arquiteturais. O input deve ser convertido para formato de espectrograma log-mel. O Distil-Whisper é a versão distilada do Whisper. A técnica de distilação de modelo permite que um modelo menor (o aluno) seja treinado em ambos os dados rotulados e previsões de um modelo maior (o professor) em uma tarefa de previsão dada.
A ideia principal por trás da distilação é que as previsões de um modelo têm informações mais ricas sobre a distribuição de dados do que as etiquetas originais. Portanto, um modelo maior pode capturar a distribuição de dados e transmiti-la para o aluno. Ambos Whisper e Distil-Whisper estão disponíveis em diferentes versões. Comparando Whisper Large-v3 com Distil-Whisper Large-v3 (utilizado nesse trabalho), pode ser observado um melhoramento de 5,8 vezes no tempo de inferência ao usar 51% menos parâmetros na versão distilada. Tabela 3 apresenta estatísticas das partições de treinamento, validação e teste de cada subconjunto do NURC-SP Audio Corpus. Um Grande Conjunto de Fala Espontânea... 11 Tabela 3. Estatísticas das partições de treinamento, validação e teste de cada subconjunto do NURC-SP Audio Corpus: duração total em horas e número de gravações de áudio (esquerda) e número de falantes em cada gênero (direita). Subconjunto Horas/Número Gravações Falantes (M|F) Treinamento Validação Teste Treinamento Validação Teste D2 75,93/75 1,45/1 3,12/3 66|82 1|1 3|3 DID 132,39/205 1,46/2 3,83/6 103|103 1|1 3|3 EF 16,15/23 1,69/3 3,87/5 17|6 1|2 2|3 Total 224,47/303 4,60/6 10,23/14 186|191 3|4 8|9 4.1 Modelos de Referência Desenvolvimento Para avaliar a qualidade do conjunto de dados, treinamos quatro novos modelos de ASR e comparamos os resultados com trabalhos relacionados anteriores. Dois deles são versões fine-tuned de Wav2Vec2 e os modelos terceiro e quarto são abordagens com Distil-Whisper. Todos os quatro modelos são descritos abaixo. Wav2Vec2-NURC-SP-1. Esse modelo é uma versão fine-tuned do Wav2Vec 2.0 XLSR-53 [2] [9], pré-treinado em 53 línguas (incluindo o português). O modelo pré-treinado foi fine-tuned com nossos conjuntos de treinamento e validação de (NURC-SP Audio Corpus) NURC-SP-AC em um GPU Nvidia DGX A100 80GB durante 16 épocas, com parada precoce de 10. As outras configurações foram as mesmas de [6], o código de treinamento está disponível no github16. Wav2Vec2-NURC-SP-2.
O segundo modelo é quase o mesmo que o primeiro, mas usando como ponto de partida o modelo treinado por [6] para CORAA-V1 e disponibilizado publicamente no HuggingFace17. Também foi treinado por 16 épocas e terminou com um melhor WER do que o modelo inicial. Distil-Whisper-NURC-SP. O terceiro modelo é uma versão distilada de Whisper Large-v3 [19], um modelo que tem suporte para o idioma português, treinado usando nosso conjunto de dados com rótulos determinados por Whisper Large-v3, com o objetivo de que mais conhecimento possa ser passado do modelo mestre para o modelo aluno dessa forma [11]. O modelo foi treinado com nossos conjuntos de treinamento e validação de NURC-SP-AC em um GPU Nvidia DGX A100 de 80 GB por 48 épocas, seguindo os passos dados pelo Distil-Whisper github18. Distil-Whisper-NURC-SP-Fine-Tuned. Esse modelo é uma fine-tuning do terceiro modelo com nossos conjuntos de treinamento e validação de NURC-SP-AC, seguindo os passos recomendados pelos desenvolvedores do Distil-Whisper19. Também foi treinado por 48 épocas em um GPU Nvidia DGX A100 de 80 GB e alcançou um melhor WER e CER (Taxa de Erro de Caracteres)20 do que o modelo terceiro.
As transcrições revisadas do Corpus de Áudio NURC-SP são de gêneros de texto diferentes (EF, D2 e DID) e utilizam letras maiúsculas e minúsculas, bem como marcas de pausa preenchidas como "eh", "hum", "ãh", etc. Para simplificar o treinamento e o cálculo de métricas de CER e WER, foi realizada a seguinte normalização: 1. Os textos foram transformados em minúsculas; 2. Todos os sinais de pontuação gerados pelo Whisper foram removidos (pontuação de interrogação, ponto e vírgula); 3. As pausas preenchidas foram padronizadas para: "eh" = "eh", "éh", "ehm", "ehn"; "uh" = "uh", "hm", "uhm", "hmm", "mm", "mhm"; "ah" = "ah", "huh", "ãh", "ã"; 4. Qualquer espaço em branco consecutivo foi substituído por um espaço único.

4.3 Resultados dos Experimentos

Para fins de comparação, mantivemos os testes nos conjuntos de dados utilizados em trabalhos anteriores e adicionamos o novo conjunto de dados disponibilizado por este trabalho. Os modelos [14] e [6] foram reexecutados em todos os conjuntos de dados, assim como os quatro novos modelos treinados. As métricas de WER e CER foram avaliadas para cada execução. O novo conjunto de dados provou ser muito desafiador para a tarefa de reconhecimento de fala, como pode ser visto na Tabela 4.

Tabela 4. Resultados dos Modelos de Referência do Corpus de Áudio NURC-SP comparados com trabalhos anteriores. Os melhores valores para CER e WER aparecem em negrito.

Conjuntos de Dados Common Voice CORAA v1 NURC-SP Audio Corpus Média Métricas CER WER CER WER CER WER CER WER Gris et al. (2022) [14] 4,50 16,32 22,32 43,70 26,52 47,74 17,78 35,92 Candido Jr et al. (2023) [6] 6,99 24,44 11,02 24,18 22,87 40,29 13,63 29,64 Wav2Vec2-NURC-SP-1 10,41 35,74 24,24 49,13 23,69 43,44 19,45 42,77 Wav2Vec2-NURC-SP-2 8,07 26,74 14,59 31,19 19,30 33,73 13,99 30,55 Distil-Whisper-NURC-SP 7,14 18,66 23,53 36,17 25,03 36,25 18,57 30,36 Distil-Whisper-NURC-SP-Fine-Tuned 5,70 17,76 14,89 26,91 15,77 24,22 12,12 22,96

Abaixo, mostramos quatro exemplos de instâncias de resultados do nosso melhor modelo (Distil-Whisper NURC-SP-Fine-Tuned) — o primeiro é a Original Normalized (ON) e o segundo é a Model's Prediction Normalized (MPN).
No primeiro dois, há problemas com entidades nomeadas, pois o "Martinelli" foi transcrito como "martini" e o nome "Paulo Emilio Salles Gomes" teve uma transcrição errada como "paulo e milho fales gomes". O modelo utilizou nomes comuns mais frequentes do que os nomes próprios "Martinelli" e "Emilio Salles". Palavras raras relacionadas a certos domínios (por exemplo, alimentação) ou terminologias de áreas de pesquisa (por exemplo, linguística) também trazem alguma dificuldade para o modelo (ver terceiro e quarto exemplos):

1. (ON) o martinelli ficou célebre em todo o exterior do estado no interior do estado de são paulo e mesmo pelo brasil afora como um arranha-céu notável para a época; (MPN) o martini ficou célebre em todo o exterior do estado do interior de estado de são paulo e mesmo pelo brasil afora como arranha-céu notável para a época;
2. (ON) você me falou em cinema eu lembrei de paulo emilio salles gomes que foi meu colega na faculdade e é um entendidíssimo de cinema né; (MPN) você me falou em cinema eu me lembrei de paulo e milho fales gomes que foi minha colega na faculdade e é um entendidíssimo de cinema né;
3. (ON) cuscuz paulista bobó de camarão essas coisas assim; (MPN) cuscos paulista babota de camarão essas coisas;
4. (ON) de um lado objeto direto do outro adjunto; (MPN) de um lado é o chefe do e o outro é de junho.

5 Discussão e Conclusões Neste artigo, apresentamos um corpus de fala espontânea livremente disponível para o idioma português brasileiro, totalizando 239,30 horas, e relatamos resultados preliminares de reconhecimento de fala automatizado, utilizando tanto o modelo Wav2Vec2-XLSR-53 quanto o modelo Distil-Whisper treinados e fine-tunados em nosso corpus. Ao melhor de nossa conhecimento, o Distil-Whisper nunca foi avaliado para conjuntos de dados BP. Para fins de comparação, também trazemos os testes nos conjuntos de dados utilizados em trabalhos anteriores. O modelo baseado em Wav2vec 2.0 da [14] permanece o melhor para o conjunto de dados Common Voice com um WER de 16,32%, mesmo na versão mais recente (versão CV 17).
0), treinado em fala preparada/leitura. No conjunto de dados CORAA v1, os modelos de [6], que também treinaram um modelo Wav2Vec 2.0, mas principalmente baseados em fala espontânea, continuam apresentando os melhores valores para esse conjunto de dados com 24,18% de WER, mas nosso modelo Wav2Vec2-NURC-SP-2 ocupou a segunda posição (pois também foi treinado para fala espontânea). Para o novo conjunto de dados, foco desse artigo, os melhores resultados foram os modelos Distil-Whisper fine-tunados sobre o Corpus de Áudio NURC-SP com um WER de 24,22%, seguidos por versões fine-tunadas do modelo Wav2Vec2-XLSR-53 com um WER de 33,73%, que é quase 10% pior que o Distil-Whisper. Esses resultados indicam que o Distil-Whisper é promissor para línguas de baixa e média recursos e deve ser avaliado com mais conjuntos de dados BP no futuro. Agradecimentos. Esta obra foi realizada no Centro de Inteligência Artificial (C4AI-USP), com apoio da Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP, processo #2019/07665-4) e da IBM Corporation. Este projeto também foi apoiado pelo Ministério da Ciência, Tecnologia e Inovação, com recursos da Lei nº 8.248, de 23 de outubro de 1991, no âmbito do PPI-SOFTEX, coordenado pela Softex e publicado na TIC 13, DOU 01245.010222/2022-44.