Desenvolvendo Modelos de Linguagem Clínica Eficientes em Recursos para Português Brasileiro

João Gabriel de Souza Pinto1,2[0009−0006−7133−3074], Andrey Rodrigues de Freitas1[0009−0008−6511−3248], Anderson Carlos Gomes Martins3[0009−0001−4637−0301], Caroline Midori Rozza Sawazaki1[0009−0008−2251−1131], Caroline Vidal1[0009−0003−6904−2719], e Lucas Emanuel Silva e Oliveira1,2[0000−0003−1811−5087]

1 Pontifícia Universidade Católica do Paraná (PUCPR), Curitiba (PR), Brasil
2 Comsentimento NLP Lab, São Paulo (SP), Brasil http://www.comsentimento.com.br
3 Instituto Federal de Goiás (IFG), Luziânia (GO), Brasil

Resumo. Neste estudo, desenvolvemos e avaliamos dois modelos de linguagem médica, Clinical-BR-LlaMA-2-7B e Clinical-BR-Mistral-7B-v0.2, especificamente projetados para o português brasileiro. Utilizando a técnica de Adaptação de Rango Baixo (LoRA), nossos modelos alcançaram melhorias significativas na geração de texto clínico sintético, especialmente em termos de Autenticidade de Formato e Estrutura, Precisão Ortográfica e Coerência Clínica. A avaliação, conduzida por estudantes de medicina usando uma escala de Likert de 5 pontos, demonstrou a eficácia de nossa abordagem em comparação com modelos de base. Os resultados indicam desempenho superior em comparação com modelos de base como LlaMA-2-7B e Mistral-7B-v0.2. Nossos resultados sugerem que esses modelos eficientes em recursos podem gerar texto clínico relevante com alta qualidade de estrutura, precisão e coerência. O trabalho futuro se concentrará em expandir conjuntos de dados, refinar protocolos de avaliação e melhorar a robustez do modelo para melhorar o desempenho em várias tarefas médicas.

Palavras-chave: Modelos de Linguagem Grande · NLP Médico · Inteligência Artificial Gerativa
Modelos de Linguagem Grande (LLMs), um subconjunto dessas arquiteturas, são centrais na pesquisa de processamento de linguagem natural (NLP) devido à sua capacidade de gerar texto coerente e relevante no contexto. Isso os torna instrumentais em aplicações que vão desde a criação de conteúdo automatizada até agentes conversacionais. A versatilidade e escalabilidade dos LLMs destaca seu potencial para revolucionar as interações com informações digitais, impulsionando avanços na inteligência artificial. No campo médico, os LLMs melhoram a eficiência e a precisão dos processos de tomada de decisão clínica[5]. Esses modelos excelentemente interpretam linguagem médica complexa e extraem informações relevantes de textos não estruturados[29], ajudando a entender histórias de pacientes e resultados de tratamentos[29]. Os LLMs simplificam procedimentos administrativos e apoiam decisões diagnósticas/terapêuticas[28], potencialmente melhorando resultados de pacientes e eficiências operacionais[5]. Por exemplo, LLMs como Med-PaLM podem alcançar escores de aprovação em exames de licenciamento médico, demonstrando seu potencial em tarefas de conhecimento clínico e respostas a perguntas[29]. Eles também extraem informações estruturadas de notas e relatórios clínicos em várias línguas, melhorando a precisão diagnóstica e ajudando na disseminação de informações relacionadas à saúde[28]. A falta de recursos em línguas outras que o inglês é um problema persistente na pesquisa de NLP, e o desenvolvimento de LLMs não é exceção. Essa lacuna é ainda agravada pelo poder computacional substancial necessário para treinar LLMs desde o início, um recurso predominantemente acessível a grandes corporações. É essencial encontrar maneiras de tornar essas tecnologias acessíveis à comunidade de pesquisa e acadêmica, aproveitando modelos multilíngues públicos e adaptando-os a contextos específicos. Neste trabalho, nos concentramos em criar um LLM médico útil para o português brasileiro (pt-BR) com recursos computacionais mínimos.
Nossa metodologia envolveu continuar o treinamento prévio (ou fine-tuning não supervisionado) dos modelos base LlaMA-2-7b e Mistral-7b-v0.2 utilizando três conjuntos de dados de narrativas clínicas. Aplicamos a técnica LoRA (Adaptação de Rango Baixo) para melhorar a eficiência desse processo. Os novos modelos, chamados Clinical-BR-LlaMA-2-7B e Clinical-BR-Mistral-7B-v0.2, foram avaliados em sua capacidade de gerar dados clínicos sintéticos e os resultados mostraram que nossos modelos superaram outros modelos de base em gerar dados clinicamente relevantes. Esta iniciativa é parte de um projeto colaborativo entre HAILab e Comsentimento, denominado MED-LLM-BR1, que visa desenvolver vários modelos LLM médicos para pt-BR, incluindo modelos base e modelos específicos para tarefas, com diferentes tamanhos. Até o melhor de nossa conhecimento, este é o primeiro modelo LLM clínico público disponível para a língua portuguesa. Nossa obra aborda as necessidades específicas e lacunas da comunidade de Informática em Saúde do Brasil, fornecendo um recurso valioso para clínicos e pesquisadores utilizar em seus projetos de IA em saúde. Ao superar desafios linguísticos e técnicos, contribuímos para o campo mais amplo de processamento de linguagem natural e informática médica, destacando a importância do conhecimento específico do domínio no processamento de linguagem médica.

2 TRABALHOS RELACIONADOS O desenvolvimento de Modelos de Linguagem Grande (LLMs) revolucionou o processamento de linguagem natural (NLP), especialmente no campo médico. Esta seção revisa avanços em arquiteturas de Transformer, a importância do aprendizado por transferência e fine-tuning, e estratégias computacionais para otimizar esses modelos. Examinamos iniciativas chave em outras línguas, particularmente em português, e destacamos 1 https://github.com/HAILab-PUCPR/MED-LLM-BR/ LLMs médicos para português brasileiro 3 modelos LLM médicos prominentes para fornecer contexto para nosso modelo LLM médico eficiente para português brasileiro. O advento dos Transformers, introduzido por Vaswani et al.
[26] revolucionou o campo de processamento de linguagem natural (PLN). Essa arquitetura utiliza mecanismos de atenção auto-ajustáveis para lidar com dependências de longo alcance em texto, melhorando significativamente o desempenho em relação a modelos de rede neural recorrente (RNN) e de rede neural convolucional (CNN) anteriores. Os Transformers podem ser configurados como modelos de codificador ou de decodificador, suportando tanto tarefas gerativas quanto não gerativas. Modelos de codificador, como o BERT [7], se concentram em entender e codificar texto de entrada em uma representação densa. Modelos de decodificador, como o GPT-3 [3], geram texto com base em uma sequência de entrada. Modelos de codificador-decodificador, como o Transformer original [26], combinam ambas as funções e são eficazes para tarefas como tradução e resumo. No contexto do aprendizado por transferência, é crucial distinguir entre modelos base e modelos específicos de tarefa. Modelos base, como o BERT e o GPT-3, são treinados em grandes corpora para aprender representações de linguagem gerais. Esses modelos podem então ser refinados em tarefas específicas, como análise de sentimento ou reconhecimento de entidade nomeada, para criar modelos específicos de tarefa que mantenham a compreensão de linguagem geral enquanto são otimizados para tarefas específicas. Treinar e refinar grandes modelos de linguagem apresentam desafios computacionais significativos. Métodos de fine-tuning eficientes em parâmetros, como a adaptação de baixa rank (LoRA) [11], introduzem matrizes ajustáveis de baixa rank em cada camada de um modelo treinado, reduzindo parâmetros ajustáveis enquanto mantém o desempenho e reduz os custos. Modelos como o LlaMA 2 [25] e o Mistral [13] são projetados para eficiência em treinamento e inferência. Os modelos LlaMA 2 e Mistral 7B oferecem desempenho superior e eficiência em recursos, com o LlaMA 2 se destacando em benchmarks e sendo acessível para várias aplicações devido ao seu tamanho menor.
A arquitetura do Mistral 7B utiliza técnicas de mistura especializadas para melhorar desempenho e escalabilidade, tornando modelos de linguagem de alta performance mais acessíveis e práticos para tarefas especializadas. Ao utilizar métodos de PEFT, é possível adaptar modelos grandes, como aqueles direcionados ao pt-BR, de forma mais eficiente, permitindo uma aplicação mais ampla apesar de restrições computacionais. Essas técnicas são críticas para o desenvolvimento de modelos especializados em ambientes com restrições de recursos, garantindo que avanços na NLP sejam acessíveis e aplicáveis em diferentes línguas e domínios. Embora a maioria dos modelos de linguagem grande tenha sido desenvolvida para o inglês, esforços significativos têm sido feitos para criar modelos para outras línguas. Por exemplo, o mBERT (BERT multilíngue) manipula 104 línguas treinando em um corpus multilíngue[27], e o XLM-R[6] estende essa ideia treinando em um grande conjunto de dados multilíngue. Existem iniciativas para desenvolver modelos para o pt-BR, como Sabiá[20], Sabiá-2[1] e Bode[9]. No entanto, relativamente poucos modelos existem para essa língua, apesar da grande população de falantes de português (praticamente 250 milhões de falantes). No campo médico, modelos como BioBERT[16] e ClinicalBERT[12] são fine-tunados em corpora específicas de domínio para capturar o vocabulário único e o contexto de textos médicos. O Med-BERT[21], treinado em registros eletrônicos de saúde (EHRs), prediz resultados de pacientes e ajuda na tomada de decisão clínica. O Meditron e o BioMistral mostraram ganhos significativos de desempenho ao estender o treinamento em corpora médicas curadas[4] e[15]. O GatorTronGPT, treinado desde o início usando 277 bilhões de palavras de texto misto clínico e inglês com uma arquitetura GPT-3, melhora a NLP biomédica para pesquisas médicas[19]. O OpenBioLLM-70B aproveita o modelo Meta-Llama-3-70B-Instruct para alcançar desempenho de ponta em várias tarefas biomédicas[2]. No contexto médico pt-BR, os dois principais recursos são o BioBERTpt[23] e o CardioBERTpt[22].
BioBERTpt é um LLM biomédico, fine-tunado a partir de BERT multilingue utilizando dados clínicos e biomédicos, apresentando resultados promissores em tarefas de texto clínico como Reconhecimento de Entidade Nomeada e Detecção de Negação. Enquanto CardioBERTpt foi fine-tunado em dados clínicos específicos de Cardiologia. O único modelo gerativo disponível para pt-BR é o GPT2-bio-pt[24], que foi treinado exclusivamente em dados biomédicos de artigos médicos. O modelo apresenta várias limitações, como ser baseado em um modelo treinado em texto automaticamente traduzido, ausência de conhecimento de narrativas clínicas e janela de contexto pequena (512 tokens).

3 MÉTODO
Este capítulo descreve nossa metodologia dividida em três etapas-chave: Aquisição de Dados, Arquitetura e Treinamento do Modelo, e Configuração Experimental. Cada subseção detalha os processos e técnicas empregados para construir e avaliar nossos modelos.

3.1 Aquisição de Dados
Nossa pesquisa combinou dados de três conjuntos de dados clínicos distintos, totalizando 2,4 GB de texto e 309 151 121 tokens. O primeiro conjunto veio dos mesmos fontes de dados utilizadas no projeto SemClinBr[18], que são compostos por 2 100 546 entradas de narrativas clínicas de múltiplos hospitais brasileiros. O conjunto de dados contém tipos de documentos diversificados (por exemplo, resumos de alta, notas ambulatoriais, notas de enfermagem) e especialidades médicas (por exemplo, cardiologia, nefrologia, endocrinologia). Os dados de prontuário eletrônico (EHR) utilizados no estudo foram desidentificados e aprovados pelo Comitê de Ética da PUCPR, certificado de apresentação para apreciação ética número 51376015.4.0000.0020. O conjunto de dados BRATECA[8] também foi coletado e é composto por 73 040 notas de internação de 10 hospitais brasileiros e associados a múltiplos departamentos médicos (por exemplo, obstetrícia, cirurgia, emergência, COVID-19, cuidados intensivos, ambulatorial). Todos os dados foram anonimizados e obtiveram aprovação ética do Comitê Nacional de Ética em Pesquisa sob o número 46652521.9.0000.5530.
O conjunto de dados está disponível sob o uso de dados de saúde credenciados pela PhysioNet [10]. Finalmente, utilizamos os dados utilizados no trabalho de Lopes et al. [17], que consistem principalmente em casos clínicos de neurologia coletados de jornais médicos escritos em LLMs Clinica para português brasileiro e português europeu. O conjunto de dados contém 3678 textos médicos e está disponível ao público [2]. 3.2 Arquitetura e Treinamento do Modelo Nesse estudo, fine-tunamos os modelos base LlaMA-2 e Mistral utilizando o framework LlaMA-Factory [30], focando nos modelos de 7 bilhões de parâmetros (7B) para minimizar os recursos computacionais. Os modelos foram treinados por 2 épocas com uma taxa de aprendizado de 2e-5, usando Adaptation de Baixo-Ranque (LoRA) para atualizar eficientemente os pesos. A LoRA introduz matrizes de baixo ranque treináveis em cada camada, reduzindo os parâmetros treináveis e as necessidades de memória do GPU. Para otimizar ainda mais a memória e o uso computacional, aplicamos LoRA com precisão de 16 bits nas projeções q_proj e v_proj, configurando LoRA R para 8, LoRA Alpha para 16 e LoRA Dropout para 0,1. Utilizamos o otimizador AdamW com configurações β1 = 0,9 e β2 = 0,999 para balancear a convergência rápida com a estabilidade durante o treinamento. A principal diferença entre os dois modelos fine-tunados foi a configuração do parâmetro max_position_embeddings: 4096 para LlaMA-2 e 32768 para Mistral. O treinamento foi realizado na Plataforma de Nuvem da Google (GCP) com um GPU NVIDIA Tesla A100, 12 vCPUs e 85 GB de RAM. Os modelos resultantes, Clinical-BR-LlaMA-2-7B e Clinical-BR-Mistral-7B-v0.2, tiveram seu Perda de Treinamento exibido na Figura 1. Utilizamos 2,4 GB de dados clínicos para este estudo. Treinar o modelo Clinical-BR-LlaMA-2-7B envolveu 309.151.121 tokens e levou 47 horas e 56 minutos, custando R$ 1250,94. O modelo Clinical-BR-Mistral-7B-v0.2 levou 50 horas e 18 minutos, custando R$ 1202,73. Fig. 1. Perda de Treinamento dos Modelos 2 https://github.com/fabioacl/PortugueseClinicalNER 6 Pinto et al. 3.
3 Configuração experimental Os modelos treinados nesse trabalho são modelos base, treinados sem supervisão em um corpus de texto para aprender representações de linguagem clínica. Eles servem como fundações para fins-tunagem mais específica. A avaliação de modelos base geralmente envolve benchmarking para tarefas como compreensão de linguagem, geração de texto e resposta a perguntas. No entanto, esses benchmarking podem ser imprecisos, muitas vezes dependendo de métricas padrão que podem não capturar as nuances da linguagem ou generalização a dados não vistos. Eles podem não refletir o desempenho real no mundo, especialmente em campos especializados como saúde, onde conhecimento específico do domínio é crucial. Por exemplo, a maioria dos LLMs médicos é avaliada em benchmarks de resposta a perguntas em inglês como MedQA[14], que não medem a habilidade do modelo para interpretar dados clínicos em prontuários eletrônicos de saúde, que é o foco desse trabalho. Para superar limitações, usamos os modelos para gerar texto clínico sintético e avaliamos seu desempenho em três critérios usando uma escala de 5 pontos de Likert (Figura 2): Autenticidade de Formato e Estrutura, Precisão de Ortografia e Coerência Clínica. Fig. 2. Exemplo de cartão de avaliação. Os avaliadores tiveram que preencher as pontuações para cada critério, com base na saída de cada modelo Critério 1 - Autenticidade de Formato e Estrutura: Avaliou se a saída do modelo atendia às normas de documentos clínicos, incluindo layout e seção. A pontuação mais alta foi dada a modelos que perfeitamente corresponderam ao padrão. Critério 2 - Precisão de Ortografia: Focou-se na correção linguística, incluindo ortografia e termos médicos em pt-BR. Os modelos perderam pontos por gramática incorreta ou geração de texto em um idioma diferente. Critério 3 - Coerência Clínica: Avaliou se o conteúdo correlacionava logicamente com a história clínica, enfatizando relevância e precisão. Esse critério envolveu mais subjetividade devido a backgrounds de avaliadores variados.
Clinicos LLMs para o Português Brasileiro

Selecionamos 100 notas clínicas aleatórias, não pertencentes ao corpus de treinamento, e usamos trechos como entrada para cinco modelos para completar as notas. Isso teve como objetivo medir a habilidade dos modelos em produzir notas clínicas consistentes. Estudantes de medicina com experiência em notas clínicas avaliaram os textos gerados pelos nossos modelos (Clinical-BR-LlaMA-2-7B, Clinical-BR-Mistral-7B-v0.2) e três modelos de referência (LlaMA-2-7B, Mistral-7B-v0.2, Sabia-7B). Eles atribuíram escores de 1 a 5 em uma escala de Likert para cada um dos três critérios. A avaliação foi cega; os estudantes não sabiam qual modelo gerou cada nota. LlaMA-2-7B e Mistral-7B-v0.2 serviram como benchmarks para medir melhorias da nossa treinamento. Sabia-7B, um modelo treinado em português, foi incluído para comparação. Essa seleção permitiu uma análise de desempenho minuciosa em relação aos benchmarks. O GPT2-bio-pt foi excluído devido ao seu tamanho de contexto limitado, o que é inadequado para geração de texto clínico.

4 RESULTADOS

Essa seção apresenta os resultados da avaliação dos modelos Clinical-BR-LlaMA-2-7B e Clinical-BR-Mistral-7B-v0.2, ao lado de três benchmarks: LlaMA-2-7B, Mistral-7B-v0.2 e Sabia-7B. A avaliação se concentrou na Autenticidade de Formato e Estrutura, Precisão de Ortografia e Coerência Clínica, usando uma escala de Likert de 5 pontos. Analisamos as médias de escores para cada critério, a frequência com que cada critério foi atendido e realizamos uma análise de erros. Isso nos ajuda a entender as fortes e fracas características de cada modelo, a destacar melhorias nos nossos modelos fine-tuned e a identificar áreas para melhorias futuras.

4.1 Geração de Texto Clínico

A média de escores dos modelos para cada critério está apresentada na Tabela 1 e empilhada na Figura 3. Além disso, a Tabela 2 apresenta as frequências de escores para cada critério na avaliação de cada modelo.
Tabela 3 apresenta alguns exemplos de notas clínicas geradas com nossos modelos que obtiveram sucesso em todos os três critérios de avaliação (escores maiores que 4 em todos os critérios), ou seja, são textos coesos em relação à estrutura, ortografia e coerência clínica. Autenticidade de Formato e Estrutura: os modelos foram avaliados em sua adesão ao formato e estrutura típicos de documentos clínicos. Sabia-7B e Mistral-7B-v0.2 obtiveram escores de 3,84 e 3,85, respectivamente, indicando adesão moderada. LlaMA-2-7B demonstrou melhor consistência com um escore de 4,45. Nossos modelos, Clinical-BR-LlaMA-2-7B e Clinical-BR-Mistral-2-7B, alcançaram os escores mais altos de 4,60 e 4,62, respectivamente. Esses modelos também tiveram um número significativo de escores perfeitos (5), refletindo seu desempenho superior em manter a autenticidade do documento. 8 Pinto et al. Precisão Ortográfica: apesar do modelo Sabia-7B não ter alcançado média de escore acima de 4 nos Critérios 1 e 3, demonstrou excelente desempenho em correção ortográfica, provavelmente devido ao seu treinamento em um grande corpus de dados pt-BR. Em contraste, o modelo Mistral-7B-v0.2 apresentou dificuldades significativas, frequentemente gerando textos ou palavras aleatórias em inglês. LlaMA-2-7B, embora não treinado com um grande corpus português, conseguiu realizar bem nesse critério. Como indicado pela gráfica, nosso modelo ajustado LlaMA (ou seja, Clinical-BR-LlaMA-2-7B) melhorou ainda mais o desempenho já bom do LlaMA-2-7B. O modelo Mistral-7B-v0.2 apresentou um aumento notável, com seu modelo base obtendo média de escore de 3,82 e alcançando um aumento para 4,69, incluindo 30 escores perfeitos adicionais (5) na distribuição de frequência, indicando excelente desempenho. Tabela 1. Escore médio dos modelos nos três critérios avaliados. Em negrito os modelos com o melhor escore para cada critério. Critérios Mistral- 7B-v0.2 Sabia-7B LlaMA-2- 7B Clinical- BR-LlaMA- 2-7B Clinical- BR-Mistral- 7B-v0.2 Autenticidade de Formato e Estrutura **4,62** 3,84 4,45
45 4,62 4,60 Precisão Ortográfica 3,82 4,56 4,39 4,59 4,69 Coerência Clínica 3,52 3,54 4,19 4,45 4,46 Fig. 3. Média ponderada de pontuação dos modelos em relação aos três critérios avaliados LLMs para o Português Brasileiro 9 Tabela 2. Frequências de pontuação para cada modelo avaliado. C1, C2 e C3 representam os critérios 1, 2 e 3, respectivamente. LlaMA-2-7B Clinical-BR-LlaMA-2-7B Pontuação C1 C2 C3 Pontuação C1 C2 C3 1 3 2 2 1 1 0 1 2 7 7 8 2 3 0 0 3 5 4 14 3 4 4 17 4 19 29 29 4 21 35 22 5 72 64 53 5 77 67 66 Mistral-7B-v0.2 Clinical-BR-Mistral-2-7B Pontuação C1 C2 C3 Pontuação C1 C2 C3 1 8 6 9 1 1 0 1 2 12 20 12 2 4 1 1 3 14 6 28 3 4 1 10 4 26 29 28 4 20 29 31 5 46 45 29 5 77 75 63 Sabia-7B Pontuação C1 C2 C3 1 12 2 11 2 10 3 12 3 13 5 24 4 21 23 29 5 50 73 30 Tabela 3. Exemplos de notas clínicas geradas com sucesso # Modelo Contexto de entrada Saída 1 Clinical-BR- LlaMA-2-7B O paciente expressa preocupações com a possibilidade de complicações metabólicas decorrentes da SOP como diabetes tipo 2 e dislipidemia. Os exames de glicose em jejum e perfil lipídico apresentam alterações indicativas de resistência à insulina e dislipidemia. O tratamento proposto envolve a abordagem multifacetada da SOP. O paciente expressou preocupações com a possibilidade de complicações metabólicas decorrentes do tratamento que serão abordadas com o paciente. Ao final do tratamento, o paciente será acompanhado com o objetivo de monitorar o estado de saúde e retornar para consulta se necessário 2 Clinical-BR- Mistral-2-7B Paciente tabagista crônico (20 anos de tabagismo) hipertenso há 10 anos com histórico de angina estável e episódios de dor torácica recorrentes nos últimos 6 meses. Realizou angiografia coronária em 2007 com diagnóstico de estenose de 70% na origem da artéria esquerda e de 90% na artéria circunflexa. Em 2008 realizou angioplastia com implante de stent em ambas artérias. Coerência Clínica: em termos de Coerência Clínica, Sabia-7B pontuou 3,54 e Mistral-7B-v0.2 pontuou 3,52.
52, ambos lutando para manter uma correlação lógica dentro do contexto clínico. O modelo LlaMA-2-7B pontuou 4,19, indicando um desempenho melhor, embora não ótimo. Os modelos Clinical-BR-LlaMA-2-7B e Clinical-BR-Mistral-2-7B novamente lideraram com pontuações de 4,45 e 4,46, respectivamente, demonstrando sua capacidade de gerar conteúdo clínico coerente. Análise de frequência revelou que esses modelos tinham o maior número de pontuações perfeitas (5), sublinhando sua superioridade em coerência.

4.2 Análise de Erros

Nesta seção, realizamos uma análise de erros no task de geração de textos clínicos, tanto dos modelos treinados quanto dos modelos de base original. A Tabela 4 apresenta alguns exemplos de textos clínicos gerados que obtiveram pontuações baixas para um ou mais critérios de avaliação. Todos os modelos avaliados demonstraram um comportamento em que repetem certas palavras ou sequências múltiplas vezes consecutivas, como no erro #2. Este problema pode ser provavelmente resolvido ajustando alguns parâmetros de inferência. Por exemplo, ao aplicar a Temperatura = 0,1 e Pena de Repetição = 1,2, a saída para o erro #2 seria “por sensação de pressão no peito. A doença foi diagnosticada há mais de um ano atrás, mas o paciente não tomou nenhuma medicação para tratamento. O exame clínico revelou: pressão arterial sistólica de 140 mmHg; frequência cardíaca regular de 76 bpm; pulso normal; auscultação cardiovascular sem alterações. . . ”.

Embora os textos clínicos sejam conhecidos por não ter uma estrutura formal padrão, pois cada médico ou instituição pode estabelecer um formato diferente, esperamos que um relatório clínico contenha, pelo menos, uma descrição parcial da condição do paciente ou da visita, seja em forma narrativa ou semi-estruturada. Portanto, alguns relatórios gerados continham uma saída mais semelhante a uma pergunta de teste ou documentação de ensino do que um relatório narrativo clínico (como mostrado no erro #1).
Este foi um comportamento mais frequente do modelo Sabia-7B, provavelmente devido à sua base de dados relacionada a provas de ensino. Quanto aos problemas na métrica de ortografia, a maioria dos problemas veio do gerar uma saída em inglês em vez de português (como no erro #3), sendo um problema mais recorrente no modelo Mistral-7B-v0.2. Em relação à dimensão de coerência clínica, a mais complexa para os modelos alcançar pontuações máximas, a maioria dos problemas está associada ao uso de termos médicos que faria sentido estar na seção do texto, mas não para a condição do paciente. Por exemplo, o erro #4 apresenta um texto que sugere o uso de atenolol, um medicamento amplamente utilizado para tratar a hipertensão, para o tratamento da dislipidemia. Isso é um problema esperado, dado que o comportamento padrão dos modelos baseados em representações vetoriais interpreta todos os termos relacionados a medicamentos como semelhantes, pois sempre são usados em contextos semelhantes.

Também tivemos problemas de coerência clínica, em que a saída gerada utiliza palavras-chave relacionadas ao contexto do paciente, mas a construção de sentenças com essas palavras não faz sentido, como no erro #5, onde a palavra "diálise" faz sentido, pois o paciente tem doença renal crônica, mas a sentença apresentada não faz sentido do ponto de vista clínico. LLMs clínicos para o português brasileiro 11 Tabela 4. Exemplos de notas clínicas mal geradas # Modelo Contexto de entrada Saída 1 Sabia-7B Paciente com histórico de hipertensão, tabagismo, cardiopatia isquêmica com infarto agudo do miocárdio em 2000 que já havia recebido implante de stent na artéria descendente anterior proximal e média (ACTP primária em 2000) retorna para avaliação de dor precordial em repouso. Qual a conduta a ser tomada? a) Realizar cateterismo cardíaco de urgência. b) Realizar cateterismo cardíaco eletivo. c) Realizar estudo hemodinâmico. d) Realizar estudo eletrofisiológico. e) Realizar estudo ecocardiográfico.
Resolve Comentários (0) 2 Mistral-7B-v0.2 # HAS # IRC em HD desde janeiro/05 # prostatectomia em 1993 # RTU em 2003 # úlcera péptica em 2003 # FA com MP (Bradarritmia sintomática) desde 1998 # CRM em 2002 # angina instável e CAT em 2003 # fratura colo fêmur em 2005 Chega à EME-HCPA queixando-se de dor precordial de forte intensidade próxima à região da fúrcula esternal acompanhada de dor no peito, dor no esterno, dor no epigástrico, dor no abdômen, dor na costela, dor no flanco, dor no quadril, dor no perineu, dor no pé, dor no braço, dor no pulso, dor no dedo, dor no joelho, dor no tornozelo, dor no pé, dor no tornozelo, dor no joelho, dor no tornozelo.

3 Mistral-7B-v0.2 Diluição: Extraia 40ml do medicamento e misture com 80ml de Solução de Cloreto de Sódio a 0,9% em um frasco de 100ml. Administre 120ml por infusão intravenosa. Estabilidade: 24 horas a 25°C.

# Description # O que é esse medicamento? DIL-TIAZEM (dil TIaze-m) é um medicamento que contém tiazida, um tipo de diurético.

paciente expressa preocupações com a possibilidade de complicações metabólicas decorrentes da SOP como diabetes tipo 2 e dislipidemia. Os exames de glicose em jejum e perfil lipídico apresentam alterações indicativas de resistência à insulina e dislipidemia. O tratamento proposto envolve a abordagem multifacetada da SOP. Para o tratamento da dislipidemia foi iniciado a atenolol 100 mg/dia e a simvastatina 20 mg/dia. Para o tratamento da resistência à insulina foi iniciado a metformina 500 mg/dia.

5 Clinical-BR- LlaMA-2-7B Paciente com histórico de doença renal crônica (DRC) de estágio 3 diagnosticado há três anos durante um exame de rotina. Apresenta proteinúria persistente com níveis de creatinina sérica elevados e clearance de creatinina estimada em 45 ml/min/1.73m². Apresenta história de frequência de uso de frascos de diálise com uso de frascos de diálise com alta e baixa qualidade.
5 DISCUSSÃO E TRABALHO FUTURO Nesse estudo, desenvolvemos e avaliamos dois modelos de linguagem médica grandes, Clinical-BR-LlaMA-2-7B e Clinical-BR-Mistral-7B-v0.2, adaptados para pt-BR. Nossos achados mostraram melhorias significativas na geração de texto clínico sintético, particularmente em estrutura, precisão ortográfica e coerência clínica. Essas melhorias são devido ao treinamento e ajuste alvo em narrativas clínicas usando Adaptação de Rango Baixo (LoRA) para adaptação eficiente. Os modelos Clinical-BR-LlaMA-2-7B e Clinical-BR-Mistral-7B-v0.2 produziram saídas que se adequarem às normas de documentos clínicos, mantiveram alta precisão de ortografia e geraram conteúdo clínico coerente. O modelo Clinical-BR-Mistral-7B-v0.2 lidou melhor com contextos longos devido à janela de contexto mais larga, um atributo crítico para documentos clínicos longos. É importante notar que o modelo Mistral-7B-v0.2 performou pior que o LlaMA-2-7B em todos os critérios de avaliação, possivelmente devido ao baixo volume de dados pt-BR na treinamento (essa informação não foi fornecida pelo time que desenvolveu o modelo). No entanto, após nosso treinamento em dados clínicos, o modelo Clinical-BR-Mistral-7B-v0.2 performou melhor ou similarmente ao modelo Clinical-BR-LlaMA-2-7B. Isso pode indicar que os detalhes arquiteturais dos modelos baseados em Mistral são mais adequados para lidar com ajustes a novos contextos em um cenário de baixo volume de dados e com o uso de técnicas PEFT. Nossa obra se concentra em modelos de linguagem médica para pt-BR, abordando desafios únicos em comparação com modelos treinados em outras línguas. Essa distinção é crítica, pois as terminologias linguísticas e clínicas do pt-BR diferem significativamente, afetando a generalização do modelo. Os modelos BioMistral e MediTron, treinados em corpora médicas extensas como PubMed Central, mostram ganhos de desempenho em tarefas de QA médica em inglês. No entanto, nossos modelos pt-BR enfrentam recursos menores e avaliações menos padronizadas, complicando comparações diretas de desempenho.
O texto traduzido é:

GatorTronGPT e OpenBioLLM-70B lidam com extensas informações clínicas em inglês, demonstrando melhorias em tarefas de NLP biomédicas. Em contraste, nosso abordagem otimiza recursos limitados enquanto mantém desempenho alto em narrativas clínicas em pt-BR, enfatizando eficiência computacional com técnicas como LoRA. Este aspecto é crucial para tornar LLMs avançados acessíveis às comunidades de pesquisa com infraestrutura computacional limitada. Os protocolos de avaliação frequentemente envolvem benchmarking bem definidos em inglês. A avaliação do BioMis- tral inclui tarefas de QA médica multilíngue, refletindo suas capacidades. Nossos métricas de avaliação são adaptadas ao pt-BR, garantindo relevância, mas limitando comparações diretas com modelos centrados em inglês. Outra limitação no protocolo de avaliação do nosso projeto é que o uso de estudantes de medicina pode não ter sido uma solução ótima, especialmente ao avaliar a coesão clínica das notas, pois os estudantes têm limitada expertise sobre algumas condições e tratamentos. Além disso, a extensão das notas clínicas, um fator crítico na geração de dados de saúde sintéticos, não foi incluída na avaliação de estrutura ou coesão. O objetivo primário foi identificar o impacto dos dados de treinamento clínico em pt-BR no desempenho do modelo, mesmo em volumes pequenos, em comparação com modelos existentes. Consequentemente, nossos modelos não são recomendados para gerar dados de saúde sintéticos plenamente coerentes. Embora os resultados sejam promissores em pt-BR, não consideramos a extensão ou completude das notas, o que levou os modelos a gerar, às vezes, apenas algumas palavras após a entrada de dados. Além disso, o uso de outras notas clínicas como referências de entrada pode complicar a geração de dados de saúde sintéticos em ambientes com dados clínicos reais limitados. Embora nossos modelos apresentem resultados promissores em pt-BR, o futuro trabalho deve se esforçar para fine-tuning e avaliar os modelos em tarefas downstream clínicas e alinhar protocolos de avaliação mais estreitamente com benchmarking internacional para facilitar comparações mais precisas.
Além de construir benchmarks mais específicos para o contexto de textos clínicos, que têm um formato e estrutura muito peculiares e diferem significativamente de artigos científicos ou provas de licenciamento médico. Além disso, ampliar nosso conjunto de dados e incorporar narrativas clínicas mais diversificadas pode melhorar a robustez e generalização do modelo. LLMs clínicos para o português brasileiro 13 Treinar um modelo com dados de prontuários médicos tende a realizar melhor em tarefas relacionadas à extração de dados, interpretação e geração nesse contexto. No entanto, compreendemos que várias tarefas no campo médico envolvem razão médica e que grande parte do conhecimento necessário para que o modelo realize essa tipo de inferência é encontrada em textos biomédicos e conjuntos de dados de resposta a perguntas específicas. Nesse contexto, pretendemos expandir nossos modelos para o contexto biomédico também, de forma que eles sejam capazes de lidar bem com todos os tipos de dados médicos. Ao abordar essas diferenças, destacamos as contribuições únicas do nosso trabalho e estabelecemos o palco para avanços futuros no desenvolvimento e avaliação de LLMs médicas em linguagens e configurações de recursos diversificadas.

CONCLUSÃO Este estudo se concentrou em desenvolver e avaliar dois modelos de linguagem grande médica, Clinical-BR-LlaMA-2-7B e Clinical-BR-Mistral-7B-v0.2, projetados para o contexto médico do português brasileiro. Esses modelos demonstraram melhorias significativas em relação aos modelos de base em geração de texto clínico sintético, particularmente em termos de Autenticidade de Formato e Estrutura, Precisão de Ortografia e Coerência Clínica, em grande medida devido ao uso eficiente da técnica LoRA e ao treinamento contínuo em um conjunto de dados de notas clínicas. O desempenho dos modelos destaca o potencial de integrar LLMs médicas eficientes em recursos em prática clínica, facilitando a geração de notas clínicas e apoio à pesquisa médica em contextos de língua portuguesa.
No entanto, limitações como a subjetividade do protocolo de avaliação, a especialização dos avaliadores e o escopo do conjunto de dados destacam áreas para melhorias futuras. A pesquisa futura deve se esforçar para fine-tuning dos modelos para tarefas específicas, alinhar protocolos de avaliação com padrões internacionais, construir benchmarks específicos para textos clínicos e expandir conjuntos de dados para incluir narrativas clínicas e textos biomédicos diversificados. Esses esforços ajudarão a melhorar a robustez e a generalização dos modelos, permitindo que eles realizem melhor em várias tarefas médicas. Nossa obra destaca a viabilidade de criar LLMs médicos eficientes em recursos para pt-BR, abrindo caminho para sua adoção mais ampla na saúde. Continuadas melhorias e expansões contribuirão para soluções de saúde mais precisas e eficientes, beneficiando tanto pacientes quanto profissionais de saúde. Agradecimentos. Gostaríamos de agradecer ao Comsentimento e ao Programa Cloud do Google para Startups por fornecer infraestrutura e suporte para desenvolver este projeto. Reconhecemos o uso de ferramentas de Inteligência Artificial Gerativa, particularmente o ChatGPT, na preparação deste artigo. O ChatGPT foi empregado para tarefas como revisão de texto, resumo de parágrafo e melhoramento da clareza e coerência do manuscrito. Declaração de Interesses. Os autores não têm interesses competitivos para declarar que sejam relevantes para o conteúdo deste artigo.