Avaliando Métodos de Quantificação de Sentimento em Corpora do Português Brasileiro

Lucas Nildaimon dos Santos Silva1[0000−0002−5606−9416], Diego Furtado Silva2[0000−0002−5184−9413] e Helena de Medeiros Caseli1[0000−0003−3996−8599]

1 Programa de Pós-Graduação em Ciência da Computação (PPGCC), Departamento de Computação, Universidade Federal de São Carlos (UFSCar)
2 Instituto de Matemática e Estatística (IME), Universidade de São Paulo (USP)
lucas.silva@estudante.ufscar.br, diegofsilva@usp.br, helenacaseli@ufscar.br

Resumo. Este artigo avalia métodos de quantificação de sentimento aplicados a corpora do português brasileiro. A quantificação de sentimento, distinta da classificação de sentimento, estima a distribuição de classes de sentimento (positivo e negativo) dentro de um conjunto de dados. Investigamos várias técnicas de quantificação, incluindo a família Classify and Count (CC) e métodos mais sofisticados, como Estimação de Densidade por Kernel (KDE) e Similaridade de Distribuição (DyS). Nossa análise utiliza cinco conjuntos de dados, cada contendo distribuições diferentes de classes de sentimento. Nossos resultados experimentais indicam que os métodos KDE e DyS consistentemente superam os outros, alcançando as melhores médias de ranking em termos de precisão de quantificação. Testes estatísticos, incluindo os testes de Friedman e Nemenyi, confirmam diferenças significativas de desempenho entre os métodos, com KDE e DyS mostrando melhoras estatisticamente significativas em relação ao método de base CC. Esses achados destacam a importância de escolher técnicas de quantificação robustas para uma quantificação de sentimento precisa em corpora em diferentes domínios.

Palavras-chave: Análise de Sentimento · Quantificação · Mudança de Distribuição · Português Brasileiro.

1 Introdução Entender o sentimento geral em relação a uma entidade é crucial para empresas, governos, figuras públicas e organizações. No mundo atualmente baseado em dados, medir corretamente a opinião pública ajuda a tomar decisões informadas, planejar ações estratégicas e melhorar as relações públicas.
Com a quantidade vasta de dados gerados por plataformas de mídias sociais, revisões e outras plataformas digitais, quantificar sentimentos tornou-se essencial. A quantificação de sentimentos eficaz permite avaliar o sentimento geral, identificar tendências, monitorar mudanças na percepção pública ao longo do tempo e responder de forma proativa a potenciais problemas. Saber a opinião pública em forma resumida melhora a eficácia dessas ações, pois analisar cada opinião ou comentário individualmente é frequentemente impraticável e ineficiente.

A quantificação de sentimentos consiste em estimar a prevalência relativa (ou distribuição) de diferentes classes de sentimentos (como positivo, neutro e negativo) dentro de uma amostra de textos não rotulados [8, 22]. O procedimento usual para quantificar as classes de um conjunto de dados depende de ajustar as previsões feitas por um modelo induzido por aprendizado supervisionado. Em contraste com a classificação de sentimentos (polaridade), a quantificação de sentimentos busca entender a distribuição geral de sentimentos dentro de um conjunto de dados. A classificação se concentra em análise item-level, enquanto a quantificação fornece uma visão geral das tendências de sentimentos. Por exemplo, em vez de identificar cada revisão de produto como positiva, negativa ou neutra, a quantificação de sentimentos visa estimar a taxa de revisões que caem em cada classe dentro do conjunto de dados usando métodos de quantificação.
Ilustração de uma tarefa de quantificação versus uma tarefa de classificação com duas classes. Essa distinção é fundamental para aplicações em que a partilha de sentimentos global é mais informativa do que o sentimento de instâncias específicas, como análise de mercado, monitoramento de sentimentos políticos e análise de feedback de clientes em larga escala [5]. A abordagem mais direta para a quantificação é a estratégia Classify and Count (CC) [10], que envolve treinar um classificador para rotular cada instância e então contar essas rotulações para estimar a prevalência da classe. No entanto, a maioria dos classificadores baseados em aprendizado supervisionado assume que a distribuição das classes no conjunto de treinamento é idêntica àquela no conjunto de teste, uma suposição conhecida como a suposição IID (Independent and Identically Distributed) [21]. No entanto, em cenários reais, essa suposição frequentemente não é válida, levando a estimativas inacuradas da prevalência da classe quando há mudanças na distribuição de dados, também conhecidas como mudanças de probabilidade anterior. Portanto, há significativos viéses na estimativa da prevalência da classe quando se utiliza a estratégia CC [22, 19]. Essas limitações da estratégia CC levaram ao desenvolvimento de métodos de quantificação que consideram mudanças de probabilidade anterior, como Adjusted Classify and Count, Saerens-Latinne-Decaestecker e métodos baseados em ajuste de distribuição, que são investigados nesse artigo. Métodos de quantificação são importantes porque abordam as limitações dos métodos de classificação tradicionais quando lidam com mudanças na distribuição de dados, como mudanças na distribuição de classes [5]. Em muitas aplicações reais, a distribuição de classes no conjunto de treinamento pode não refletir precisamente a distribuição em dados não rotulados.
Ao se concentrar na estimativa de distribuições de classes em vez de classificar instâncias individuais, a quantificação fornece uma abordagem mais robusta e precisa para análise de dados em cenários caracterizados por mudanças distribucionais [22, 12]. Este trabalho apresenta uma avaliação comparativa de métodos de quantificação no contexto de análise de sentimento em textos em português brasileiro. O objetivo é investigar a eficácia de vários métodos de quantificação em estimar a distribuição de sentimento em diferentes conjuntos de dados, incluindo tweets e resenhas de produtos. As principais contribuições do nosso trabalho são: – Fornece uma avaliação comparativa de 11 algoritmos de quantificação no contexto de análise de sentimento em textos em português brasileiro, abrangendo conjuntos de dados diversificados, incluindo tweets e resenhas de produtos. – Investigamos sistematicamente a robustez e desempenho desses métodos de quantificação sob diferentes tamanhos de teste e graus de mudança na prevalência de sentimento. – Oferece insights sobre as fortes e fracas características de diferentes métodos de quantificação, especialmente destacando sua eficácia em abordar mudanças na probabilidade a priori na quantificação de sentimento.

2 Trabalhos Relacionados Os trabalhos relacionados sobre métodos de quantificação, mudanças de conjunto de dados e suas aplicações abrangem vários aspectos desses tópicos, incluindo propostas de algoritmos, definição de configurações experimentais e metodologias de avaliação [9, 10, 8, 11, 16, 17, 26, 21, 22, 4, 12]. Esta seção revisa contribuições e métodos significativos de vários estudos-chave. A Quantificação de Sentimento (SQ) foi inicialmente introduzida por [8]. Os autores criticaram a omissão em decidir se a análise de sentimento em grandes coleções de texto deve se concentrar em níveis individuais ou agregados. Eles também enfatizam a importância de distinguir entre a classificação de sentimento e a SQ, reconhecendo-as como aplicações separadas com características distintas.
Como consequência, cada uma dessas tarefas requer abordagens específicas. Os autores sustentam que supor que um melhoramento na precisão de um classificador no nível individual necessariamente levaria a uma precisão mais alta no nível agregado não é válido. Eles ilustram esse ponto destacando como o Score F1, uma medida amplamente utilizada para a avaliação de classificação, pode ser enganosa. Enquanto um modelo pode alcançar um Score F1 superior comparado a outro classificador, pode alcançar desempenho de quantificação inferior. Para ilustrar, em uma tarefa de classificação binária, se o classificador C1 tem 20 erros (10 falsos positivos e 10 falsos negativos) e o classificador C2 tem 10 erros (8 falsos positivos e 2 falsos negativos), C1 seria considerado pior como classificador. No entanto, C1 se revela um melhor quantificador do que C2 porque os falsos positivos e os falsos negativos estão balanceados, compensando-se mutuamente em termos de estimativa da distribuição de classes. O método CC exemplifica esse princípio em sua abordagem. Uma avaliação empírica abrangente de métodos de quantificação é apresentada em [26]. Este estudo avalia 24 métodos de quantificação em mais de 40 conjuntos de dados, dos quais 3 são dados de texto, abrangendo configurações binárias e multiclass. Em ambos os cenários binário e multiclass, o estudo examinou divisões com proporções variáveis de amostras de treinamento e testes. Essa abordagem simulou cenários com informações limitadas e abundantes disponíveis para treinar os modelos. Os resultados revelam que nenhum algoritmo singularmente supera outros em todas as configurações. No entanto, um grupo de métodos, incluindo os métodos de seleção de limiar Median Sweep e TSMax, o framework DyS e o método de Friedman, exibiu o melhor desempenho no cenário binário.
Os melhores desempenhos no cenário multiclass foram alcançados pelo método de Contagem Probabilística Ajustada Geralizada, o método readme, o método de minimização da distância de energia, o algoritmo EM para quantificação e o método de Friedman. O estudo revela que o cenário multiclass apresenta um desafio consideravelmente maior para os métodos de quantificação estabelecidos, como demonstrado por escores de erro consistentemente muito mais altos do que na caso binário. Além disso, é mostrado que os algoritmos que seguem o princípio de classificar e contar, mesmo quando otimizados para quantificação, tendem a realizar pior, em média, em comparação com outros métodos especializados. O estudo de [12] examina o desempenho dos métodos de quantificação existentes em diferentes tipos de mudanças nos conjuntos de dados, pois a maioria da pesquisa anterior se concentrou principalmente em mudanças de probabilidade anterior. Os autores propõem uma taxonomia de tipos de mudanças nos conjuntos de dados e introduzem protocolos experimentais para simular essas mudanças. Ao testar os métodos de quantificação existentes em conjuntos de dados gerados usando esses protocolos, o artigo busca identificar as fortes e fracas características desses métodos em diferentes condições. Um achado chave é que enquanto muitos métodos de quantificação são robustos a mudanças de probabilidade anterior, eles lutam com outros tipos de mudanças nos conjuntos de dados. Os autores introduzem novos protocolos de avaliação para simular diferentes mudanças nos conjuntos de dados e testam vários métodos de quantificação nessas condições. Além disso, revelam que métodos como PCC são eficazes apenas em mudanças puras de covariáveis, enquanto SLD e PACC realizam melhor quando as mudanças de covariáveis são acompanhadas por mudanças na prioridade das classes. No entanto, todos os métodos, incluindo o SLD, mostram instabilidade em mudanças locais de covariáveis e limitações significativas em lidar com mudanças de conceito. O estudo destaca a necessidade de métodos de quantificação mais eficazes capazes de abordar diferentes tipos de mudanças nos conjuntos de dados.
A importância do tamanho do conjunto de teste na pesquisa de quantificação também foi destacada por estudos recentes. Segundo [17], o tamanho do conjunto de teste é um fator crucial, ainda pouco considerado, na pesquisa de quantificação. Através de análise empírica, os autores demonstram que o desempenho dos quantificadores flutua significativamente com diferentes tamanhos de conjuntos de teste e que os métodos atuais geralmente desempenham mal em conjuntos de teste menores. Para abordar isso, eles propõem um esquema de aprendizado meta que seleciona o melhor quantificador com base no tamanho do conjunto de teste. Os autores defendem que futuras pesquisas devem incorporar considerações sobre o tamanho do conjunto de teste ao avaliar novas propostas de quantificação. O artigo de [23] avalia diferentes métodos de quantificação aplicados a dados de sentimento de resenhas de produtos em inglês e explora como a quantificação pode melhorar a precisão da classificação de sentimento. Os autores utilizaram seis conjuntos de dados de resenhas de produtos, um modelo de linguagem treinado previamente (Twitter-roBERTa-base) e dez métodos de quantificação. O Protocolo de Prevalência Artificial (APP) foi empregado para gerar amostras de teste com distribuições de classes variáveis para avaliar o erro dos métodos de quantificação e seu impacto na ajuste dinâmico da borda de decisão para a classificação. Os resultados demonstraram que oito dos nove métodos de quantificação superaram significativamente o método CC em tarefas de quantificação. Além disso, o uso de métodos de quantificação para ajustar a borda de decisão melhorou significativamente a precisão da classificação em comparação ao método CC. Em contraste com o corpo de trabalho existente, nosso estudo se concentra especificamente na aplicação de métodos de quantificação de sentimento a textos em português brasileiro, um problema de pesquisa que recebeu menos atenção.
A seguir, a tradução do texto científico para português do Brasil:

Ao avaliar sistematicamente 11 algoritmos de quantificação em cinco conjuntos de dados de tweets e resenhas de produtos em português brasileiro, nosso estudo visa fornecer uma compreensão nuances da eficácia dessas metodologias nessa língua. 3 Metodologia Nesta seção, detalhamos a metodologia empregada para avaliar a eficácia de vários métodos de quantificação de sentimento em textos em português brasileiro. Descrevemos os conjuntos de dados utilizados e fornecemos um panorama dos métodos de quantificação testados. Além disso, explicamos o arranjo experimental, que inclui tamanhos de teste múltiplos, validação cruzada k-fold, um protocolo de avaliação específico para quantificação e o métrica utilizada para avaliar o desempenho de cada método. 3.1 Conjuntos de dados Utilizamos cinco conjuntos de dados com textos escritos em português brasileiro anotados com polaridade de sentimento para avaliar os métodos de quantificação. Os conjuntos de dados selecionados abrangem uma variedade de corpora utilizadas para análise de sentimento em português brasileiro. O conjunto de dados "Computer-BR" [18] consiste em tweets relacionados a computadores e notebooks com quatro classes possíveis: ironia, negativa, neutra e positiva. O conjunto de dados "Livros" [1] contém resenhas de livros anotadas manualmente para classes positiva e negativa. O conjunto de dados "Sentencas" [1] compreende sentenças manualmente rotuladas de revisões de produtos eletrônicos com classes negativa e positiva. O conjunto de dados "Eleicoes2018" [3] inclui tweets relacionados às eleições de 2018 no Brasil, anotados manualmente com polaridades positiva e negativa. O conjunto de dados "RePro" [25] consiste em resenhas de produtos anotadas por sentimento com quatro classes: positiva, negativa, neutra e uma classe ambígua "negativa/positiva". Este conjunto de dados também inclui anotações de tópicos de revisões, que desconsideramos aqui.
Nesse estudo, apenas consideramos tarefas binárias de quantificação de sentimento, excluindo amostras com rótulos diferentes das classes negativa e positiva dos corpora. As distribuições de classes variam entre conjuntos de dados, refletindo diferentes proporções de sentimentos positivos e negativos em cada corpus, como mostrado na Tabela 1. Conjunto de dados Número de Amostras Distribuição de Classes (Negativa×Positiva) Computador-BR 604 0,67 × 0,33 Livros 175 0,50 × 0,50 Eleições 2018 447 0,55 × 0,45 Sentenças 175 0,31 × 0,69 RePro 7576 0,46 × 0,54 Tabela 1: Resumo dos Conjuntos de Dados. 3.2 Métodos de Quantificação Este estudo se concentra em métodos de quantificação baseados em agregação, que estimam a prevalência de classes utilizando a saída de classificadores duros ou probabilísticos. O trabalho de [11] classificou os métodos de quantificação em três grupos: (i) Classificar, Contar e Corrigir, que envolve classificar instâncias e então corrigir contagens de classes; (ii) Adaptar algoritmos de classificação tradicionais para funcionar como quantificadores; e (iii) Ajuste de Distribuição, que modela a distribuição de treinamento e encontra o melhor ajuste contra o conjunto de teste. A abordagem mais básica é o método CC. Isso envolve treinar um classificador padrão para atribuir cada amostra de dados a uma classe específica e então contar as classes preditas para avaliar sua distribuição. No entanto, o CC frequentemente falha em estimar a prevalência de classes com precisão quando há mudanças na distribuição de dados, também conhecidas como mudanças de probabilidade anterior. Uma variante do CC é o Probabilistic Classify and Count (PCC) [2], que utiliza classificadores probabilísticos para estimar a prevalência de classes. Diferentemente do CC, que depende de classificações duras, o PCC considera as probabilidades posteriores atribuídas a cada classe pelo classificador, visando estimativas mais precisas de prevalência.
A Classificar e Contar Ajustada (ACC) [10] refinaria ainda mais o abordagem CC, corrigindo as contagens brutas com base na Taxa de Verdadeira Positiva (TPR) e na Taxa de Falsa Positiva (FPR) estimadas em um conjunto de validação, oferecendo uma estimativa mais balanceada. A Classificar e Contar Ajustada Probabilística (PACC) [2] estende a ACC ao integrar classificadores probabilísticos. Técnicas de seleção de limiar, como Sweep Médio (MS) [9, 10], MS2 [9, 10] e MAX [9, 10], aproveitam o método ACC para melhorar a estimativa de prevalência de classe, abordando preocupações de estabilidade que surgem quando TPR e FPR estão próximas, especialmente em cenários em que a classe positiva é rara. Essas técnicas empregam vários limiares para ajustar a fronteira de decisão do classificador para uma estimativa mais precisa de prevalência, cada uma seguindo estratégias tailandes para desempenho ótimo. O método MS calcula a média das estimativas de prevalência obtidas ao aplicar o método ACC em uma gama de limiares de classificação, usando validação cruzada para estimar TPR e FPR, seguida da correção ACC. MS2, uma variante mais rigorosa do MS, se concentra especificamente em limiares onde a diferença entre TPR e FPR ultrapassa 0,25, melhorando a precisão ao filtrar potenciais outliers. Por outro lado, o método MAX seleciona um limiar que maximize a diferença entre TPR e FPR, otimizando a fronteira de decisão do classificador dentro do framework ACC. Todos os métodos descritos anteriormente são exemplos da família Classificar, Contar e Corrigir. Uma adaptação de um algoritmo de classificação tradicional para funcionar como quantificador, SVM(MAE) [7, 22] é uma variante de Máquina de Vetor de Suporte projetada para minimizar a Erro Absoluto Médio (MAE) ao se concentrar explicitamente na medida de erro usada para avaliar a precisão de quantificação. Esse método garante que o algoritmo de aprendizado alvo e reduza a medida de erro especificada.
O SVM(MAE) é uma instância do framework SVMPerf [14], que pode produzir classificadores otimizados para funções de perda multivariadas. O algoritmo Saerens-Latinne-Decaestecker (SLDC) [24] também adapta um classificador ao empregar uma abordagem de expectação-maximização (EM) com um componente transutivo para refinar previsões de teste. Este processo iterativo atualiza estimativas de prevalência de classe e probabilidades posteriores, levando em conta dados rotulados e não rotulados. Inicialmente, o algoritmo utiliza um classificador treinado em dados rotulados para estimar probabilidades posteriores para o conjunto de teste. No passo E, essas probabilidades são utilizadas para atualizar as estimativas de prevalência de cada classe. Durante o passo M, as prevalências atualizadas refinam as probabilidades posteriores. Este processo repete-se até a convergência. Técnicas de ajuste de distribuição, como HDy [13], DyS [16] e KDEy-ML [20], operam modelando a distribuição do conjunto de treinamento e ajustando parâmetros para alinhar essa distribuição com a do conjunto de teste. O HDy utiliza a distância de Hellinger para comparar distribuições de escores de probabilidade entre os conjuntos de treinamento e teste. A distância de Hellinger mede a diferença entre duas distribuições de probabilidade, fornecendo um métrica robusta para comparar distribuições em um espaço de alta dimensionalidade. O HDy funciona calculando a distância de Hellinger para cada classe, permitindo quantificar o grau de mudança de distribuição e ajustar as prevalências de classe de acordo. O DyS estende o HDy ao incorporar várias funções de distância para refinar a comparação. Além da distância de Hellinger, o DyS pode utilizar métricas como a divergência de Jensen-Shannon, distância de Bhattacharyya ou distância de Movimento da Terra, entre outras. Ao utilizar múltiplas funções de distância, o DyS pode capturar diferentes aspectos das mudanças distribucionais entre os conjuntos de treinamento e teste.
A abordagem KDEy-ML aborda o problema modelando distribuições utilizando Modelos de Misto Gaussiana (MMG) e otimizando dentro do framework de máxima verossimilhança. Este método envolve ajustar MMG para os dados de treinamento para modelar sua distribuição como uma mistura de componentes Gaussiana. Em seguida, KDEy-ML visa minimizar a divergência de Kullback-Leibler (KL) entre a densidade de probabilidade de dados de teste e a densidade de mistura de dados de treinamento.

3.3 Protocolo de Avaliação de Quantificação
A avaliação de métodos de quantificação apresenta um desafio devido à disparidade entre tarefas de classificação e quantificação. Em uma tarefa de classificação, um conjunto de dados com n pontos de dados resulta em n pontos de dados de teste. No entanto, o mesmo conjunto de dados fornece apenas 1 ponto de dados de teste em uma tarefa de quantificação, como observado na Figura 1. Esta disparidade surge porque a tarefa de quantificação visa estimar a distribuição geral em um lote de dados, resultando em uma única previsão para todo o lote, ao contrário da classificação, que envolve n previsões. Vários protocolos de avaliação para quantificação foram propostos para abordar esse desafio, como o Protocolo de Prevalência Artificial (PPA) [24]. O PPA é um protocolo de avaliação amplamente utilizado para avaliar algoritmos de quantificação. Ele envolve extraíndo amostras múltiplas de um conjunto de dados de teste com valores de prevalência controlados e simulando cenários onde as prevalências de classe difiram entre conjuntos de treinamento e teste para avaliar a robustez dos quantificadores em relação a mudanças de probabilidade anterior. Os passos do PPA incluem extraíndo amostras com valores de prevalência pré-definidos, gerando amostras de teste por subsampling da classe positiva, aplicando o quantificador para estimar a prevalência de classe e comparando prevalências estimadas com prevalências verdadeiras. Um protocolo mais recente, o Protocolo de Prevalência Uniforme (PPU) [6], representa uma modificação do PPA projetada para gerar amostras artificiais com valores de prevalência de classe diversificados.
A seguir, a tradução do texto científico para português do Brasil:

Diferentemente do APP, o UPP não depende de um conjunto pré-definido de valores de prevalência de classe; em vez disso, ele emprega o algoritmo de Kraemer [27] para permitir que esses valores variem aleatoriamente. Essa flexibilidade permite ao UPP oferecer várias vantagens em relação ao APP, como permitir que os usuários especifiquem o número desejado de amostras e facilitar a seleção de qualquer vetor de distribuição concebível. Essas capacidades são cruciais para cenários em que a distribuição de prevalências de classe é complexa e não pode ser capturada facilmente por uma grade fixa de valores. Para conduzir nossos experimentos, escolhemos o UPP.

3.4 Métrica de Avaliação de Quantificação

Para avaliar e comparar nossos métodos, empregamos o Erro Absoluto (EA), uma medida comum utilizada em avaliação de quantificação. Essa métrica calcula a diferença absoluta entre a prevalência estimada e a prevalência verdadeira, fornecendo uma avaliação quantitativa da precisão das estimativas de prevalência obtidas dos métodos de quantificação. A fórmula para o EA é dada por:

EA(p, ˆp) = 1 |Y | X y∈Y |ˆp(y) −p(y)| (1)

onde p representa a prevalência de classe verdadeira, ˆp representa a prevalência de classe estimada e Y é o conjunto de classes de interesse.

O Erro Médio Absoluto (EMA) fornece uma avaliação direta da precisão geral das estimativas de prevalência, simplesmente retornando a média do erro absoluto entre as prevalências estimadas e verdadeiras para todas as classes de interesse.

4 Configuração Experimental

Escolhemos um classificador de Regressão Logística regularizada L2 (LR) como modelo subjacente para nossos métodos de quantificação, exceto para SVM(MAE), que utiliza SVMPerf. Classificadores de Regressão Logística oferecem probabilidades posteriores bem calibradas, essenciais para métodos como PCC, PACC, DyS e SLD [22, 12], e é amplamente utilizado na literatura de quantificação [22, 26, 20, 12, 4].
Começamos com a pré-processamento de texto, que inclui a conversão do texto para minúsculas, remoção de acentos e eliminação de pontuação e caracteres especiais. A extração de características é realizada usando TF-IDF, descartando características que aparecem em menos de 5 documentos de treinamento. O experimento é avaliado usando a validação cruzada k-fold (kCV) com k = 3. Para cada tupla única (p(positivo), p(negativo)) representando valores de prevalência de classe, onde cada valor de prevalência de classe é uniformemente selecionado do simplex-unidade [27], geramos m amostras aleatórias, cada contendo q documentos. Essas amostras são criadas para refletir os valores de prevalência de classe especificados pela tupla. Nesses experimentos, definimos m = 1000 e avaliamos múltiplos valores de q (20, 50, 100, 500) para considerar o impacto do tamanho de teste no desempenho, como demonstrado em [17]. Para cada rótulo y (denotando positivo e negativo), e para cada amostra, a extração é realizada usando amostragem sem substituição se houver amostras suficientes no conjunto de treinamento; caso contrário, a amostragem é feita com substituição. Realizamos otimização de hiperparâmetros nos classificadores subjacentes, SVM-perf para o método SVM(MAE) e Regressão Logística (LR) para todos os outros. Como destacado em pesquisas anteriores [21], esse passo é importante para mitigar bias na experimentação de métodos de quantificação agregativa. A otimização deve ser conduzida usando uma perda orientada para a quantificação em vez de uma perda orientada para a classificação [22]. Portanto, otimizamos o Erro Absoluto Médio (MAE) em um conjunto de validação composto por 30% do conjunto de treinamento completo. Para executar essa otimização, aplicamos o Protocolo de Prevalência Uniforme (UPP) novamente. Extraímos m amostras de q documentos cada para cada combinação de valores de prevalência de classe do conjunto de validação de cada fold na validação cruzada k-fold (kCV). Os valores de prevalência de classe são uniformemente selecionados do simplex-unidade.
Nesse contexto, m′ é um valor constante definido como 200, e o valor de q′ varia ao longo de q. A otimização é realizada utilizando o método de Pesquisa em Rede [15]. Os parâmetros otimizados para LR são a força de regularização C (variando de 10−3 a 103) e configurações de pesos de classe (ou 'Nenhum' ou 'balanceado'). Para SVMperf, otimizamos a força de regularização C sobre o mesmo intervalo. Finalmente, o melhor modelo obtido através desse processo é re-treinado no conjunto de treinamento completo e estima os valores de prevalência de classe para o conjunto de teste do fold de validação especificado. A métrica de avaliação utilizada para avaliar a precisão geral dos métodos de quantificação de sentimento é a MAE. 10 Autores Suprimidos devido à Extensão Excessiva

5 Resultados

Nesta seção, apresentamos os resultados da avaliação de vários métodos de quantificação de sentimento aplicados a textos em português brasileiro. Fornece uma análise da performance de cada método em diferentes conjuntos de dados sob condições de tamanho de teste e graus de mudanças na prevalência de sentimento.

5.1 Tamanho de teste

Começamos observando os resultados médios para todos os tamanhos de teste apresentados na Tabela 2. Os resultados sugerem que nenhum método alcançou o melhor desempenho em todos os conjuntos de dados e tamanhos de teste. Cada método mostrou graus variados de eficácia dependendo do conjunto de dados e condições de tamanho de teste específicas.

Tabela 2: MAE média para cada método em todos os tamanhos de teste.

Conjuntos de dados cc acc pcc pacc sldc kde hdy ms2 ms max svmmae dys Computador-BR 0,175 0,126 0,164 0,103 0,152 0,089 0,129 0,103 0,130 0,132 0,170 0,103 Livros 0,205 0,191 0,201 0,150 0,190 0,165 0,193 0,204 0,180 0,171 0,209 0,162 Eleições2018 0,091 0,050 0,091 0,056 0,067 0,044 0,051 0,042 0,040 0,074 0,090 0,048 RePro 0,016 0,014 0,017 0,013 0,010 0,011 0,012 0,022 0,028 0,014 0,018 0,010 Sentenças 0,271 0,168 0,238 0,144 0,225 0,185 0,270 0,130 0,133 0,134 0,229
216 Examínamos ainda a performance média dos métodos de quantificação de sentimento utilizando o teste de Friedman e o teste de Nemenyi, como ilustrado no diagrama de diferença crítica na Figura 2. O teste de Friedman gerou um valor-p de 0,01, o que nos levou a rejeitar a hipótese nula de que todos os algoritmos têm a mesma performance a um nível de significância de 5%. Isso indica diferenças estatisticamente significativas na performance dos métodos de quantificação avaliados. O diagrama de diferença crítica do teste de Nemenyi post-hoc esclarece ainda mais essas diferenças. Ele classifica as pontuações de performance média dos métodos, onde rankings mais baixos indicam melhor performance. A partir do diagrama, observamos que os métodos KDE e DyS alcançaram os melhores rankings médios (3,2), indicando sua superior performance global em relação aos conjuntos de dados e tamanhos de teste. Notadamente, foram os únicos métodos de quantificação que alcançaram diferenças estatisticamente significativas em relação à baseline CC. O método PACC seguiu-se de perto, demonstrando desempenho forte. Métodos como MS, ACC e MS2 exibiram desempenho moderado. Mais abaixo na escala de performance, os métodos SLDC, HDy e MAX mostraram desempenho comparativamente mais baixo, mas ainda foram eficazes em certos cenários. Os métodos PCC e SVM(MAE) tiveram rankings médios mais baixos, destacando seu desempenho relativamente pior. Por fim, o método CC teve o pior ranking médio, confirmado sua limitada eficácia nos contextos testados. A variabilidade na eficácia dos métodos destaca a importância de escolher o método de quantificação certo com base nas características específicas do conjunto de dados e condições de teste. Observamos resultados diferentes quando analisamos o diagrama de diferença crítica para cada tamanho de conjunto de teste.
Não houve diferença estatística entre as performances dos métodos para os tamanhos de teste mais pequenos (20 amostras) e mais grandes (500 amostras). Podemos esperar esse comportamento para o tamanho de teste mais pequeno, como evidenciado por pesquisas anteriores [17], indicando que os métodos geralmente desempenham pior em tamanhos pequenos. Em relação ao tamanho mais grande, um fator importante é o tamanho original dos conjuntos de dados e a estratégia de amostragem utilizada para formar os 500 documentos com a UPP. Nesse caso, houve a necessidade de aplicar amostragem com substituição para 3 dos 5 conjuntos de dados avaliados (Livros, Eleições2018 e Sentenças), o que implica sobre-amostragem dos conjuntos de treinamento e teste. Especificamente, a amostragem com substituição pode resultar em certos exemplos sendo selecionados várias vezes, o que pode levar a algum viés, como reduzir a capacidade do modelo para generalizar para dados verdadeiramente desconhecidos. Observamos diferenças significativas para tamanhos de teste de 50 e 100 amostras, como ilustrado na Figura 3. Em geral, os diagramas destacam que o KDE alcançou o melhor desempenho para ambos os tamanhos de teste e foi o único método com diferenças estatisticamente significativas em relação ao baseline CC. O PACC e o DyS desempenharam bem consistentemente em ambos os tamanhos de teste, enquanto o SVM(MAE) desempenhou-se consistentemente pior, superando apenas o baseline. 5.2 Deslocamento distribucional Continuando nossa análise, examinamos o desempenho do nosso método em relação à intensidade do deslocamento entre conjuntos de treinamento e teste. Classificamos esses deslocamentos usando a distância de Manhattan para medir a dissimilaridade entre as distribuições de treinamento e teste. Consideramos um deslocamento major se a distância foi de 0,8 ou maior, minor se foi menor que 0,4 e médio em caso contrário. A Figura 4 apresenta a média do MAE dos métodos em todos os conjuntos de dados para cada intensidade de deslocamento. Como esperado, maior é a intensidade do deslocamento, pior é o erro.
Figura 5 apresenta a distribuição AE para cada conjunto de dados e intensidade de deslocamento para fornecer uma compreensão mais detalhada da estabilidade dos métodos em diferentes intensidades de deslocamento e características dos conjuntos de dados. Alguns métodos, como as seleções de limiar MS e MS2, bem como PACC, DyS e KDE, exibem desempenho relativamente estável em diferentes intensidades de deslocamento. Em contraste, métodos como a base CC, PCC, SLDC e SVM(MAE) demonstraram comportamento mais instável, produzindo resultados consideravelmente piores à medida que a intensidade de deslocamento da distribuição aumentava. Os padrões são consistentes em todos os conjuntos de dados, exceto pelo RePro, onde os métodos demonstraram superior estabilidade e desempenho geral melhor. Essa diferença pode ser atribuída à disponibilidade de dados, pois RePro é o maior conjunto de dados com 7.576 amostras. Consequentemente, não requer o oversampling discutido anteriormente para gerar todos os tamanhos de amostra. Fig. 4: MAE média para cada método em todos os conjuntos de dados e intensidades de deslocamento. Título Suprimido devido à Extensão Excessiva 13 Fig. 5: Valores AE em diferentes intensidades de deslocamento para todos os conjuntos de dados.

Conclusão
Este estudo forneceu uma avaliação abrangente de vários métodos de quantificação de sentimento aplicados a textos em português brasileiro em diferentes condições, particularmente se concentrando em tamanhos de teste variáveis e intensidade de deslocamento da distribuição entre conjuntos de treinamento e teste. Vários achados emergiram da nossa análise. Em primeiro lugar, o KDE consistentemente exibiu desempenho superior em diferentes tamanhos de teste, mostrando melhorias estatisticamente significativas em relação à base CC. Ao lado do KDE, PACC e DyS também se saíram bem, demonstrando estabilidade e eficácia em diferentes condições. Em contraste, o SVM(MAE) conseguiu resultados ruins, desempenhando-se apenas marginalmente melhor que a base. O impacto da intensidade de deslocamento da distribuição no desempenho dos métodos de quantificação foi significativo.
Como esperado, intensidades de mudança mais altas levaram a erros aumentados para todos os métodos. No entanto, certos métodos, como os métodos de seleção de limiar (MAX, MS, MS2), PACC e KDE, apresentaram desempenho relativamente estável, indicando sua robustez à mudança de distribuição. Por outro lado, métodos como o CC de base, PCC, SLDC e SVM(MAE) exibiram maior instabilidade e desempenho piorado à medida que a intensidade de mudança aumentava. O teste de Friedman e o teste de Nemenyi post-hoc confirmaram ainda mais as diferenças significativas no desempenho entre os métodos. KDE e DyS foram os únicos métodos que alcançaram melhorias estatisticamente significativas em relação ao baseline em todos os tamanhos de teste. Olhando para o futuro, pesquisas futuras podem investigar a integração de técnicas de aprendizado profundo, como modelos de transformadores, em métodos de quantificação para potencialmente melhorar o desempenho significativamente. Entender os trade-offs entre a precisão de quantificação e a eficiência computacional será crucial para implementar esses métodos em ambientes com recursos limitados. Além disso, realizar avaliações mais abrangentes em uma ampla gama de línguas e domínios ajudará a generalizar os resultados e validar a aplicabilidade dos métodos. Explorar métodos semi-supervisionados e não supervisionados pode reduzir a dependência de dados rotulados, tornando a quantificação de sentimento mais acessível e escalável em vários domínios e tamanhos de amostra. Além disso, examinar o impacto de diferentes técnicas de aumento de dados na performance dos métodos de quantificação pode oferecer insights para melhorar a generalização e robustez. Em resumo, nossos achados destacam a importância de selecionar métodos de quantificação apropriados com base nas características específicas do conjunto de dados e nas intensidades de mudança de distribuição previstas.
Métodos como KDE e DyS oferecem desempenho robusto em várias condições, tornando-os particularmente adequados para aplicações práticas em que se esperam distribuições de intensidade variáveis. Agradecimentos. Esta pesquisa foi financiada em parte pela Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Código de Financiamento 001.