Um Conjunto de LLMs finetunados com LoRA para NER em documentos legais portugueses Rafael Oleques Nunes1[0009−0007−8842−421X], Letícia Puttlitz1[0009−0002−0313−1017], Antonio Oss Boll1[0009−0004−5440−6126], Andre Spritzer1[0009−0002−4232−1585], Carla Maria Dal Sasso Freitas1[0000−0003−1986−8435], Dennis Giovani Balreira1[0000−0002−0801−9393], e Anderson Rocha Tavares1[0000−0002−8530−6468] Universidade Federal do Rio Grande do Sul, Brasil {ronunes,spritzer,carla,dgbalreira,artavares}@inf.ufrgs.br {leticia.puttlitz,antonio.boll}@ufrgs.br Resumo. Dado os altos custos computacionais dos métodos de fine-tuning tradicionais e o objetivo de melhorar o desempenho, este estudo investiga a aplicação da adaptação de baixo rango (LoRA) para fine-tuning modelos BERT para Reconhecimento de Entidade Nomeada (NER) em documentos legais portugueses e a integração de Modelos de Linguagem Grande (LLMs) em um setup de ensemble. Focando na língua portuguesa subrepresentada, objetivamos examinar a confiabilidade das extrações habilitadas pelos modelos LoRA e obter insights práticos dos resultados dos modelos LoRA e LLMs operando em conjunto. Os modelos LoRA demonstraram desempenho competitivo, alcançando pontuações F1 de 88,49% para o corpus LeNER-Br e 81,00% para o corpus UlyssesNER-Br. Nossa pesquisa demonstra que a incorporação de definições de classes e contagem de votos por classe melhoria substancialmente os resultados do ensemble de LLMs. Em geral, essa contribuição avança as fronteiras da mineração de texto legal com inteligência artificial, propõe modelos pequenos e engenharia de prompts iniciais para condições de baixa recursos que são escaláveis para uma representação mais ampla. Palavras-chave: Reconhecimento de Entidade Nomeada · Modelos de Linguagem Grande · LoRA. 1 Introdução Com os avanços na Processamento de Linguagem Natural (NLP), o domínio de NLP legal também segue em constante evolução, enfrentando desafios e obstáculos semelhantes.
Uma das principais áreas de trabalho no campo de Processamento de Linguagem Natural (NLP) jurídico é a Reconhecimento de Entidades Nomeadas (NER), que visa identificar e extrair entidades em texto, como petições, projetos de lei e frases de documentos legais. O processo de reconhecimento dessas entidades tornou-se desafiador devido à falta de estrutura e entidades pré-definidas em corpora do sistema jurídico. Outro obstáculo é trabalhar no domínio do sistema jurídico brasileiro, pois não tem o mesmo número de documentos e modelos que o idioma inglês. Para abordar esses problemas, os pesquisadores criaram múltiplos corpora para abordar a questão da NER no domínio jurídico brasileiro, como o LeNER-Br [5] e o UlyssesNER-Br [3], adicionando entidades a múltiplas frases e ajudando a criar um corpus amigável à NER jurídica. Além disso, muitos modelos BERT foram criados para facilitar a NER jurídica brasileira, incluindo o LegalBERT-pt [29], o BERTikal [23] e muitos outros. Neste trabalho, propomos uma investigação abrangente centrada em dois objetivos primários: primeiro, analisar a eficácia do deployment do LoRA para fine-tuning de arquiteturas BERT para tarefas de NER no domínio jurídico, em contraste com a literatura relevante; e segundo, apresentar um estudo sobre o design e a implementação de táticas de engenharia de prompts para modelagem de ensembles de LLM, com o objetivo de melhorar a robustez. Nossas contribuições principais são (i) entregar um estudo sobre a praticidade e vantagens de acoplar modelos LoRA e BERT no contexto especializado da NER jurídica, complementado por comparação com trabalhos relacionados, e (ii) uma exposição sobre a exploração de ingredientes de prompts assorted para construir ensembles de LLM. O Processamento de Linguagem Natural está constantemente evoluindo com a introdução de novos conceitos. Cada nova técnica abre portas para vários campos de trabalho, cada um com seu abordagem de modelagem e saída únicos.
Introduzido por Mikolov em 2013 [18], as representações de palavras em vetores deu um novo olhar à área de processamento de linguagem natural (PLN), permitindo que as palavras fossem representadas de forma vetorial. Depois disso, em 2017, o modelo Transformer [31] revolucionou grande parte do trabalho na área de PLN, inspirando a criação de vários modelos com base em suas ideias, como BERT [13] e GPT [24]. O BERT, por exemplo, utiliza apenas a parte de codificação do transformer e pode entender o contexto e trabalhar em várias outras tarefas que requerem compreensão de texto. Enquanto o GPT, utilizando a parte de decodificação, pode gerar texto, entre outras tarefas. Outro modelo digno de menção é o ELMo, introduzido em 2018 [22], que representa termos em um espaço vetorial e os torna sensíveis ao contexto. Os pesquisadores têm criado vários modelos específicos para domínios nos últimos anos devido à capacidade do BERT de treinar pré-seleção de dados. O BERTimbau [30], por exemplo, foi desenvolvido especificamente para fazer inferências com dados em português, com foco principal em corpora de português brasileiro. Ele aproveita a arquitetura do transformer para alcançar desempenho alto em várias tarefas de processamento de linguagem natural, como Similaridade Textual de Sentença e Reconhecimento de Entidade Nomeada. Para o treinamento, o BERTimbau utilizou o corpus brWaC [32], que consiste em um grande corpus da Web para português brasileiro. No domínio jurídico, vários modelos foram desenvolvidos para vários idiomas, incluindo inglês [10], alemão [12], árabe [2], francês [14] e português [29]. O corpus também é um passo crucial para obter os resultados, pois precisa ser do domínio e ter as entidades corretas ligadas a ele. Alguns dos corpora jurídicos do sistema jurídico brasileiro para Reconhecimento de Entidade Nomeada são LeNER-Br [5], UlyssesNER-Br [3], corpus do Supremo Tribunal Federal [11] e CDJUR-BR [9].
Noscentamos em LeNER-Br e UlyssesNER-Br, pois o primeiro é predominantemente um corpus judiciário e o segundo é um corpus legislativo. Esta abordagem fornece uma perspectiva interessante sobre dois aspectos diferentes do domínio jurídico. Em relação ao corpus LeNER-Br, Bonifácio et al. (2020) [6] encontraram que o uso de um corpus específico do domínio durante o treinamento prévio do BERT multilíngue pode melhorar a reconhecimento de entidades nomeadas. Além disso, Zanuz et al. (2022) [35] alcançaram resultados de ponta utilizando o corpus LeNER-Br para fine-tuning o BERTimbau. Para o corpus UlyssesNER-Br, Albuquerque et al. (2023) [4] fine-tunaram o BERTimbau e conduziram a primeira avaliação desse corpus usando modelos BERT. Nunes et al. (2024) [20] investigaram como uma técnica semi-supervisionada pode melhorar o desempenho do BERTimbau no domínio legislativo, demonstrando que essas técnicas podem melhorar os resultados dos modelos. Um estudo recente [21] se concentrou em como um Modelo de Linguagem Gerativa (GLM) especializado em português brasileiro se comporta em ambos os corpora. Os autores encontraram que o Aprendizado em Contexto pode fornecer resultados iniciais e demonstraram o potencial desses modelos para extrair entidades. No entanto, eles notaram que estudos adicionais são necessários, pois os modelos BERT têm mostrado melhor desempenho. Conhecemos melhor que nosso trabalho é o primeiro a analisar a eficácia do uso de LoRA para fine-tuning modelos BERT específicos para tarefas de reconhecimento de entidades nomeadas jurídicas e a exploração do design e implementação de técnicas de engenharia de prompts para usar LLM para ensembles de modelos de reconhecimento de entidades nomeadas jurídicas. Essas contribuições oferecem uma perspectiva inédita sobre o estudo do reconhecimento de entidades nomeadas jurídicas e os benefícios práticos de combinar LoRA e modelos BERT dentro desse domínio especializado. 3 Corpora do Domínio Este Seção apresenta dois tipos de corpora que reconhecem entidades nomeadas jurídicas, cada um se concentrando em um tipo diferente de dados. Eles são chamados de LeNER-Br [5] e UlyssesNER-Br [3], ambos provenientes do sistema jurídico brasileiro.
Escolhemos esses dois corpora porque podem acessar diferentes frentes de domínios legais, aproveitando textos judiciais e legislativos. Ambos corpora estão inseridos no contexto legislativo. Enquanto as sentenças no LeNER-Br são extraídas de textos de tribunal e legislativos, aquelas no UlyssesNER-Br vêm de inquéritos legislativos e projetos de lei da Câmara dos Deputados do Brasil. O primeiro corpus, introduzido em 2018 e chamado LeNER-Br [5], se concentra em dados do sistema de justiça brasileiro. Ele é composto por um total de 70 documentos legais de diferentes tipos de tribunais e legislações, contendo um total de 10.392 sentenças e 318.073 tokens. As categorias de entidades nomeadas incluem "Pessoa", "Casos legais", "Tempo", "Localização", "Legislação" e "Organização". Focalizando no lado legislativo dos dados legais, o UlyssesNER-Br [3] consiste em dois tipos de dados da Câmara dos Deputados do Brasil: consultas legislativas (ST) e projetos de lei (PL). O banco de dados ST tem 790 sentenças e 77.441 tokens, enquanto o corpus PL contém 9.526 sentenças com 138.741 tokens. Ambos os conjuntos de dados incluem as seguintes categorias de entidades nomeadas: "Fundamento", "Organização", "Produto de lei", "Local", "Data" e "Evento". 4 Modelos e fine-tuning com LoRA Esta seção fornece uma visão geral dos modelos utilizados para reconhecer entidades legais nomeadas. Esses modelos são variações do modelo BERT [13], cada um pré-treinado com um tipo específico de dados, além da adição de um Modelo de Linguagem Grande. O BERTimbau [30], lançado em 2020, é um dos primeiros modelos BERT brasileiros. Foi pré-treinado usando o brWaC [33], um grande corpus de dados da web brasileira. Dois tipos de BERTimbau foram pré-treinados: BERTimbau Base, com 12 camadas, tamanho de ocultação de 768, 12 cabeças de atenção e 110 milhões de parâmetros; e BERTimbau Grande, com 24 camadas, tamanho de ocultação de 1024, 16 cabeças de atenção e 330 milhões de parâmetros.
O comprimento máximo para uma sentença é de 512 tokens para ambos os modelos. Utilizamos o BERTimbau Base para a análise. Foi treinado separadamente nos corpora UlyssesNER-Br e LeNER-Br. Os hiperparâmetros utilizados para esse modelo estão especificados na Seção 6.2. Com o objetivo de desenvolver um modelo focado em direito, o LegalBERT-pt [29] é um modelo BERT pré-treinado em corpora jurídicas brasileiras. Os autores pré-treinaram dois tipos de modelos: LegalBert-pt SC e LegalBert-pt FP. O modelo SC é formulado por pré-treinar um modelo BERT desde o início, com a mesma configuração do BERTimbau base. Diferentemente, o modelo FP pré-treina um BERTimbau-Base com corpora específicas do domínio. Os corpora utilizados no artigo consistem em múltiplos casos de tribunal brasileiros e, para o modelo SC, artigos da Wikipédia em português, totalizando 1.500.000 documentos jurídicos e um vocabulário de 36.345 palavras. O LegalBERT-pt FP foi selecionado como o modelo mais bem-desempenhado. Neste estudo, o LegalBERT-pt FP foi treinado usando os corpora descritos na Seção 3, com seus hiperparâmetros detalhados na Seção 6.2. Concluindo os modelos variantes de BERT, o BERTikal [23] é um modelo BERT treinado em corpora de clippings, casos de tribunal e movimentos em dados jurídicos brasileiros. Os modelos descritos anteriormente foram treinados nos corpora LeNER-br e UlyssesNER-Br (ver Seção 3) e utilizaram hiperparâmetros descritos na Seção 6.2. Realizamos a fine-tuning com LoRA [16], método que reduz o número de parâmetros treináveis e, consequentemente, o custo computacional de treinamento utilizando álgebra de matrizes. O método congela os parâmetros pré-treinados do modelo e otimiza a decomposição de rank das matrizes de camada densa. O único Modelo de Linguagem de Grande Escala (LLM) que utilizamos é o Mistral 7B [17]. Ele é composto por 7 bilhões de parâmetros, tem um tamanho de dimensão de 4096, 32 camadas, um tamanho de vocabulário de 32.000 e múltiplos outros parâmetros e hiperparâmetros. Obtemos o corpus de treinamento da web aberta.
Como LLM, o modelo Mistral 7B é capaz de realizar uma gama diversa de tarefas, incluindo conhecimento, matemática, razão, compreensão e código, entre outras. Utilizamos esse modelo para a técnica de ensemble explicada na Seção 5 e para um método de Aprendizado por Transferência sem Treinamento. Título Suprimido devido à Extensão Excessiva 5 5 Um Ensemble de Modelos de Linguagem O aprendizado por ensemble visa melhorar o desempenho preditivo de um modelo único treinando vários modelos e combinando suas previsões [26]. Uma forma simples de ensemble é a votação, onde classificadores de base são apresentados com uma entrada e cada faz uma previsão. Em seguida, a previsão que recebe o maior número de votos é selecionada como saída final. Aplicamos o aprendizado por ensemble para combinar vários modelos de linguagem na tarefa de Reconhecimento de Entidade Nomeada no Direito. Em nossa abordagem, modelos BERT servem como classificadores de base para o Reconhecimento de Entidade Nomeada. Após os modelos BERT gerarem suas previsões, o Mistral atua como agregador no ensemble, combinando os resultados. Esse método fornece um resultado unificado que incorpora e valoriza todas as previsões dos modelos BERT. Baseamos nossa engenharia de prompt nos seguintes elementos: pessoa, definições, votos, formato de resposta, sentença e consulta. O prompt começa com uma pessoa, que é usada para imitar um linguista especializado em ciência política para alcançar o melhor desempenho possível [27]. Escolhemos esse papel para a pessoa porque é uma tarefa linguística que requer conhecimento de domínio jurídico. Utilizamos as definições das classes de entidade para aliviar o problema de algumas categorias terem nomes que não são intuitivos com base em seus nomes. Por exemplo, a categoria fundamento1 não parece estar diretamente ligada a normas ou projetos de lei. Portanto, também damos as definições das classes de entidade no prompt. As definições foram fontes de corpora [3,5] e traduzidas para o português.
No entanto, adaptamos as definições para pessoa e localização a partir de Harem [28] porque ambos os corpora foram influenciados por Harem e aderiram às definições de classe. Desenhamos as votações para fornecer informações ao modelo sobre as classes que receberam mais ou menos votos de cada classificador. Presentamos a votação no formato Classe: x votos, onde classe é o nome da classe de entidade e x é o número de votos. No formato de resposta, incentivamos o modelo a usar a cadeia de pensamento (CoT) [34] explicando a resposta antes de fornecer a classe de entidade. Dividimos as respostas em explicação e resposta para facilitar a pós-processamento para obter a classe. Finalmente, no final da solicitação, fornecemos a sentença e pedimos ao modelo que atribua a classe de entidade certa a um termo ou diga que não tem classe no formato de pergunta-resposta. Forneceu-se entidades dos classificadores, definições e votos. Também testamos usando todas as entidades para as definições e perguntas. Todas as classes utilizadas na solicitação estão dadas com a primeira letra em maiúscula e o resto em minúscula. Além disso, usamos espaço para dividir termos de n-gram, como em produto de lei. Optamos por usar esse formato para que os nomes permaneçam em um idioma mais fluido, fornecendo um contexto melhor e evitando a divisão desnecessária em mais tokens dos termos. 1 Em inglês: fundação. 6 Autores Suprimidos devido à Excessiva Duração Às vezes, o modelo fornece classes de resposta próximas dos objetivos, mas com nomes ligeiramente diferentes (por exemplo, erros de digitação). Usamos Sentence-BERT (SBERT) [25] para pós-processamento, juntamente com as classes [21]. Depois da comparação, convertemos a resposta em um vetor anotando a entidade do termo na sentença. O vetor é anotado incrementalmente porque usamos uma solicitação para cada termo.
6 Avaliação Experimental Esta seção fornece uma visão geral do ambiente experimental, abrangendo as principais bibliotecas utilizadas, hiperparâmetros e uma explicação detalhada da otimização de hiperparâmetros realizada para os modelos LoRA. Além disso, discutimos as métricas de desempenho utilizadas para avaliar nossos modelos.

6.1 Configuração Realizamos os experimentos em um computador com 12 GB de RAM e um GPU Nvidia GeForce RTX 4070. Devido ao amplo suporte às bibliotecas relacionadas à aprendizagem de máquina e processamento de linguagem natural, decidimos implementar nossos experimentos utilizando o Python 3.7.6. A quantização, realizada utilizando a biblioteca bitsand-bytes2, foi empregada para reduzir os requisitos de memória e custos computacionais. Os modelos BERT e Mistral foram obtidos do Hub HuggingFace. As funções e métodos de LoRA e a execução dos modelos foram utilizados do HuggingFace.

6.2 Hiperparâmetros Nesta seção, descrevemos os hiperparâmetros do experimento. Primeiramente, apresentamos a busca de hiperparâmetros e os valores finais para o treinamento do LoRA. Aqui, apresentamos os valores utilizados para treinar os modelos BERT. Finalmente, apresentamos os hiperparâmetros para a quantização e geração de respostas para o LLM. LoRA. Utilizamos o Optuna [1] para realizar a busca de hiperparâmetros. Definimos o intervalo de otimização para dropout em torno de 0,1 e 0,5, e o intervalo de r e alpha entre 16, 32, 64 e 128, com 100 iterações. Os hiperparâmetros finais foram r = 32, alpha = 1,747406, dropout = 0,1 (sem otimização) e bias = “all”. BERT. Seguimos estudos anteriores que utilizaram os mesmos corpora legais para definir os hiperparâmetros do classificador BERT [20,35,7]. O conjunto de valores foi learning_rate = 1e-3, batch_size = 10 e weight_decay_size = 0,01. LLM. Utilizamos quantização de 4 bits para carregar o modelo em nossa máquina, definindo os atributos load_in_4bit = Verdadeiro, bnb_4bit_compute_dtype = torch.bfloat16, torch_dtype = torch.float16, device_map = “auto”.
Para gerar a resposta, utilizamos o valor padrão do método generate, definindo max_new_tokens = 800 e pad_token_id = model.config.eos_token_id. 2 https://github.com/TimDettmers/bitsandbytes Título Suprimido devido à Extensão Excessiva 7 6.3 Métricas Utilizamos Seqeval [19] para calcular as métricas. Essa biblioteca avalia os resultados considerando a sequência de tags atribuídas a cada entidade em uma sentença completa, claramente reconhecendo entidades além de tokens individuais. Nossa métrica primária é o F1-Score, calculado para cada classe e global. Além disso, precisão e recall foram computados para cada classe, enquanto a precisão foi determinada para os resultados globais. 7 Resultados e Discussão Nesta seção, apresentamos os resultados de nossos experimentos envolvendo LoRa aplicada a modelos BERT e LLM como abordagem de ensemble. Inicialmente, vamos explorar o desempenho dos classificadores LoRa individuais, considerando qualquer conexão potencial com o domínio de treinamento prévio dos modelos base subjacentes. Posteriormente, revelamos os resultados combinados alcançados pela técnica de ensemble LLM e examinamos as relações entre os modelos selecionados e prompts. Finalmente, comparamos nossos resultados com os existentes baselines de estado-da-arte (SOTA). 7.1 Classificadores LoRA Tabela 1. F1-Score para cada corpus para os classificadores LoRA. Modelo LeNER-Br UlyssesNER-Br LegalBERT-pt 88,49 76,96 BERTimbau 87,12 81,00 BERTikal 81,53 67,72 Presentamos o F1-Score final dos classificadores para cada corpus na Tabela 1. LegalBERT-pt apresentou o melhor resultado para LeNER-BR, obtendo 88%, seguido por BERTimbau, com 87%. Para UlyssesNER-Br, obtivemos o resultado inverso, onde BERTimbau teve o melhor resultado a 81%, seguido por LegalBERT-pt com 76%.
Este resultado provavelmente ocorreu porque o LegalBERT-pt segue a pré-entrenamento do BERTimbau em documentos judiciários (mais informações na Seção 4), que é o domínio de dados do LeNER-Br, enquanto o UlyssesNER-Br utiliza documentos legislativos, o que pode explicar os resultados mais baixos nesse caso. No entanto, é importante notar que não podemos expressar a consistência da diferença entre os resultados próximos porque não utilizamos a validação cruzada k-fold para obter o resultado médio e a variância em torno das fold. Além disso, não computamos testes para obter significância estatística. Outra observação se refere ao formato de resposta, que descobrimos durante a inspeção manual. Especificamente, existem instâncias onde os outputs gerados falham em aderir estritamente aos formatos prescritos. Dado essa discordância, há um risco de que respostas precisas possam não ser detectadas devido à apresentação inesperada, consequentemente contribuindo para reduzir as pontuações F1.
20 BERTimbau + legalbert + bertikal definições + votos + todas as etiquetas 47,80 legalbert definições + todas as etiquetas 47,09 bertikal definições + votos 46,76 BERTimbau + legalbert definições + todas as etiquetas 45,99 bertikal definições 45,95 bertikal definições + votos + todas as etiquetas 44,97 BERTimbau + legalbert + bertikal definições + todas as etiquetas 43,65 BERTimbau + bertikal definições + todas as etiquetas 42,28 legalbert + bertikal definições + todas as etiquetas 40,48 bertikal definições + todas as etiquetas 34,78

O desempenho resultante do emprego do Mistral 7B como abordagem de ensemble pode ser encontrado nas Tabelas 2 e 3. Realizamos experimentos abrangendo combinações diversificadas de prompts e modelos para avaliar o efeito de cada par de modelos-prompt. Surpreendentemente, a combinação de múltiplos modelos não gerou resultados melhorados. Os três melhores escores em ambos os conjuntos de dados apresentaram BERTimbau e LegalBert-Pt utilizados independentemente, ecoando a tendência vista na Tabela 1, na qual BERTimbau superou UlyssesNER-Br, enquanto LegalBert-Pt provou ser superior quando trabalhando com LeNER-Br. Em relação ao engenharia de prompts, nossa investigação revelou que o uso de descrições de definições paradas com votos gerou os resultados mais bem-sucedidos. Esta estratégia oferece ao modelo definições de classe claras e preferências individuais de classe indicadas pelos modelos participantes. Embora a distribuição de etiquetas seja Title Suprimido devido ao comprimento excessivo 9 Tabela 3. F1-Score global para LeNER-Br usando Mistral 7B como ensemble. Modelo Prompt F1-Score legalbert definições + votos 63,03 BERTimbau definições + votos 62,38 legalbert definições + votos + todas as etiquetas 61,71 legalbert + bertikal definições + votos 61,18 BERTimbau definições + votos + todas as etiquetas 61,07 legalbert definições 60,99 BERTimbau + bertikal definições + votos 60,74 BERTimbau definições 60,60 BERTimbau + legalbert definições + votos 60,46 BERTimbau + legalbert definições + votos + todas as etiquetas 60,46
13 definições legais + definições bertikais + votos + todos os rótulos 60,12 BERTimbau + definições legais + definições bertikais + votos 59,57 definições legais + definições bertikais 59,31 BERTimbau + definições bertikais + votos + todos os rótulos 59,22 BERTimbau + definições bertikais 59,11 BERTimbau + definições legais 58,94 BERTimbau + definições legais + definições bertikais + votos + todos os rótulos 57,62 BERTimbau + definições legais + definições bertikais 57,41 definições legais + todos os rótulos 51,86 definições BERTimbau + todos os rótulos 50,55 definições legais + definições bertikais + todos os rótulos 50,24 BERTimbau + definições legais + todos os rótulos 49,86 BERTimbau + definições bertikais + todos os rótulos 49,33 BERTimbau + definições legais + definições bertikais + todos os rótulos 48,59 definições bertikais + votos 5,26 definições bertikais + votos + todos os rótulos 5,18 definições bertikais 4,91 definições bertikais + todos os rótulos 3,65 10 Autores Suprimidos devido à Excessiva Longevidade aparecem dispersos ao longo dos achados, a maioria convergindo perto de 50% para LeNER-Br e caindo abaixo desse limiar para UlyssesNER-Br. Essas observações sublinham a importância de definir um espaço de solução constrangido ao utilizar LLMs, facilitando capacidades de categorização aprimoradas. Considerando os achados exploratórios apresentados até agora, é importante destacar a necessidade de realizar exames mais deliberados para adquirir comparações estatísticas robustas entre estruturas de prompts e modelos variados, como discutido na Seção 7.1. Além disso, abraçar uma estratégia de validação multifacetada, incluindo experimentação repetida (K iterações), cálculo de valores médios e determinação de desvios padrão, constitui outro elemento fundamental para fomentar maior certeza em torno dos resultados obtidos. 7.3 Nossos Resultados e o Estado-da-Art Tabela 4. Pontuação F1-Geral para cada corpus para nossos classificadores e classificadores SOTA. Os resultados para UlyssesNER-Br são de Nunes et al. (2024) [20], e os resultados para LeNER-Br são de Zanuz et al.
(2022) [35]. Modelo UlyssesNER-Br LeNER-Br BERTimbau + autoaprendizado 86,70 ± 2,28 - BERTimbau 83,53 ± 2,56 91,14 ± 0,39 LoRA (nossos) 81,00 88,49 LLM (nossos) 69,86 63,03 Tabela 5. Taxa F1 para cada classe no UlyssesNER-Br para nosso classificador e de Nunes et al. (2024) [20]. Os valores sublinhados são aqueles próximos aos resultados estabelecidos.

Categoria BERTimbau + LoRA BERTimbau + Autoaprendizado PRODUTODELEI 57,14 75,42 ± 4,47 PESSOA 89,60 87,48 ± 2,79 ORGANIZACAO 74,01 84,89 ± 5,77 LOCAL 74,37 86,46 ± 3,73 FUNDAMENTO 85,06 88,60 ± 2,29 EVENTO 47,06 58,10 ± 34,16 DATA 97,49 94,77 ± 2,65

Comparando nossos resultados com os da SOTA, a Tabela 4 revela que os classificadores LoRA alcançaram desempenho semelhante a estudos anteriores [20,35]. Especificamente, nosso classificador LoRA obteve resultados próximos àqueles de um classificador que apenas utilizava BERTimbau [20] no corpus UlyssesNER-Br e aproximou-se do desempenho do classificador proposto por Zanunz et al. (2022) [35] no corpus LeNER-Br.

Título Suprimido devido à Extensão Excessiva 11 Tabela 6. Taxa F1 para cada classe no LeNER-Br para nosso classificador e de Zanuz et al. (2022) [35]. Os valores sublinhados são aqueles próximos aos resultados estabelecidos.

Categoria LegalBert-Pt + LoRA BERTimbau TEMPO 92,02 96,04 ± 0,58 PESSOA 95,86 97,38 ± 0,44 ORGANIZACAO 86,04 86,66 ± 1,17 LOCAL 73,58 75,67 ± 3,18 LEGISLACAO 92,66 95,90 ± 0,83 JURISPRUDENCIA 78,66 87,76 ± 0,87

As classes que alcançaram resultados semelhantes aos de Nunes et al. (2024) [20] foram PESSOA, FUNDAMENTO, EVENTO e DATA, como mostrado na Tabela 5. Notavelmente, a estruturação de entidades, como DATA (data) e PESSOA (pessoa), apresenta uma explicação plausível para seu desempenho relativamente mais alto. As datas tipicamente aderem a formatos específicos, facilitando sua distinção de outras entidades, enquanto os nomes de indivíduos frequentemente seguem padrões reconhecíveis semelhantes aos observados em FUNDAMENTO (leis).
Conversamente, o EVENTO (evento) apresentou um desafio mais significativo devido à limitada treinamento e exemplos de teste disponíveis para essa classe. Além disso, uma análise comparativa com Zanuz et al. (2022) [35] revelou que classes como PESSOA, ORGANIZACAO e LOCAL demonstram tendências semelhantes de desempenho, como mostrado na Tabela 6. Essa alinhamento pode ser atribuído a fatores como treinamento extensivo, validação e dados de teste disponíveis para cada classe, bem como as complexidades estruturais inerentes associadas a entidades nessas categorias, como discutido anteriormente. Esses resultados demonstram a potência do LoRA para a tarefa de NER, com a capacidade de alcançar resultados bons ao treinar um número menor de parâmetros, ou seja, diminuimos o número de parâmetros de treinamento de 109.532.186 para apenas 1.291.789 parâmetros. Assim, essa abordagem resulta em modelos menores e mais rápidos que podem ser usados com menos potência de computador e armazenados em menos espaço.

8 Conclusão

Neste trabalho, apresentamos um estudo sobre o uso do LoRA para fine-tuning modelos BERT portugueses para tarefas de NER no domínio jurídico. Os modelos LoRA alcançaram resultados próximos aos de estado-da-arte para cada conjunto de dados testado. Outro ponto é que temos o advantage de treinar modelos menores, o que é um advantage para armazenar e executar o modelo em máquinas com baixa processamento. Também testamos o uso do Mistral 7B aplicado ao ensemble, que foi infrutífero. Os resultados foram inferiores em comparação com os modelos LoRA. No entanto, reconhecemos que alguns outputs não respeitam o formato solicitado, o que pode ser um ponto que as respostas certas não sejam reconhecidas. Em trabalho futuro, aspiramos incorporar ajustes de parâmetros hiper personalizados para cada modelo aprimorado com LoRa, potencialmente amplificando o desempenho global. Além disso, pretendemos utilizar validação cruzada k-fold para derivar resultados mais autorizados e comparáveis à literatura existente.
Quanto à ensemblagem, vislumbramos ampliar nosso escopo explorando prompts alternativos projetados utilizando técnicas de manipulação de prompts. Complementarmente, buscamos desenvolver rotinas de pós-processamento que acomodem flutuações menores no formato de saída, minimizando assim falsos negativos e melhorando indicadores de desempenho. Testar outros LLMs multilíngues (por exemplo, LLaMA2) e trializar mecanismos de recuperação, como Aprendizado em Contexto, são outras possibilidades para melhorar os resultados. Outra abordagem interessante para o futuro é comparar nossos resultados com estratégias de ensemblagem clássicas, como bagging [8] e boosting [15]. Esquemas de fusão simples como média e seleção máxima podem fornecer vias promissoras merecendo investigação nesse domínio dinâmico. Agradecimentos. Este trabalho foi parcialmente financiado pela Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Código de Financiamento 001. Também agradecemos apoio financeiro da agência de financiamento brasileira CNPq.