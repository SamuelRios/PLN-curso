Hipótese de Ajuste de Criação: Combinando Espaços Discretos e Contínuos para Detecção de Discurso de Ódio Zero-Shot

Lorenzo Puppi Vecchi1[0009−0003−9483−2026], Sylvio Barbon Junior2[0000−0002−4988−0702], e Emerson Cabrera Paraiso1[0000−0002−6740−7855] 1 Programa de Pós-Graduação em Informática - Pontifícia Universidade Católica do Paraná - Brasil {lorenzo.vecchi,paraiso}@ppgia.pucpr.br 2 Departamento de Engenharia e Arquitetura - Universidade de Trieste - Itália sylvio.barbonjunior@units.it Resumo. A detecção de discurso de ódio é um esforço crucial para manter a segurança dos espaços online, mas a eficácia dos abordagens supervisionadas depende principalmente da disponibilidade de dados rotulados. Pesquisas anteriores exploraram a utilidade de modelos de inferência de linguagem natural (NLI) para detecção de discurso de ódio zero-shot (ZSHSD), que aproveita a capacidade desses modelos para aprender relações semânticas e se adaptar a tarefas downstream sem depender de grandes conjuntos de dados rotulados. Os modelos NLI avaliam se uma sentença de premissa lógicamente implica uma sentença de hipótese, dependendo do design preciso de hipótese para alcançar desempenho adequado em tarefas downstream. Marcas de referência que usam modelos NLI para ZSHSD se baseiam em múltiplas inferências com hipóteses diferentes para extrair características e alcançar resultados desejáveis. Diante dos desafios envolvendo ZSHSD e o método de designar hipóteses com tokens discretos, objetivamos otimizar e identificar representações intermediárias ideais aplicando técnicas de ajuste p-tuning. Em HateCheck, uma hipótese completamente ajustada levou a uma melhoria de 18,8 pontos percentuais (pp) na precisão, em comparação com uma hipótese designada discretamente. Nossa obra superou o trabalho anterior ao alcançar uma melhoria de 5,6 pp na precisão, superando abordagens anteriores que requeriam múltiplas inferências. Além disso, os tokens otimizados revelam relações a aspectos mais amplos do discurso de ódio, oferecendo insights para o design de hipóteses.
Palavras-chave: Discurso de ódio · Inferência de Linguagem Natural · Classificação de Texto Zero-Shot · P-tuning

1 Introdução Recentemente, houve um aumento no conteúdo prejudicial, como discurso de ódio e desinformação, na internet. Este aumento pode não ser apenas consequência do uso mais amplo de blogs e mídias sociais, mas também pode resultar da polarização política global e de algoritmos de mídias sociais que amplificam conteúdo divisivo para aumentar a engajamento.

2 Lorenzo Puppi Vecchi et al. Fig. 1. Combinando hipóteses padrão e tokens virtuais p-tunados

Isso torna a segurança online e a moderação de conteúdo crucial para manter a civilidade e proteger os usuários (19; 9). Antes, a detecção de discurso de ódio se baseava principalmente em modelos de classificação que necessitavam de grandes quantidades de dados rotulados (21; 2). Recentemente, modelos de Inferência de Linguagem Natural (NLI), que são usados em tarefas como classificação zero-shot (7), estão sendo explorados. Os modelos NLI apresentam resultados promissores para a detecção de discurso de ódio zero-shot (ZSHSD) porque capturam relações entre frases, o que ajuda a adaptar a moderação de conteúdo ao mundo online em constante mudança. Em nosso estudo, estamos usando o termo "discurso de ódio" para abranger um amplo espectro de conteúdo ofensivo criado por usuários. Escolhemos este termo porque reflete comunicação ofensiva que espalha hostilidade através de estereótipos (15). Em modelos NLI, gerar hipóteses é crucial e afeta tarefas como ZSHSD (20). Mesmo pequenas alterações na forma como uma hipótese é formulada podem ter um grande impacto no desempenho do modelo. Por exemplo, compare "O aquecimento global é um problema global pressante" com "É necessário cooperação global urgente para abordar o aquecimento global". Ambas as declarações expressam preocupação com o aquecimento global, mas a segunda destaca a necessidade de cooperação global, o que pode levar a conclusões e resultados diferentes em ZSHSD. Para lidar melhor com a complexidade de formar hipóteses, os frameworks atuais que utilizam modelos NLI adotam abordagem passo a passo.
Eles utilizam múltiplas hipóteses para entender diferentes aspectos de texto, como identificar discurso de ódio em uma tweet. Por exemplo, uma hipótese pode detectar linguagem explícita ("Essa tweet contém palavras ofensivas"), enquanto outra profundiza nos significados implícitos ("O tom dessa tweet encoraja hostilidade"). Este método ajuda o modelo a compreender vários aspectos do texto, o que é crucial para o desempenho preciso da detecção de discurso de ódio sem treinamento (7). Tradicionalmente, abordagens utilizavam hipóteses simples como "Isso é discurso de ódio", que, embora seja algo eficaz, muitas vezes simplifica demais a complexidade da identificação de discurso de ódio. Essas hipóteses básicas lutam para capturar a ampla gama de nuances, sutilezas e variações dependentes do contexto do discurso de ódio (13; 7). Isso levou a uma crescente reconhecimento da necessidade de geração de hipóteses mais sofisticadas na detecção de discurso de ódio sem treinamento. Em nosso trabalho, objetivamos abordar as dificuldades relacionadas à geração de hipóteses em modelos de inferência de linguagem natural. Introduzimos um método que otimiza a criação de hipóteses por meio de um processo chamado "p-tuning" (10), que começa com uma prompt de hipótese padrão, mas a refina iterativamente para se adaptar às complexidades da detecção de discurso de ódio. O p-tuning permite que adaptemos a hipótese às características específicas e nuances do task, garantindo que ela encapsule uma gama mais ampla e representativa de expressões de discurso de ódio. Objetivo geral: Este artigo visa abordar o desafio da geração de hipóteses para inferência de linguagem natural quando usada para classificação zero-shot. Ele utiliza o "p-tuning" como método para otimizar a criação de hipóteses para abordar as nuances envolvidas na detecção de discurso de ódio. Nesse processo, começamos com uma hipótese inicial (H), escolhendo a mais precisa dentre diferentes sentenças de hipótese projetadas para detectar discurso de ódio usando NLI. Os resultados podem ser vistos na Tabela 2.
A seguir, a hipótese otimizada (OH) é gerada, que começa a partir da melhor hipótese realizada por H e é otimizada usando p-tuning. Diferentes estratégias de uso dos tokens virtuais resultantes podem ser vistas na Tabela 4 e Tabela 5. Finalmente, a hipótese combinada (CH) é formada pela integração dos tokens mais relevantes da OH com a hipótese original H. A precisão de diferentes tokens virtuais inseridos na hipótese original pode ser vista na Tabela 4 e Tabela 5, e um benchmark geral, comparando com trabalhos anteriores, pode ser visto na Tabela 6. Um esquema visual desse conceito é ilustrado na Figura 1. Nossos achados destacam a eficácia da nossa hipótese otimizada (OH), pois certos tokens virtuais dentro do espaço de embedding contínuo aproximam-se estreitamente de conceitos mais amplos de discurso de ódio. Isso sugere que incluir esses tokens virtuais (OH) na formulação da hipótese original (H) possa melhorar a capacidade do modelo para detectar e abordar o discurso de ódio de forma mais abrangente. Isso abre três vias promissoras para pesquisas futuras: Primeiro: os pesquisadores podem usar diretamente os pesos de tokens virtuais ao lado de RoBERTa Large MNLI para melhorar seus modelos de ZSHSD3. Segundo: eles podem empregar seleção de tokens de modelo de nosso estudo para adaptar seus modelos para classificação melhorada. Por fim: nossa pesquisa encoraja a criação de hipóteses com base nos conceitos médios, ampliando a aplicabilidade do método.
Vários abordagens para detecção de discurso de ódio foram exploradas, incluindo modelos de aprendizado de máquina tradicionais como Regressão Logística com N-gram de caracteres (8), Máquinas de Vetores de Suporte (17) e redes superficiais com embeddings pré-treinados como MLP com Codificação de Pares de Bytes (BPE) (22). Geralmente, esses modelos mais simples sobrepõem-se em relação a redes neurais profundas. Antes de 2019, redes neurais recorrentes eram comumente usadas como modelos SOTA (8). A introdução do BERT (Representações de Codificação Bidirecional a partir de Transformadores) por (3) marcou um avanço significativo, estabelecendo o BERT e suas variantes como o novo SOTA na detecção de discurso de ódio (6; 4). Embora o BERT e seus derivados tenham melhorado na captura de informações contextuais e nuances linguísticas, nosso estudo se concentra em modelos de Inferência de Linguagem Natural (NLI). Esta escolha se baseia na razão de que o aprendizado lógico explícito através da entailed textual pode reduzir bias e melhorar a reconhecimento de comunidades sociais (12). Os modelos NLI se destacam na detecção de discurso de ódio ao aproveitar o treinamento lógico, o que melhora os sistemas de detecção de discurso de ódio zero-shot com contexto (ZSHSD). Diferentemente dos encoders de sentenças pré-treinados que podem perpetuar estereótipos, os modelos NLI ajudam a mitigar o bias presente nos dados de treinamento (14). O uso de modelos NLI para tarefas preditivas envolve transformar a tarefa-alvo em um formato NLI, o que é essencialmente uma tarefa de fine-tuning. O modelo avalia uma premissa e uma hipótese para determinar sua relação lógica - se a premissa implica, contradiz ou é neutra em relação à hipótese. Um método recente aplica modelos NLI para classificação de tópicos zero-shot, onde o texto de entrada serve como premissa e cada tópico tem uma hipótese correspondente como "Este texto é sobre <tópico>". Aqui, os rótulos "neutro" e "contradição" se fundem em "não-entailed", significando que uma previsão de "entailed" indica relevância para o tópico, enquanto "não-entailed" sugere irrelevância.
É importante notar que uma previsão de implicação indica alinhamento com certos artefatos de modelo, não necessariamente corretude (25). 3 Abordagem Proposta Nossa metodologia se concentrou em identificar a hipótese mais eficaz para avaliar a existência de discurso de ódio em um dado contexto. Em vez de iniciar o processo de otimização a partir de pesos aleatórios, utilizamos os resultados mais bem-sucedidos obtidos em experimentos anteriores, como visto na Tabela 2. Este ponto de partida estratégico visava acelerar o processo de otimização e construir sobre conhecimento existente. 3.1 Otimização de Hipóteses No âmbito dos modelos de NLI, uma hipótese é uma sentença, que é codificada em uma sequência de tokens. Cada token é representado como um inteiro e, após passar pelo layer de embedding do modelo, cada token se torna um vetor. Ao trabalhar com p-tuning, em vez de alterar a sentença de hipótese inicial, os vetores contínuos são otimizados diretamente. Esta otimização cria o que é chamado de "tokens virtuais", pois não perfeitamente se assemelham a um token propriamente dito. Criação de Hipóteses Tunadas para Detecção de Discurso de Ódio Zero-Shot 5 Isso significa que o algoritmo aprende a ajustar a representação de vetor contínua da hipótese inicial para melhorar seu desempenho na detecção de discurso de ódio zero-shot. Um esquema geral do processo utilizado para otimizar a hipótese inicial H e testar diferentes combinações de tokens de hipótese pode ser visto na Tabela 2. Denominamos H como vetores de hipótese inicial (selecionados dos melhores resultados da Tabela 2) e OH como vetores de hipótese otimizados. O processo de aprendizado envolve ajustar os pesos θ do modelo para encontrar um vetor de hipótese otimizado OH que minimize uma função de perda específica, tipicamente associada à detecção de discurso de ódio zero-shot. Matematicamente, isso pode ser representado como: OH = argminθL(P, H, θ) (1) Aqui, L representa a função de perda, P denota o contexto específico da tarefa, H significa os vetores de hipótese inicial e θ simboliza os pesos do modelo.
O objetivo é encontrar o conjunto ótimo de pesos que minimiza a função de perda, produzindo assim os vetores de hipótese otimizados OH. Os modelos de NLI tipicamente apresentam uma camada softmax no final, que serve para produzir três saídas primárias denotando as relações potenciais entre um dado premissa P e hipótese H: contradição, neutralidade e implicação. Essas saídas, semelhantes a probabilidades, são normalizadas para garantir que a soma coletiva seja igual a 1, indicando efetivamente a probabilidade de cada relação possível. O objetivo do treinamento de um modelo de NLI é minimizar o erro na previsão dessas probabilidades, garantindo que o modelo identifique corretamente a relação lógica entre a premissa e a hipótese. Isso é tipicamente alcançado através do uso de uma perda de entropia cruzada categórica L, definida como: L = − ∑3 c=1 yi log(pi) (2) No contexto da detecção de discurso de ódio binário, a função de perda nesse caso é otimizada para aumentar a saída de implicação quando há ódio presente, ou seja, c = 3, e maximizar a saída de contradição quando não há ódio presente, ou seja, c = 1. Dado que c = [1, 2, 3] representa, respectivamente, contradição, neutralidade e implicação. Após otimizar nossas hipóteses, procedemos com uma sequência estruturada de experimentos para avaliar tanto o desempenho individual quanto combinado dos tokens otimizados. Esses experimentos foram projetados para avaliar a eficácia do nosso abordagem em contextos diversificados, iluminando assim sua capacidade de discernir discurso de ódio. Em nosso estudo, utilizamos o modelo RoBERTa Large4 (11), que havia sido fine-tunado em uma variedade de fontes de texto, incluindo Wikipedia, o Corpus do Livro e o conjunto de dados MultiNLI. O treinamento desse modelo foi realizado em um GPU NVIDIA 4090 RTX, durando aproximadamente 12 horas, com um regime de treinamento que se estendeu por 20 épocas.
Para otimização, utilizamos o otimizador AdamW e implementamos um cronograma de taxa de aprendizado linear de decréscimo, estabelecendo a taxa de aprendizado inicial em 1e-3. É digno de nota que nosso processo de treinamento requereu significativamente menos tempo e recursos computacionais em comparação ao trabalho anterior (19). Notadamente, otimizamos nossa abordagem para alcançar a utilização eficiente de recursos, empregando p-tuning exclusivamente na hipótese, em contraste com as metodologias tradicionais de fine-tuning do modelo inteiro. Essa adaptação estratégica provavelmente contribuiu para o tempo e recursos computacionais reduzidos necessários para nosso processo de treinamento.

3.2 Análise de Similaridade de Hipótese

A nossa fase inicial envolveu a análise da hipótese otimizada e a avaliação de sua similaridade com os tokens originais antes da otimização. Isso nos permitiu avaliar o grau até o qual nossas otimizações retiveram o sentido fundamental do texto original. Medimos essa distância utilizando a similaridade cosina com as representações de embeddings do modelo base.

3.3 Avaliação da Importância de Token

Na segunda fase, nos aprofundamos na importância de cada token otimizado individualmente. Examinamos os resultados da utilização desses tokens em três maneiras distintas: isoladas (uma a uma), inseridas individualmente e cumulativamente. Seguindo a ideia da CH apresentada na Figura 1.

Isoladas (Nível de Token): No experimento "Isolate", isolamos um token por vez da hipótese otimizada. O objetivo principal foi determinar se os tokens isolados apresentaram desempenho melhorado, especialmente em relação a termos associados à linguagem de ódio, como "odioso" e "ofensivo".

Individual (Inserção Contextual): No experimento "Individual", uma vez que os tokens foram otimizados nesse novo espaço contínuo, são reinsertados na estrutura de hipótese padrão. Isso significa substituir os tokens originais por suas representações ajustadas na hipótese.
Vamos supor que o token "odioso" seja inicialmente representado como [0,2, 0,5, -0,3] e, após ajuste, torna-se [0,8, -0,1, 0,7]. Após reinsertar os tokens ajustados, a hipótese pode parecer com isso: "essa sentença contém [odioso] entradas ou linguagem ofensiva" "essa sentença contém [odioso] entradas ou [ofensiva] [linguagem]" Aqui, [odioso], [ofensiva] e [linguagem] representam as versões transformadas dos tokens originais "odioso", "ofensiva" e "linguagem". A motivação por trás dessa abordagem é aproveitar uma representação mais refinada e consciente do contexto das termos-chave na hipótese. Ao fazê-lo, esse estudo visa medir se essa compreensão aprimorada de pistas linguísticas específicas pode levar a desempenhos melhorados na detecção de discurso de ódio como tarefa de NLI. A ideia é que as representações ajustadas possam capturar sutilezas e contexto melhor do que os tokens originais estáticos. Aditivo (Inserção Sequencial): O experimento "Aditivo" envolveu a adição incremental dos tokens otimizados na ordem natural da sentença. Embora tenhamos previsto um melhoramento geral, o objetivo aqui foi analisar picos de desempenho correspondentes à introdução de cada novo token otimizado. Esse passo forneceu insights sobre o impacto cumulativo do nosso método no ZSHSD. Criação de Hipótese para Detecção de Discurso de Ódio Zero-Shot 7 3.4 Banco de Dados Na fase final de nossa experimentação, nosso foco mudou para análise comparativa. Especificamente, procuramos avaliar a eficácia da nossa metodologia, que incorporou a hipótese otimizada e os tokens, em comparação com bancos de dados estabelecidos apresentados em estudos anteriores (19; 16; 7). Conjunto de Dados O conjunto de dados utilizado nesse estudo é composto por duas fontes distintas, cada uma contribuindo valiosos insights para o ZSHSD. Em agregado, o conjunto de dados empregado em nossa pesquisa compreende 40.772 instâncias. Ele consiste em 22.571 instâncias rotuladas como "odioso" e 18.201 instâncias categorizadas como "não-odioso".
Este conjunto de dados diverso e multifacetado serve de base para nossas explorações sobre detecção de discurso de ódio, permitindo-nos analisar e avaliar o desempenho do método em uma ampla variedade de cenários reais de discurso de ódio. Tabela 1. Conjunto de dados - Fontes de dados e número de exemplos

Hateful Non-hateful Total
Aprendizado com o Pior: Treinamento 17.740 15.184 32.924
Aprendizado com o Pior: Teste 2.268 1.852 4.120
HateCheck: Teste 2.563 1.165 3.728
Total 22.571 18.201 40.772

O Aprendizado com o Pior (19) é um processo para criar conjuntos de dados para treinar modelos de detecção de discurso de ódio melhores. Ele utiliza tanto humanos quanto modelos. O conjunto de dados tem 37.044 instâncias: 20.008 são rotuladas como "ódio", e 17.036 são "não ódio". Seguimos o setup dos autores, treinando o modelo nas quatro rodadas disponibilizadas pelo conjunto de dados (R1, R2, R3 e R4) e testando em cada divisão de rodada separadamente.

HateCheck (16) é uma suite de testes para avaliar modelos de detecção de discurso de ódio. Ele tem 29 funções de teste baseadas em pesquisas anteriores e em input da sociedade civil. O conjunto de dados contém 3.728 instâncias: 2.563 são "ódio", e 1.165 são "não ódio". Usamos as pontuações de precisão do modelo mais acurado para testes individuais com base no trabalho original (16). Para o desempenho médio melhor, usamos a precisão de "Todas" as hipóteses diferentes combinadas, como proposto por (7).

4 Resultados

Através da avaliação da hipótese otimizada e representações de token, bem como seu impacto no desempenho do ZSHSD, buscamos fornecer uma visão detalhada sobre as capacidades e limitações de nossa abordagem.

4.1 Hipótese de discurso de ódio Zero-Shoot

Nesta hipótese de teste, empregamos um modelo de NLI para avaliar a eficácia de várias hipóteses em medir discurso de ódio e linguagem ofensiva. A Tabela 2 apresenta os resultados, incluindo o desempenho médio. A hipótese melhor, com um resultado de precisão média de 0,669, foi "essa sentença contém entradas ódiosas ou linguagem ofensiva". Este resultado sugere que...
explicitamente abordando entradas odiosas e linguagem ofensiva na hipótese captura efetivamente um espectro mais amplo de conteúdo prejudicial. O modelo destaca-se aqui porque não precisa fazer uma distinção rigorosa entre os dois tipos de conteúdo, o que pode ser desafiador dada a sua sobreposição nuances. Realizamos este breve experimento utilizando a divisão de teste "Aprendizado com o Pior" e "HateCheck" dos conjuntos de dados. Tabela 2. Teste de Hipótese (Os resultados mais bem-sucedidos estão destacados)

Contém entradas odiosas ou linguagem ofensiva nessa sentença 0,597 0,741 0,669
Essa sentença contém linguagem ofensiva 0,601 0,728 0,665
Essa sentença contém entradas odiosas ou linguagem ofensiva 0,603 0,727 0,665
Essa sentença contém linguagem ofensiva 0,600 0,728 0,664
Essa sentença contém entradas odiosas 0,580 0,745 0,663
Essa contém discurso de ódio 0,590 0,728 0,659
Essa contém discurso de ódio 0,590 0,727 0,658
Essa sentença contém entradas odiosas 0,568 0,737 0,653
Ódio 0,566 0,728 0,647
Linguagem ofensiva 0,569 0,714 0,641
Discurso de ódio 0,564 0,707 0,635
Ódio 0,524 0,678 0,601
Média 0,579 0,724 0,652

A hipótese pior, com um resultado médio de precisão de 0,601, foi "Ódio". Este resultado pode ser atribuído à brevidade extrema e à falta de contexto na hipótese. Ela não fornece informações suficientes para o modelo classificar corretamente o discurso de ódio ou linguagem ofensiva. Além disso, o termo "ódio" sozinho é altamente ambíguo, tornando difícil para o modelo fazer previsões precisas. Esses resultados mostram que o desempenho do modelo NLI no ZSHSD está estreitamente ligado à clareza e especificidade da hipótese. Hipóteses que abarcem um espectro mais amplo de conteúdo prejudicial e forneçam contexto tendem a produzir resultados melhores. Por outro lado, hipóteses muito breves ou ambíguas podem dificultar a capacidade do modelo de fazer classificações precisas, como demonstrado por esses resultados.
Portanto, é imperativo criar hipóteses cuidadosamente que abrança a diversidade e o contexto do discurso de ódio para melhorar a eficácia dos modelos de ZSHSD baseados em NLI. 4.2 Análise de Similaridade de Hipóteses Na tabela 3 abaixo, apresentamos as representações de token originais ao lado de suas contrapartes otimizadas. Vamos examinar as mudanças em cada token e discutir por que a versão otimizada pode ser mais ou menos eficaz para prever discurso de ódio. Além disso, vamos destacar os tokens que são provavelmente mais úteis para essa tarefa. É importante notar que essas transformações ocorrem dentro de um espaço de embedding contínuo, portanto, a similaridade dos tokens otimizados com palavras específicas não captura plenamente as sutilezas de sua representação. Além disso, mesmo que tokens pareçam menos interpretáveis à perspectiva humana, podem ainda impactar os pesos internos do modelo de maneira que possa afetar positiva ou negativamente sua performance na tarefa. Ajuste da Criação de Hipóteses para Detecção de Discurso de Ódio Zero-Shot 9 essa - A versão otimizada adiciona caracteres e símbolos únicos, como "[...]" e " ", que podem melhorar a capacidade do modelo para detectar discurso de ódio nuanciado ou ironia. No entanto, torna mais difícil a interpretação direta. sentença - A versão otimizada inclui palavras não relacionadas como "edits", "in-scrição" e "subtítulos", que não parecem ajudar com ZSHSD, mas sim se dirigem a termos não relacionados. contém - A versão otimizada contém caracteres e sequências aleatórias ("pha", "nton", etc.) que não se alinham com o significado original do token. Essa transformação não se relaciona com ZSHSD. odioso - A versão otimizada mantém "odioso" e "despicable" enquanto adiciona termos como "fascista" e "discriminatório", ampliando sua capacidade para detectar discurso de ódio com termos mais discriminatórios e extremistas. entradas - A versão otimizada introduz termos não relacionados ("ansk", "owitz", etc.)
Aqui está a tradução do texto científico para português do Brasil:

Os tokens otimizados adicionam termos como "antagonista", "favoravelmente" e "extremistas", que não estão diretamente relacionados ao token original, mas podem ajudar a identificar discurso de ódio envolvendo comparações, contraste ou pontos de vista extremistas. Ofensivo - A versão otimizada inclui termos como "transsexual", "LGBT" e "étnico", que são relevantes para detectar discurso de ódio envolvendo temas sensíveis e linguagem discriminatória. Linguagem - A versão otimizada introduz termos não relacionados ("andra", "eto", etc.) que não contribuem positivamente para o ZSHSD. A avaliação dos tokens otimizados e sua contribuição para o ZSHSD pode variar significativamente dependendo do modelo de NLI específico e seus objetivos. A análise fornecida aqui representa uma avaliação humana da relevância dos tokens para o ZSHSD. Embora alguns tokens, como "odioso" e "ofensivo", pareçam fortalecer a capacidade do modelo para identificar discurso de ódio ao introduzir termos relevantes, outros parecem não estar relacionados à tarefa e podem introduzir ruído. Tabela 3. Similaridade de Cosine entre Token Original e Token Otimizado/p-tunado Token Original Token Otimizado/p-tunado ... contém "contém": 1,0 "contém": 0,786 "contendo": 0,752 "contido": 0,727 "pha": 0,221 "nton": 0,217 "pton": 0,204 "beit": 0,199 odioso "odioso": 1,0 "despicable": 0,767 "hideous": 0,761 "vile": 0,755 "odioso": 0,245 "fascista": 0,237 "discriminatório": 0,237 "despicable": 0,233 ... ou "ou": 1,0 "e": 0,724 "Ou": 0,693 "OR": 0,64 "antagonista": 0,195 "favoravelmente": 0,194 "parasita": 0,184 "extremistas": 0,183 ofensivo "ofensivo": 1,0 "Ofensivo": 0,725 "defensivo": 0,68 "ofensivo": 0,654 "transsexual": 0,233 "LGBT": 0,233 "transgênero": 0,232 "étnico": 0,232 ...
54 entradas CH 0,71 0,50 0,43 0,55 CH ou 0,68 0,63 0,38 0,57 CH ofensivo 0,85 0,68 0,49 0,67 CH linguagem 0,87 0,61 0,43 0,63 Média 0,65 0,56 0,43 É fundamental notar que a eficácia da otimização de tokens é dependente do contexto e deve ser avaliada no quadro mais amplo dos objetivos do modelo de NLI. Mais experimentação e ajustes finos podem ser necessários para determinar as representações de tokens mais apropriadas. No final, a escolha de tokens otimizados deve alinhar-se com as nuances e objetivos específicos do problema em questão, e a avaliação deve ser guiada por ambos a julgamento humano e métricas de desempenho quantitativas.

4.3 Avaliação da Importância de Tokens

O desempenho de três métodos diferentes para incorporar tokens otimizados na hipótese para ZSHSD—"Acumulativo", "Individual" e "Isolado"—é apresentado na Tabela 4. As pontuações de precisão sobre todos os dados de teste revelam as contribuições específicas de tokens: "este" e "sentença": Contribuem consistentemente em todos os métodos, com "Individual" mostrando um impacto ligeiramente maior. "Contém": Aumenta significativamente o desempenho no método "Acumulativo", indicando seu valor em melhorar o ZSHSD. "Hateful": Tem um forte impacto no método "Acumulativo", mas diminui nos métodos "Individual" e "Isolado", sugerindo seu efeito menos significativo isoladamente. "Entradas" e "ou": Variam na contribuição, com o método "Acumulativo" mostrando os valores mais altos. "Ou" contribui menos no método "Isolado". "Ofensivo" e "linguagem": Como "hateful", esses tokens são cruciais no método "Acumulativo", mas menos impactantes individualmente. O método "acumulativo" apresenta a melhor melhoria no desempenho ao adicionar tokens sequencialmente, realçando o valor de considerar tokens acumulativamente para melhorar o ZSHSD. No entanto, o impacto de tokens individuais varia e não todos contribuem igualmente. Tabela 5. Tokens Virtuais Otimizados Inseridos na Hipótese Padrão Desempenho de Precisão CH - contém hateful
578 H - hipótese pura 0,665 CH - contém ofensivo 0,698 CH - ofensivo e odioso 0,759 CH - ofensivo ou odioso 0,828 CH - contém ofensivo ou odioso 0,834 CH - contém ofensivo 0,837 OH - otimizado em sua totalidade 0,870 Ajuste de Hipótese para Detecção de Discurso de Ódio sem Treinamento 11 A abordagem "Cumulativa" apresenta o melhor desempenho, destacando a importância de considerar tokens coletivamente para uma detecção melhor de ZSHSD. Para análise adicional, os tokens "contém", "odioso", "ou" e "ofensivo" foram selecionados devido a seus papéis pivais. O método "Individual" foi usado para testes adicionais: "contém": Relacionado diretamente à identificação de discurso de ódio. "Odioso" e "ofensivo": Explicitamente ligados ao discurso de ódio, melhorando o poder discriminatório do modelo. "Ou": Apesar de ser uma conjunção comum, introduz interpretações nuances com termos como "antagonista", "favoravelmente" e "extremistas", demonstrando sua capacidade de capturar nuances variadas de discurso de ódio. A tabela 5 mostra a hipótese "full_optimized" alcançando a maior precisão (0,870), indicando sua eficácia. Combinações incluindo "odioso" e "ofensivo" também exibem alta precisão, próxima à hipótese "full_optimized", sugerindo que uma combinação selecionada de tokens pode alcançar resultados semelhantes. A análise estatística revela uma melhoria significativa do "full optimized" em relação à "hipótese pura". Considerando o primeiro valor e o segundo valor como a significância para "full optimized" e "hipótese pura", respectivamente, entre as combinações analisadas, "contém ofensivo odioso" (0,321 vs. 0,001), "odioso ou ofensivo" (0,216 vs. 0,003) e "contém ofensivo ou odioso" (0,262 vs. 0,003) mostram melhorias significativas quando comparadas à "H - Hipótese Pura". As análises foram conduzidas com um intervalo de confiança de 95%, considerando valores de p abaixo de 0,05 como significativos, confirmando assim a hipótese alternativa.
Tabela 6 apresenta resultados de precisão de testes de discurso de ódio, destacando o desempenho de diferentes configurações de hipóteses, desde hipóteses puras até otimização completa (OH) ... Tokens virtuais inseridos na hipótese padrão CH contém discurso de ódio CH contém ofensivo CH contém discurso de ódio ou ofensivo CH contém discurso de ódio ofensivo OH Trabalho anterior (19) (16) Aprendizado R2 0,519 0,578 0,669 0,805 0,796 0,834 0,779 ... Aprendizado R3 0,635 0,574 0,706 0,75 0,756 0,793 0,768 Check de ódio derog dehum h 0,493 1,0 0,843 1,0 0,993 1,0 0,986 Check de ódio derog impl h 0,229 0,914 0,486 0,964 0,936 0,921 0,85 ... Check de ódio espaço add h 0,769 0,78 0,983 0,919 0,971 0,74 Check de ódio espaço del h 0,738 0,851 0,702 0,901 0,865 0,943 0,801 Check de ódio TOTAL 0,613 0,741 0,725 0,884 0,895 0,929 0,873 Média 0,596 0,672 0,737 0,869 0,88 0,908 12 Puppi Vecchi et al. Observações-chave incluem um aumento da precisão com a adição de tokens otimizados à hipótese pura e uma tendência geral de aumento da precisão de inicial para hipóteses otimizadas, sublinhando a eficácia do procedimento de otimização em melhorar as capacidades do modelo de ZSHSD. Algumas testes mostram que passos intermediários específicos superam outros, indicando a necessidade de adaptar o processo de otimização às diferentes categorias de discurso de ódio. Representações intermediárias, como "Discurso de ódio ou Ofensivo", "Contém Discurso de ódio ou Ofensivo" e "Contém Discurso de ódio Ofensivo", apresentam consistentemente alta precisão em várias testes, reforçando sua importância em melhorar o desempenho. Melhorias notáveis foram observadas quando testadas no conjunto de dados de aprendizado com o pior (19), com ganhos de precisão de 5%.
5 pontos percentuais (pp) e 1,2 pp nos splits de teste R2 e R4, respectivamente. Diferentemente de esforços anteriores que se basearam em modelos de classificação completamente ajustados, nosso abordagem otimiza a hipótese de entrada de um modelo de NLI. Nos dados do Hatecheck (16), tokens otimizados levaram a uma melhoria média de precisão de 7,74 pp, com uma melhoria de melhor cenário de 18,8 pp usando uma hipótese completamente ajustada (OH) em comparação com (H). Nossa hipótese completamente ajustada também superou o trabalho anterior em 5,6 pp. Esses achados destacam a eficácia do procedimento de otimização em melhorar as capacidades do modelo ZSHSD. Ambas as hipóteses completamente ajustadas e inserções significativas de tokens em hipóteses padrão mostraram melhorias na precisão, sugerindo direções futuras para um design mais amplo de hipóteses dentro de modelos de NLI.

5 Conclusão Nossa pesquisa destaca a importância da seleção cuidadosa de tokens e a sinergia entre diferentes aspectos da otimização de hipóteses. Melhorando como as hipóteses são formuladas, podemos melhorar o desempenho dos modelos de NLI em ZSHSD, contribuindo para o trabalho vital da moderação de conteúdo online e segurança digital. Nossos resultados destacaram a importância de tokens específicos como "odioso" e "ofensivo" em melhorar a precisão do modelo. Ao otimizar esses tokens, expandimos a capacidade do modelo para detectar uma ampla gama de linguagem e ideologias odiosas. Por exemplo, a versão otimizada de "odioso" introduziu termos como "fascista" e "discriminatório", enriquecendo o entendimento do modelo sobre ódio e intolerância. Da mesma forma, otimizar "ofensivo" incorporou termos como "transsexual", "LGBT" e "étnico", que são relevantes para a detecção de linguagem ofensiva. Essas adições melhoraram a capacidade do modelo para reconhecer nuances sutis em linguagem discriminatória. O futuro deve se concentrar em expandir e diversificar pools de tokens, adaptar modelos para contextos multilíngues e desenvolver mecanismos de adaptação em tempo real para manter o ritmo com a evolução da linguagem ofensiva.
Aprimorar análise contextual, incorporando retroalimentação do usuário, abordando questões éticas e de viés, são fundamentais para melhorar a precisão e a utilidade prática na moderação de conteúdo online. Declaração de Interesses. Os autores não têm interesses competitivos a declarar que sejam relevantes para o conteúdo deste artigo.