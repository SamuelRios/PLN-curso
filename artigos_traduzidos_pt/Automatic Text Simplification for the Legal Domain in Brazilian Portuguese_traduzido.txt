Simplificação de Textos Automática para o Domínio Jurídico no Português Brasileiro

Francielle Vasconcellos Pereira1, Ana Frazão2 e Viviane P. Moreira1

1 Instituto de Informática, UFRGS Porto Alegre, Brasil {fvpereira, viviane}@inf.ufrgs.br
2 USP São Paulo, Brasil anarosapaiva@usp.br

Resumo. Documentos jurídicos e legais, como sentenças, leis, acordos e contratos, contêm termos e jargões específicos do domínio, frases longas e complexas que podem ser difíceis de entender para pessoas leigas sem expertise no domínio, com problemas de leitura ou com baixo nível de educação. A simplificação desses documentos tem sido um concernente há vários anos, visando a democratizar o acesso à justiça. Os tribunais já estão adotando linguagem mais simples, especialmente em documentos destinados a pessoas leigas, como mandados e notificações, para melhorar a inclusão e a clareza. A simplificação de textos automáticos, um subcampo do Processamento de Linguagem Natural, busca tornar textos complexos mais acessíveis. Este artigo explora a tarefa de simplificação de textos automáticos em português para o domínio jurídico. O principal desafio aqui é a falta de conjuntos de dados contendo frases complexas e suas versões simplificadas. Este trabalho investiga como os conjuntos de dados, métodos e métricas utilizados para a simplificação de textos se comportam quando aplicados a textos legais em português. Presentamos análises qualitativas e quantitativas utilizando cinco modelos. Os resultados mostram que os modelos baseados em GPT têm os melhores resultados, mas a fine-tuning com dados do domínio é uma alternativa viável e de código aberto.

Palavras-chave: Simplificação de Textos Automática · Textos Legais · Processamento de Linguagem Natural · Linguagem Clara.
Quando se trata de documentos forenses, é necessário ter termos específicos para entender os fatos sem deixar margem para interpretações diferentes. A preocupação com a simplificação do direito tem sido um tema em debate há alguns anos. A necessidade de democratizar o Judiciário brasileiro, ampliando o acesso à justiça, foi levantada por alguns estudos [1,4,18]. No entanto, eles se concentraram em defender que os documentos sejam escritos e publicados de forma simplificada. Escrever simplesmente sem perder o essencial do texto que a lei exige é uma prática que precisaria ser amplamente disseminada e incentivada em todos os tribunais do país. Para incentivar os tribunais a usar linguagem clara, o CNJ (Conselho Nacional de Justiça) lançou o Pacto Nacional do Judiciário pela Linguagem Clara e, por meio da Portaria nº 351/2023, o Selo de Linguagem Clara. Seu objetivo é reconhecer e promover, em todos os segmentos do Judiciário e em todos os níveis de jurisdição, o uso de linguagem clara e compreensível na elaboração de decisões judiciais e em comunicação geral com a sociedade.4 Isso é um processo prolongado que pode levar anos para se materializar, ainda assim, pode beneficiar-se de avanços resultantes do uso de ferramentas automáticas para simplificação de texto. Ao longo do tempo, o português brasileiro evoluiu para exibir características distintas não típicas do português europeu. O Brasil, com seu vasto território e diversidade cultural, enfrentou desafios no desenvolvimento de seu quadro jurídico. A incorporação de influências das tradições jurídicas romano-germânica, francesa e portuguesa ocorreu no contexto de adaptação às especificidades da sociedade brasileira. Consequentemente, o sistema jurídico brasileiro representa uma fusão dessas tradições jurídicas e uma abordagem adaptativa para questões locais. Essas características influenciam sem dúvida práticas de escrita jurídica, afetando formatos de documentos, hermenêutica e argumentação de várias maneiras [10, Capítulo 26].
O desafio abordado nesse trabalho é como simplificar automaticamente textos escritos em português brasileiro no domínio jurídico. A simplificação de textos (TS) é um subcampo do Processamento de Linguagem Natural (NLP). Busca traduzir textos complexos para um idioma mais simples, visando à acessibilidade. Existem várias razões para simplificar textos, como cobrir a compreensão de falantes não nativos, pessoas com um nível mais baixo de educação ou deficiências cognitivas. Aplicamos Modelos de Linguagem Grande (LLM) para gerar simplificações a partir de uma sentença complexa de um texto jurídico. Experimentamos com diferentes abordagens, incluindo a fine-tuning de um modelo T5 e estratégias de prompting. Além disso, melhoramos um modelo fine-tunado com aprendizado por reforço (RL) para avaliar se isso poderia melhorar os resultados de simplificação. A avaliação da TS ainda é um problema aberto. Não há um único métrica que possa avaliar precisamente todas as nuances envolvidas na simplificação de um texto. Como resultado, esse trabalho fornece uma análise quantitativa utilizando quatro métricas de avaliação. Também realizamos uma análise qualitativa, que é crucial para permitir insights nos simplificações geradas. As principais contribuições desse trabalho podem ser resumidas da seguinte forma: 1. uma avaliação de cinco LLMs aplicadas à simplificação de textos no domínio jurídico brasileiro; 2. uma avaliação quantitativa da qualidade da simplificação utilizando quatro métricas de avaliação; e 3. uma avaliação qualitativa dos outputs do modelo por um especialista humano. 3 https://www.cnj.jus.br/gestao-da-justica/acessibilidade-e-inclusao/pacto-nacional-do-judiciario-pela-linguagem-simples/ 4 https://www.cnj.jus.br/gestao-da-justica/acessibilidade-e-inclusao/pacto-nacional-do-judiciario-pela-linguagem-simples/selos/ Simplificação de Textos Automaticamente para o Domínio Jurídico no Português Brasileiro 3 Os nossos achados mostraram que os modelos GPT ainda têm os melhores resultados para a TS, mesmo nesse domínio específico. O uso do RL trouxe melhorias na performance do modelo fine-tunado.
Além dos modelos GPT, usar apenas instruções e exemplos não é suficiente para obter resultados bons. 2 Trabalhos Relacionados Documentos legais têm termos complexos e específicos em qualquer língua. Uma compilação de conjuntos de dados existentes, métodos e métricas foi investigada para a simplificação de texto (TS) em textos legais [11]. Eles relataram os resultados em termos de legibilidade, simplicidade, similaridade, presença de alucinações e fluência de métodos. Sua análise mostrou que a maioria dos métodos se concentra em split-and-rephrase, transformando sentenças maiores e mais complexas em sentenças mais curtas e simples. Em geral, a pesquisa em línguas diferentes do português inclui métodos diversificados, como tradução por máquina (supervisionada e semisupervisionada). O primeiro trabalho relevante em TS usando um método supervisionado empregou métricas como recompensas em um algoritmo de RL chamado DRESS (Deep REinforcement Sentence Simplification) [27]. Outro trabalho se concentrou na substituição de palavras complexas e na divisão de sentenças. Eles superaram outros métodos não supervisionados para TS em diferentes domínios em inglês [6]. Mais recentemente, o BLESS (Benchmarking Large Language Models on Sentence Simplification) avaliou 44 modelos para TS em três conjuntos de dados (Wikipedia, notícias e médico). Eles mostraram que alguns modelos pré-treinados, mesmo não treinados para TS, podem desempenhar bem em comparação ao MUSS [15] estado-da-arte escolhido como baseline no artigo [13]. Não há consenso sobre o que constitui linguagem clara, mas há princípios que orientam a prática da TS. O livro básico da teoria da Linguagem Clara [8], estabelece 25 diretrizes capazes de garantir a clareza do texto. Os Tribunais de Justiça Brasileiros estão trabalhando para implementar a linguagem clara tanto em documentos textuais quanto em serviços pessoais fornecidos aos cidadãos. Essas iniciativas visam garantir que a informação seja transmitida de forma clara e objetiva sem a necessidade de recorrer a terceiros (como advogados). Os principais alvos são documentos que atingem pessoas leigas, como mandados de busca e citações.
5 Existem poucos recursos linguísticos para o Tratamento de Simplificação (TS) em português [9,12,16], e algumas contribuições recentes para Processamento de Linguagem Natural (NLP) e domínio jurídico [19,23]. No entanto, na maioria dos casos, os abordagens em português são limitadas apenas à simplificação lexical, não abrangendo a sintaxe integral do texto. Métodos lexicais em TS são restritos a lidar com palavras complexas em sentenças. Simplesmente mudar palavras complexas não é suficiente para simplificar sentenças jurídicas. Mesmo em inglês, há uma falta de dados disponíveis para TS no domínio, e os métodos sintáticos se concentram em dividir sentenças em menores unidades. Uma abordagem que considera a sintaxe integral das sentenças em português foi desenvolvida no PorSimples [14]. O projeto começou em 2009 e, em 2010, um sistema baseado em regras foi desenvolvido e tornou-se disponível, embora não esteja mais sendo mantido. Os autores também contribuíram com o único conjunto de dados paralelo em português contendo sentenças originais e simplificadas. Um trabalho recente apresentou métodos de Aprendizado de Máquina para TS em resumos judiciais de dois tribunais brasileiros [3]. No entanto, eles apenas relataram resultados de métricas de legibilidade e essas não são comumente usadas para avaliar sistemas de TS. Este trabalho complementa os estudos anteriores ao avaliar o TS em cinco modelos de Linguagem Larga (LLMs) aplicados a textos jurídicos, comparando-os com documentos simplificados por especialistas humanos em tribunais brasileiros. É incerto como modelos com arquiteturas diferentes podem se comportar em português. O domínio jurídico apresenta dificuldades e falta de dados, mesmo em inglês. Nossa intenção é responder se um conjunto de dados para TS em português é suficiente para generalizar e permitir simplificar textos jurídicos, e como alguns modelos podem realizar essa tarefa em domínio específico. Até o melhor de nossa conhecimento, este é o primeiro trabalho a avaliar o domínio em português usando métricas que consideram sentenças de referência.
3 Materiais e Métodos Esta seção descreve os passos macro para simplificação de texto automática que adotamos nesse trabalho para responder às seguintes perguntas de pesquisa: RQ1 Os principais conjuntos de dados para simplificação de texto e principais modelos pré-treinados em português são capazes de generalizar e ser aplicados ao domínio jurídico? RQ2 Como bem os modelos de linguagem de ponta e recentemente lançados se saem em simplificar texto no domínio e língua escolhidos? RQ3 O reforço por aprendizado profundo pode melhorar os resultados de simplificação de texto para modelos fine-tuned? A Figura 1 apresenta um panorama geral da nossa pipeline. Nossa entrada vem de três fontes de dados que foram combinadas em um conjunto de dados mergeado. Esse conjunto de dados é então usado de diferentes maneiras: (i) para fornecer dados de treinamento para fine-tuning um modelo baseado em Transformer e (ii) enriquecer prompts enviados a modelos geradores. Os resultados produzidos pelos modelos são avaliados contra as sentenças simplificadas de referência.

Fig. 1. Pipeline com os passos da nossa metodologia. Simplificação de Texto Automática para o Domínio Jurídico em Português Brasileiro 5 3.1 Montagem de um Conjunto de Dados para Simplificação de Texto em Português As instâncias de entrada para treinamento e avaliação de sistemas de simplificação de texto consistem em uma dupla ⟨O, R⟩, onde O é a sentença original (complexa) e R é sua versão simplificada. Infelizmente, não há um conjunto de dados assim para o domínio jurídico em português. Como resultado, montamos um conjunto de dados para treinamento e avaliação de modelos de simplificação de texto, combinando dados de fontes diferentes (alguns no domínio jurídico e alguns genéricos). As seguintes três fontes de dados foram usadas. PorSimples [14] criou quatro linhas de base referidas como PorSimplesSent. Eles contêm um corpus paralelo com dois níveis de simplificação, natural e forte. Os textos foram extraídos de artigos de notícias brasileiros.
Cada par ⟨O, R⟩ é rotulado de acordo com o nível de simplificação, que pode ser N (simplificação natural) ou S (simplificação forte). A diferença entre eles é o grau de operações realizadas nas sentenças originais. No N, a divisão e a inversão da ordem de cláusulas são aplicadas discretamente, e no S, todas as regras mapeadas são aplicadas na simplificação (reescrita e substituição lexical, mudança para ordem canônica, mudança para voz ativa, reordenação, divisão, junção e eliminação). Todas as combinações dos pares nesse conjunto de dados foram usadas no passo de fine-tuning, totalizando 8.120 pares. As combinações são O→N, O→S e N→S, onde o texto à esquerda é igual ou mais complexo que o texto à direita. O status de atualização de casos legais é um conjunto de dados montado pela JusBrasil, uma empresa brasileira que fornece acesso a informações sobre casos legais. Eles usaram um modelo gerativo da OpenAI para explicar o significado de atualizações de status como “Remetidos os Autos (em diligência) para Central de perícia”. Há 1.656 explicações. Cada explicação é anotada por especialistas que avaliaram sua qualidade de acordo com diferentes aspectos, nomeadamente: (i) se a explicação é juridicamente precisa e se é de alguma forma útil. As respostas possíveis foram ‘sim’, ‘não’, ‘parcialmente’ ou ‘não aplicável’; e (ii) a qualidade da explicação. As respostas possíveis foram ‘bom’, ‘ruim’, ‘parcialmente’ ou ‘não aplicável’. Descartamos os 231 instâncias anotadas com qualidade ‘ruim’ ou que não foram consideradas juridicamente precisas ou úteis. Os 1.424 instâncias restantes foram usados no passo de fine-tuning. Exemplos manualmente selecionados. Selecionamos manualmente 149 pares de sentenças ⟨O, R⟩ de materiais publicados por tribunais de justiça envolvidos em projetos de linguagem plana promovidos pelo governo brasileiro. A lista dessas fontes pode ser encontrada em.7 As instâncias das três fontes foram mescladas e usadas como dados de treinamento.
O conjunto de dados de teste consiste em 91 instâncias do conjunto de dados selecionado que são do domínio jurídico. As divisões de treinamento, validação e teste foram descontínuas. Algumas estatísticas dos conjuntos de dados estão na Tabela 2. Algumas exemplos da estrutura do conjunto de dados estão na Tabela 1, e estatísticas dos conjuntos de dados estão na Tabela 2.

Tabela 1. Exemplos de linhas do conjunto de dados utilizados no trabalho

Complexo (O) Simples (R) ORI→NAT Autores de furto estariam migrando para o roubo. Autores de furto estariam mudando para o roubo. ORI→STR O CNJ enviou a demanda de informações à SECTI. O Conselho Nacional de Justiça pediu as informações à Secretaria de Tecnologia da Informação.

Tabela 2. Estatísticas do conjunto de dados utilizado para fine-tuning

Número de instâncias O→N O→S N→S Total Total 2.931 2.570 4.101 9.602

Treinamento 2.631 2.318 3.692 8.641

Validação 300 252 409 961

3.2 Modelos de Aprendizado de Máquina

Empregamos quatro modelos de Transformer amplamente utilizados para realizar a TS. Foram utilizados tanto modelos de decoder-only quanto modelos de encoder-decoder.

PTT5 [5] é um modelo T5 [20] pré-treinado no corpus BrWac [24], uma grande coleção de páginas da web em português. Ele melhora o desempenho do T5 em tarefas de similaridade e inferência de sentenças em português. Está disponível em três tamanhos (pequeno, base e grande) e com duas vocabularias. Utilizamos o modelo base como um baseline inicial. O PTT5 também pode ser utilizado com instruções apenas (e sem fine-tuning), mas essa abordagem não funcionou bem nesse modelo, pois os outputs finais foram apenas uma cópia da solicitação. Portanto, apenas a alternativa de fine-tuning do PTT5 é relatada aqui e é rotulada como FT-PTT5.

Flan-T5-Large [7] também é um modelo multilíngue da família T5. Ele tem 700 milhões de parâmetros e continua o treinamento a partir de um checkpoint adaptado do T5-LM. Ele utiliza uma variedade ampla de dados rotulados para fine-tuning com instruções, mas o dados não é específico para a tarefa de TS. GPT-3.
5-Turbo e GPT4o Os modelos GPT originam-se da pesquisa da OpenAI em NLP, com GPT-3.5-Turbo e GPT-4o apresentando avanços como parâmetros e janelas de contexto aumentados. Os modelos GPT-3.5-Turbo e Flan foram escolhidos com base em uma avaliação de modelos geradores para TS [13]. Ao melhor de nossa conhecimento, o GPT-4o não foi testado para TS devido ao lançamento recente. Esses três modelos foram usados exclusivamente com prompting sem fine-tuning.

3.3 Abordagem de Fine-tuning e Aprendizado por Reforço

O fine-tuning é o processo de ajustar um modelo de linguagem treinado previamente em um conjunto de dados específico para uma tarefa ou domínio específico. Neste trabalho, a tarefa TS é o alvo. O fine-tuning foi feito usando o modelo PTT5.

Procedimento de Fine-tuning

Para fine-tuning o PTT5 para realizar TS, seguimos as divisões descritas na Tabela 2. O tamanho de batch e o batch por dispositivo em treinamento foram definidos como 4 sentenças devido a limitações de infraestrutura. Usamos 3 passos de acumulação de gradientes e um tamanho de batch de 64 por dispositivo para avaliação. A taxa de aprendizado foi de 1e-4, a decaimento de peso foi de 0,01 e o treinamento foi realizado por 100 épocas com checkpoints a cada 3000 passos. A função de perda foi usada para escolher o melhor modelo e o ponto flutuante de 16 bits (half-precision floating point) foi definido como verdadeiro. O métrica SARI foi calculada durante a validação.

Aprendizado por Reforço

O modelo fine-tunado foi usado para aplicar Aprendizado por Reforço (RL) com base em DRESS [27]. Referimo-nos ao modelo resultante como FT-PTT5 + RL. No entanto, o DRESS usou um LSTM para gerar os outputs e aplicou RL posteriormente. No contexto de tarefas de texto, o RL é usado como treinamento adicional para modelos pré-treinados. Uma técnica comum é linearmente interpolar a recompensa do RL com a perda de entropia cruzada para evitar treinamento errôneo devido ao grande espaço de ação. Durante o RL, usamos métricas que não dependem de simplificações de referência.
Essas métricas são diferentes das utilizadas para avaliar a qualidade das previsões no conjunto de teste (Seção 3.5), pois não é possível otimizar e avaliar usando as mesmas métricas. 1. Nível de Ensino Flesch-Kincaid (FKGL) é utilizado para avaliar a legibilidade de um texto, indicando o nível de educação necessário para entender facilmente. Ele considera o número médio de palavras por sentença e o número médio de sílabas por palavra, oferecendo uma pontuação correspondente a anos de escolaridade necessários para entender um texto. Como SARI é a métrica principal para avaliar automaticamente saídas em TS, o FKGL foi calculado como um prêmio de simplicidade no RL. 2. Anotação Semântica para Leitura por Máquina (SAMSA) se concentra na habilidade do sistema para entender e extrair informações semânticas precisas de textos complexos. Usando anotações semânticas, como entidades nomeadas e relações entre conceitos, a SAMSA busca quantificar a qualidade da interpretação do sistema identificando se ela captura corretamente o sentido essencial do texto [21]. A SAMSA atribui pontuações altas a sentenças divididas, o que é uma ação de simplificação citada como uma das mais importantes em TS [14]. 3. Distância de Levenshtein — calcula a diferença entre duas strings. Esta medida quantifica o número mínimo de operações necessárias para transformar uma sequência em outra, onde operações válidas são a inserção, a exclusão ou a substituição de um caractere único. Esta métrica é amplamente utilizada em aplicações de processamento de palavras, como correção de erro de ortografia e detecção de plágio, fornecendo um método eficiente para comparar e medir a similaridade entre strings diferentes. O cálculo desse prêmio visa preservar o sentido original da sentença. No passo de RL, o modelo é treinado para aumentar um prêmio. O modelo fine-tunado PTT5 é visto como um agente; ele lê a sentença fonte e então toma uma ação de acordo com uma política. O agente gera a sequência de saída como 8.
O texto simplificado; um prêmio é calculado e o reforço atualiza o estado do agente. O prêmio r(ˆY) utilizado nesse trabalho seguiu a base de prêmio do DRESS [27] e é calculado de acordo com a Equação 1, considerando três perspectivas: simplicidade (rS), relevância (rR) e fluência (rF): r(ˆY) = λSrS + λRrR + λFrF (1) onde rS é a pontuação FKGL para simplicidade, rR é a distância de Levenshtein para preservação do sentido e rF é a pontuação SAMSA para fluência. λS, λR e λF são constantes ∈[0, 1] e foram definidas como 1, 0,25 e 0,5, respectivamente.

3.4 Abordagem Gerativa

A falta de um conjunto de dados paralelo para permitir a fine-tuning ou aprendizado de transferência nos vários modelos de linguagem de larga escala (LLMs) disponíveis torna os modelos gerativos uma boa alternativa. As instruções não precisam de muitos exemplos para que o modelo entenda o objetivo principal de uma tarefa. Isso é exatamente o cenário que temos para o domínio jurídico português. Adaptamos a prompt do BLESS [13], que utiliza o aprendizado de contexto em poucas tentativas e alcançou resultados bons em três domínios em inglês. A prompt utilizada foi escrita em português, mas a instrução traduzida está apresentada na Figura 2.

Reescreva a frase complexa para facilitar o entendimento para pessoas que não são da área jurídica. Você pode fazer isso substituindo palavras complexas por sinônimos mais simples, excluindo informações sem importância, reordenando informações e/ou dividindo uma frase complexa longa em várias outras mais simples. Complexa: {exemplo complexo do domínio} Simples: {versão simplificada do complexo} Complexa: {entrada complexa de interesse} Fig. 2. Estrutura da instrução submetida aos modelos gerativos.

3.5 Métricas de Avaliação

A avaliação do TS é um problema aberto. Métricas projetadas para a legibilidade e níveis de compreensão de texto são principalmente usadas na pesquisa linguística sobre a tarefa, mas não são adequadas para avaliar resultados de simplificação. FKGL é um exemplo dessas métricas.
A sua nota é altamente sensível à comprimento da sentença e, em alguns casos, fornece apenas impactos menores em outros métricos, como Simplificação de Texto Automática para o Domínio Jurídico em Português Brasileiro 9 SARI [22]. Uma sentença com muitas exclusões de tokens pode causar perda de informações importantes e não ser capaz de representar o real significado do mensagem de texto. BLEU e SARI também estão sendo amplamente utilizados para avaliar a qualidade do TS. BLEU visa correlacionar-se gramaticalmente com a percepção humana, além de preservar o significado. SARI é uma boa opção para obter uma visão geral da simplicidade, mas tem resultados melhores ao avaliar sistemas de paraphrasing lexical [2]. O BERTScore tem resultados bons ao identificar quando as sentenças de referência são semelhantes às saídas do sistema. É também uma boa opção para avaliar a preservação do significado e semelhanças com a referência, mas um alto score não significa que a saída foi simplificada [2]. As métricas que usamos para avaliar a saída são brevemente descritas abaixo. 1. SARI — Sistema para Avaliação Automática é um métrico automático que avalia bem uma sistema preserva o significado ao tornar o texto acessível e mais fácil de entender. O métrico é calculado através de precisão, recall e F1-score para unigrams, bigrams e trigrams, incorporando similaridade semântica [25]. O SARI é calculado sobre os n-grams adicionados e mantidos, e a proporção dos que foram excluídos. 2. BLEU — Estudante de Avaliação Bilingue é um métrico usado para avaliar sistemas de tradução de máquina. Ele mede o overlap de n-gramas entre a saída e um ou mais referências humanas. Quanto mais próximo o score BLEU estiver de 1, mais próxima a saída estará da referência humana de acordo com a sintaxe (escolhas de palavras e sua ordem) [17]. 3. BERTScore — é um métrico para avaliar a qualidade da geração de texto ao comparar o texto gerado com um texto de referência. Diferentemente de métricos tradicionais como BLEU ou ROUGE, que se baseiam na extensão exata do comprimento de n-gramas, o BERTScore aproveita embeddings contextuais do modelo BERT.
Ele tem uma avaliação melhor da preservação do sentido devido ao background de BERT, computando uma pontuação de similaridade entre cada token nos textos candidatos e de referência, capturando nuances semânticas e significado contextual [26]. 4. ROUGE — Estudo de Avaliação Orientado para Recuperação de Conteúdo — avalia a sobreposição de n-gramas, bem como a sobreposição de sequências de palavras e pares de palavras, entre o texto gerado e um texto de referência. Ao se concentrar na recuperação, o ROUGE destaca a captura de quanto possível do conteúdo do texto de referência. 4 Experimentos e Resultados Os experimentos realizados nessa obra visam responder à pergunta de pesquisa formulada na Seção 3. 4.1 Avaliação Quantitativa A avaliação quantitativa realizada foi baseada nos métricas descritas na Seção 3.5. Essa validação foi feita sobre as 91 instâncias manualmente selecionadas que são de textos legais (Seção 3.1). O valor médio das métricas obtidas por cada modelo é apresentado na Tabela 3 e nos fornece uma visão geral breve para responder à RQ2. Tabela 3. Valores médios de cada métrica de avaliação no conjunto de testes BLEU SARI BERTScore ROUGE Precisão Recall F1 Precisão Recall F1 FT-PTT5 .42 .40 .89 .83 .86 .74 .70 .65 FT-PTT5 + RL .51 .36 .93 .91 .92 .88 .90 .84 GPT-3.5-Turbo .15 .42 .84 .81 .83 .74 .75 .75 GPT-4o .19 .43 .83 .82 .83 .69 .68 .67 Flan-T5-Large .66 .37 .95 .96 .96 .96 .97 .96 O SARI é a métrica principal utilizada para avaliar o TS, enquanto o BERTScore, o BLEU e o ROUGE são utilizados para medir a preservação do sentido do texto simplificado. Os modelos GPT e FT-PTT5 obtiveram os valores mais altos para o SARI, indicando que as frases são mais fáceis de entender. O vencedor aqui é o GPT-4o, embora ao se olhar para os scores de BLEU, os dois modelos GPT tenham scores mais baixos em comparação a todos os modelos T5, o que significa que não preservaram a sintaxe das frases originais. Todos a família T5 têm scores BLEU altos. No entanto, isso muda quando se olha para os valores de BERTScore e ROUGE.
Os modelos GPT ainda são superados pelos modelos T5. De acordo com o BERTScore e ROUGE, GPT-4o e GPT-3.5-Turbo são boas opções para TS, pois possuem os maiores escores de simplicidade ao preservar o sentido. Flan-T5-Large é o vencedor na preservação do sentido; ele tem os melhores escores de similaridade com as simplificações de referência, embora o SARI seja o penúltimo escore. As simplificações geradas são realmente discretas, não fazendo mudanças significativas na sentença original, como mostrado no exemplo seguinte: Original: “Nada veda que a declaratória seja ajuizada em conexão com o pedido constitutivo ou condenatório.”. Simplificada: “Nada proíbe que a declaração seja julgada em conexão com o pedido constitutivo ou condenatório.” A Figura 3 revela que o GPT-4o pode ser o melhor modelo, com uma leve vantagem sobre o GPT-3.5-Turbo. FT-PTT5 tem resultados melhores para o SARI em comparação com o FT-PTT5+RL. O RL se baseou no FKGL como métrica de simplicidade, e esses resultados reforçam a ideia de que o FKGL não é adequado para medir o TS. Os escores do modelo com RL e Flan-T5-Large são muito semelhantes, com uma leve vantagem para o Flan-T5-Large. Como os resultados para a preservação do sentido são um pouco melhores do que os fine-tuned PTT5, parece que a Distância de Levenshtein também é uma boa alternativa para verificar a similaridade entre as sentenças. 4.2 Avaliação Qualitativa Para fornecer insights sobre a qualidade das simplificações geradas, os resultados dos métodos de TS foram analisados por um especialista em domínio. O anotador é um analista judicial que assume o papel de revisor editorial em um Tribunal de Justiça brasileiro. A amostra selecionada para anotação contém 91 pares para cada um dos cinco métodos de simplificação, contendo a sentença original e a saída de simplificação de cada método adotado.
O anotador não estava ciente do método utilizado para produzir a simplificação. O especialista recebeu as sentenças originais e simplificadas e foi solicitado a avaliar a qualidade da simplificação respondendo a três perguntas: – Q1 — A simplificação é correta? Uma simplificação correta deve ser gramaticalmente correta do ponto de vista das normas da língua portuguesa e da norma legal conceitual. Os valores possíveis de resposta são: ‘Sim’, ‘Não’ ou ‘Parcialmente’. – Q2 — A sentença simplificada é de fato mais fácil de entender? Os valores possíveis de resposta são: ‘Sim’, ‘Não’ ou ‘Igual’ (quando a saída é idêntica à entrada). – Q3 — Qual é a qualidade da sentença simplificada? Isso se baseia nas perguntas anteriores, ou seja, como a sentença simplificada pode ser avaliada. Os valores a serem anotados são: ‘Bom’, ‘Razãoável’ ou ‘Péssimo’. As diretrizes principais do Livro de Linguagem Plana [8], que apoiaram a avaliação das sentenças geradas pelos modelos, são: escrever sentenças curtas com uma média de 15 a 20 palavras; escolher palavras comuns; usar a ordem direta da sentença; preferir verbos de ação direta; dividir o texto em tópicos; evitar marcos de gênero; apresentar a informação mais importante no início; e usar pontuação apropriada. A Tabela 4 fornece um resumo da avaliação humana das simplificações geradas por cada modelo para cada pergunta (resposta à RQ2). Ela mostra a porcentagem de instâncias que foram classificadas pelo especialista em cada uma das categorias possíveis. O GPT-3.5-Turbo é o vencedor, com o desempenho mais robusto em todas as três perguntas avaliativas. Apenas um output com qualidade ‘péssima’ é apresentado na Tabela 5, 12 F. Pereira et al. Tabela 4. Resultados da avaliação qualitativa. Modelos Q1 Q2 Q3 Sim Não Parcialmente Sim Não Igual Bom Péssimo Razãoável FT-PTT5 .44 .31 .25 .45 .31 .24 .14 .30 .56 FT-PTT5 + RL .43 .53 .04 .05 .57 .38 .01 .02 .97 GPT-3.5-Turbo .99 — .01 .98 .02 — .84 .01 .15 GPT-4o .89 .04 .07 .81 .18 .01 .57 .19 .
24 Flan-T5-Large .17 .83 - .01 .03 .96 .01 .99 - Tabela 5. Sentenças simplificadas pelo modelo vencedor, GPT-3.5-Turbo Original (O) Simplificada Q1 Q2 Q3 Este é um direito básico que deve ser garantido a todo cidadão. Todo cidadão tem o direito básico de manter o mesmo plano de saúde coletivo após se aposentar. Não Não Oficie-se como requer para que remeta a este juízo, no prazo de 15 dias, cópia do documento X. Anexar ao ofício cópia da inicial e desta decisão. Envie um ofício pedindo uma cópia do documento X no prazo de 15 dias. Anexe ao ofício uma cópia da petição inicial e desta decisão. Sim Sim Bom juntamente com um exemplo de qualidade "bom". O primeiro não corresponde ao conteúdo da sentença de referência, adicionando informações. No segundo output, a ordem das sentenças é alterada para permitir uma leitura fluida, e o vocabulário é simplificado usando palavras mais frequentes. O GPT-4 também demonstra resultados competitivos, com contagens altas em "Sim" para Q1 e Q2. Ambos os modelos apresentam escores muito baixos no BLEU, mas altos no BERTScore. O BLEU utiliza o tamanho exato de n-grama, enquanto o BERTScore considera o contexto da sentença. Ao aplicar TS, mudanças de sintaxe e lexical podem trazer diferenças em termos de n-gramas, mas manter o sentido. Isso mostra a importância de usar métricas diferentes no passo de avaliação. O Flan-T5-Large apresentou escores altos para preservação de significado, mas baixos para simplicidade. Como podemos ver na Tabela 4, 95,6% das sentenças não foram modificadas em suas formas originais, concordando com os resultados apresentados na avaliação quantitativa na Tabela 3. FT-PTT5 e FT-PTT5 + RL têm resultados semelhantes na avaliação quantitativa, com o primeiro vencendo em simplicidade, mas na avaliação qualitativa, FT-PTT5 + RL têm menos sentenças de qualidade ruim do que FT-PTT5 apenas. O FT-PTT5 e FT-PTT5+RL fornecem resultados melhores em comparação ao Flan-T5-Large, considerando tanto avaliações qualitativas quanto quantitativas.
Ambos os modelos baseados em PTT5 apresentam problemas no Q1, não preservando o sentido legal ou com problemas de gramática. Como o fine-tuning não tem exemplos de treinamento do domínio, isso pode ser a principal razão pela qual não é possível generalizar para o domínio (resposta à RQ1). O DRESS [27] é o estado-da-arte de RL com os melhores resultados até agora, mas a mudança de métricas utilizadas nesse trabalho não gerou resultados bons em ambas as avaliações. A finetuning de modelos mostrou-se promissora para a TS, e o RL é capaz de aumentar o desempenho também (resposta à RQ3), pois apenas 2% dos outputs foram considerados ruins contra 30% do FT-PTT5.

5 Conclusão
Este trabalho avaliou LLM para a TS em textos legais escritos em português. Fine-tunamos o modelo pré-treinado PTT5 e desenvolvemos um algoritmo de RL com base no DRESS [27], mas com PTT5 como base para geração de texto para texto. Esses modelos foram comparados com dois GPT-3.5-Turbo, GPT4o e FLAN-T5-Large. Existem muitos tipos de documentos legais em português disponíveis, mas sem suas simplificações em linguagem plana, não podem ser facilmente usados para treinar ou avaliar a TS. Relyemos em um conjunto de atualizações de casos legais com suas explicações em linguagem plana obtidas a partir de modelos baseados em GPT como dados de treinamento. Embora esse dado esteja no domínio de interesse, não é um conjunto de TS no sentido estrito. Além disso, nossa avaliação qualitativa foi feita apenas por um especialista. Como trabalho futuro, experimentos com outros modelos, como BARD8 e Maritaca,9 podem ser realizados. A fine-tuning de versões maiores de LLMs pode ser muito custosa, mas já foi mostrada que melhora os resultados da TS. O uso do RL teve resultados semelhantes ao modelo fine-tunado. No entanto, o uso de outras métricas nas funções de recompensa pode melhorar o desempenho, fechando a lacuna em comparação com os modelos GPT. Outras abordagens, como realizar a TS de acordo com níveis de legibilidade, podem ser exploradas.
O domínio jurídico apresenta diferentes subáreas, o que torna difícil generalizar um modelo para todos os setores. Uma expressão pode ter diferentes significados ao longo das subáreas do domínio jurídico, ou mesmo ao longo dos estados brasileiros. Assim, a questão de saber se um modelo pode ser robusto a essas variações ainda não foi respondida. Agradecimentos: Este trabalho foi parcialmente financiado pelo CNPq-Brasil e pelo Código de Financiamento da Capes 001. Os autores agradecem a Edleno Silva de Moura por compartilhar as atualizações de casos jurídicos criadas pelo JusBrasil e à INOVAJUS por compartilhar exemplos de simplificação.