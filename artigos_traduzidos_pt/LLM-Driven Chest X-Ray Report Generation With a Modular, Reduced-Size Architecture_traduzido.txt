Geração de Relatório de Raios-X do Peito com Arquitetura Modular e Reduzida com Modelo de Linguagem LLM Talles Viana Vargas1, Helio Pedrini1 e André Santanché1 1Instituto de Computação (IC), Universidade Estadual de Campinas, Campinas, SP, Brasil tallesviana1@gmail.com, helio@ic.unicamp.br, santanche@ic.unicamp.br Resumo. Os Modelos de Linguagem de Grande Escala (LLMs) têm sido amplamente empregados em várias tarefas de processamento de texto. Em visão computacional, esses modelos encontraram aplicação em geração de legendas e texto a partir de imagens naturais, bem como em sistemas de Resposta a Perguntas Visuais (VQA). No campo da imagem médica, há estudos baseados na geração de texto que propõem diagnósticos automatizados de raios-X, imagens de ressonância magnética, tomografias computadorizadas e outras modalidades. Poucas iniciativas buscam aplicar e explorar o potencial dos LLMs na geração de texto médico; elas utilizam modelos com dezenas de bilhões de parâmetros e são assim computacionalmente caras. Este trabalho aborda essa lacuna avaliando o uso de modelos pré-treinados congelados (CXAS U-Net e BioGPT) para geração de relatórios de raios-X do peito. Adaptamos a arquitetura modular BLIP-2, onde apenas um módulo de alinhamento multimodal precisa ser treinado para gerar texto a partir de imagens. Conseguiu-se alcançar escores competitivos em métricas de Eficácia Clínica (CE) em comparação com alguns métodos de ponta (SOTA), enquanto obteve-se escores mais baixos em métricas de Geração de Linguagem Natural (NLG). Nossos achados sugerem que as métricas de NLG podem não servir como proxies adequados para avaliar modelos no task de geração de relatórios de raios-X do peito. Palavras-chave: Geração de Relatório de Raios-X · Modelos de Linguagem · Treinamento Leve · Raios-X do Peito 2 T.V. Vargas, H. Pedrini, A. Santanché 1 Introdução Os raios-X do peito são uma ferramenta diagnóstica fundamental na saúde, contendo informações valiosas que podem ajudar profissionais de saúde a diagnosticar várias condições pulmonares, cardíacas ou traumáticas [1].
Eles fornecem um instantâneo da estrutura torácica interna de um paciente, revelando detalhes críticos que podem orientar os profissionais de saúde em seus processos de tomada de decisão. No entanto, interpretar e extrair as conclusões de raios X torácicos é um processo complexo, repetitivo e frequentemente tempo-consumidor. A análise dessas imagens tradicionalmente depende da expertise de radiologistas, que interpretam a informação visual contida nos raios X e, em seguida, transcrevem suas conclusões em relatórios textuais. Este processo manual não apenas é intensivo em mão de obra, mas também é suscetível a variações na interpretação e no relatório. É nesse contexto que soluções inovadoras são necessárias para melhorar a eficiência, a precisão e a acessibilidade da análise de raios X torácicos [12]. Nos últimos anos, os avanços na inteligência artificial (IA) e processamento de linguagem natural (PLN) estão revolucionando a forma como os seres humanos interagem com as máquinas [20, 31]. Algumas aplicações de Modelos de Linguagem Grande (MLL), como o ChatGPT, estão demonstrando capacidades excepcionais de geração de texto, podendo produzir textos com qualidade sintática e semântica impressionante. Eles também demonstram um desempenho bom ao realizar tarefas que não foram treinados anteriormente, por exemplo, um pode solicitar que eles gerem um diagnóstico clínico ou escrevam um código para resolver um problema dado. Vários trabalhos estão aproveitando os desenvolvimentos em IA e PLN para automatizar e otimizar o processo de transformar informações visuais de raios X torácicos em descrições textuais informativas, no entanto, apenas poucos têm capitalizado o potencial dos MLL. Esses métodos existentes predominantemente dependem de modelos intensivos em recursos que apresentam desafios de acessibilidade para aqueles com recursos computacionais limitados.
Portanto, para aproveitar as capacidades dos LLMs enquanto minimiza os recursos necessários para treinar e implantar um sistema de geração de relatórios de exame de tórax, este estudo propõe adaptar a arquitetura modular BLIP-2 [16] para utilizar modelos pré-treinados congelados em domínio para tanto codificação de visão quanto geração de texto. Empregamos BioGPT [18] como nosso modelo de geração de texto, que se baseia em GPT-2 com 354M de parâmetros, oferecendo significativamente menores requisitos de recursos em comparação com modelos utilizados em outros estudos [6, 30, 36] e LLMs amplamente utilizados em áreas de aprendizado profundo [20, 31]. Ao adotar essa arquitetura modular, onde os pesos do codificador de imagem e do decodificador de texto são congelados, apenas um módulo de alinhamento visão-texto precisa ser treinado, reduzindo a complexidade de treinamento e custos computacionais em comparação com ter que treinar um grande LLM. Ao aproveitar o poder dos LLMs enquanto busca eficiência computacional, esta pesquisa contribui para o avanço da análise de imagens médicas com inteligência artificial. Este avanço tem o potencial de reduzir a carga de trabalho dos radiologistas ao servir como apoio à decisão, aumentar a produtividade, minimizar atrasos no diagnóstico e permitir que eles se concentrem em análises complexas onde sua expertise é mais valiosa [11].

2 Fundo

A revisão da literatura se concentra em três áreas: Captionagem de Imagem, Captionagem Médica e Modelos de Linguagem. Nas primeiras duas áreas, realizamos uma exploração abrangente, focalizando em diferentes abordagens, arquiteturas, benchmarks e métricas aplicáveis para nossa tarefa. Por fim, investigamos alguns Modelos de Linguagem, objetivando identificar seus tamanhos e número de parâmetros.

2.1 Captionagem de Imagem

A tarefa de captionagem de imagem visa a geração automática de descrições de imagens naturais. Na literatura, encontramos muitos métodos projetados para alcançar esse objetivo [16, 17, 23, 33, 37, 39].
Esses trabalhos variam com base na escolha do encoder de visão, modelo de linguagem e abordagem de alinhamento trans-domínio, que envolve alinhar características entre os domínios de imagem e texto. Vinyals et al. [33] e Rennie et al. [23] utilizaram modelos CNN para extrair características de imagens e rede LSTM para gerar textos de forma regressiva. As características CNN fornecem uma representação simples e compacta de uma imagem, mas podem obstruir descrições mais finamente detalhadas devido à compressão. A rede LSTM, devido à sua natureza sequencial, pode ser muito lenta para produzir o texto. Zhou et al. [37] e Li et al. [17] exploraram o potencial do Transformer [32] juntamente com características de imagem regionais extraídas por Fast-RCNN [9], um CNN amplamente utilizado para extração de características. Zhou et al. [37], por exemplo, usaram um módulo Transformer compartilhado com múltiplos camadas responsável tanto pela codificação de visão quanto pela decodificação de texto, portanto, este módulo único é responsável por alinhar as características de imagem e gerar a legenda. Trabalhos muito recentes [16, 39] aproveitam o potencial de poucas iterações de LLMs [3]. Zhu et al. [39] introduziram MiniGPT-4, que combina um encoder de visão Transformer com um modelo de linguagem aberto baseado em Llama [31], alinhando ambos os domínios de imagem e texto usando apenas uma camada de projeção única. Os autores o nomearam MiniGPT-4 devido à sua capacidade semelhante de geração de descrições em comparação ao GPT-4. Construído sobre modelos pré-treinados, Li et al. [16] introduziram o framework BLIP-2, que requer treinamento apenas de um módulo de alinhamento trans-modal chamado Q-Former para ponte entre as representações de imagem e texto. Essa abordagem aproveita encoders de imagem pré-treinados e modelos de linguagem grandes (LLMs) congelando seus pesos (ou seja, não os treinando). O componente nuclear, Q-Former, é um módulo baseado em BERT responsável por alinhar as características extraídas de imagens e texto. Ao treinar apenas o Q-Former, o BLIP-2 reduz a complexidade de treinamento e custos computacionais.
Aqui está a tradução do texto científico para português do Brasil:

A legenda médica envolve a geração automática de legendas descritivas e informativas para imagens médicas, como raios X, exames de ressonância magnética, exames de tomografia computadorizada e mais. O objetivo é fornecer descrições textuais precisas que transmitam informações médicas relevantes, possivelmente ajudando profissionais médicos a fazer diagnósticos mais rápidos e precisos, facilitando a pesquisa e o treinamento, e melhorando o cuidado ao paciente. Considerável progresso foi feito nesse campo, com numerosos arquiteturas e abordagens propostas [4, 5, 6, 26, 30, 35, 36]. Algumas obras, particularmente aquelas focadas na legenda de raios X do tórax, empregam CNNs como codificadores de imagem e Transformadores como decodificadores de texto [4, 5]. Chen et al. [4] introduziram o uso de um módulo de memória relacional novel ao lado do Transformer para melhorar a geração de legendas, enquanto Chen et al. [5] se concentraram na alinhamento do domínio de imagem e texto, propõem o uso de uma rede de memória cruz-modular para facilitar as interações entre modalidades (imagem e texto). Por outro lado, um estudo recente [35] adota um transformador de visão como seu codificador de imagem e introduz Tokens de Especialista, projetados para interagir com patches de imagem extraídos e entre si. Como resultado, cada token pode se concentrar em uma parte diferente da imagem. Recentemente, outros papéis começaram a explorar o potencial de LLMs para geração de legendas médicas [6, 30, 36]. Yang et al. [36] adotaram a arquitetura BLIP-2, embora não tenham seguido o processo de treinamento original, em vez disso, eles utilizaram diretamente um grande codificador de visão e um decodificador LLM, e fine-tunaram o Q-Former e o LLM para a tarefa ImageClef, que envolve prever legendas para imagens de raios X gerais. Eles compararam seu desempenho apenas com base na classificação de ranking da competição.
[30] utilizou o Vision Transformer do MedCLIP [34] como encoder de visão, aplicou uma transformação linear simples para alinhar características e empregou um LLM baseado em LLaMA [31] como decoder de texto. Este estudo não compara sua abordagem com outras. Danu et al. [6] adotaram uma abordagem diferente, classificando multi-classe doenças em imagens de raio-x de tórax com caixas de bounding e, em seguida, usando essas características de anomalia como entrada para o LLM. O Med-PaLM [26, 27] é um sistema de inteligência artificial biomédica de grande escala, capaz de interpretar várias modalidades de dados biomédicos, incluindo tarefas como geração de relatórios de raio-x de tórax e respostas a perguntas visuais médicas. Ele emprega um encoder de visão baseado em ViT e PaLM como decoder de texto. Construindo sobre o trabalho anterior, Nicolson et al. [19] investigaram o impacto de combinar modelos pré-treinados diferentes para geração de relatórios de raio-x de tórax. Eles experimentaram com várias combinações de encoders de visão e decoders de texto, tanto do domínio médico (in-domain) quanto de domínios gerais. Interessantemente, seus achados revelaram que uma combinação de um encoder de visão out-of-domain, CvT-21, e um decoder de texto de domínio geral, DistilGPT-2, alcançou desempenho superior em benchmarks de geração de relatórios de raio-x de tórax após fine-tuning. Essa descoberta sugere que aproveitar modelos out-of-domain, quando escolhidos cuidadosamente e fine-tunados, pode levar a resultados state-of-the-art (SOTA). 5 2.3 Modelos de Linguagem Em anos recentes, houve significativos avanços no desenvolvimento de modelos de linguagem. Um marco importante foi a introdução do BERT [8] (Representações de Codificação Bidirecional a partir de Transformadores), que tem 110 milhões de parâmetros na sua versão base e 340 milhões na sua versão grande. O treinamento bidirecional do BERT em tarefas de modelagem de linguagem mascarada permitiu que ele alcançasse resultados state-of-the-art em uma variedade de benchmarks de NLP.
As arquiteturas baseadas em transformadores foram escaladas ainda mais para desenvolver modelos com tamanhos de parâmetros ainda maiores. Por exemplo, o GPT-2 [21], da OpenAI, com 1,5 bilhão de parâmetros em seu modelo maior, demonstrou o potencial de modelos de linguagem não supervisionados para gerar texto coerente e relevante no contexto. A arquitetura do GPT-2 abriu caminho para seu sucessor, o GPT-3 [3], que apresenta um recorde de 175 bilhões de parâmetros, mostrando capacidades impressionantes em aprendizado com poucas amostras e tarefas de processamento de linguagem natural (NLP) diversificadas sem requerer ajustes finos tarefas específicas. Embora tenham desempenho impressionante, esses modelos de grande escala necessitam de recursos computacionais substanciais para treinamento e inferência, limitando sua acessibilidade. Esta desafio impulsionou o desenvolvimento de modelos mais eficientes em recursos. O DistilBERT [24], com 66 milhões de parâmetros, e o ALBERT [14], com 12 milhões de parâmetros em sua versão básica, oferecem alternativas mais compactas que mantenham níveis de desempenho competitivos. No domínio biomédico, modelos de linguagem especializados foram desenvolvidos para abordar os desafios únicos apresentados por textos médicos. O BioBERT [15], baseado no BERT com aproximadamente 110 milhões de parâmetros, e o ClinicalBERT [2], também baseado no BERT, foram ajustados em grandes corpora de literatura biomédica e notas clínicas, resultando em desempenho melhorado em tarefas específicas do domínio. Seguindo a linha dos LLMs, o BioGPT [18], um modelo derivado do GPT-2 com 354 milhões de parâmetros, especialmente projetado para geração de texto biomédico, fornece uma abordagem equilibrada, oferecendo as capacidades extensas dos modelos maiores enquanto mantém requisitos de recursos mais baixos. Esta eficiência torna o BioGPT uma escolha prática para gerar texto biomédico de alta qualidade, garantindo acessibilidade para um público mais amplo de pesquisadores. Em resumo, observamos que muitos trabalhos em captioning médico, especificamente captioning de raio-x de tórax, estão focados em melhorar métricas de desempenho.
Além disso, aqueles que se concentram em aproveitar o poder dos LLMs não parecem priorizar a busca por arquiteturas reduzidas que permitiriam computadores simples com recursos computacionais limitados executarem esses modelos. 3 Materiais Esta obra adota o conjunto de dados MIMIC-CXR [13], um conjunto de dados amplamente utilizado para gerar legendas para raios-x do tórax. É atualmente o maior conjunto de dados públicos de raios-x do tórax, contendo 377.100 radiografias e 227.835 relatórios de texto livre. O conjunto de dados tem uma divisão oficial de dados que permite comparações justas de nossos modelos com métodos da literatura e técnicas diferentes. A maioria dos trabalhos relacionados também utiliza o conjunto de dados IU Xray [7]. No entanto, nesse estudo, escolhemos usar apenas o MIMIC-CXR devido ao seu tamanho maior. No futuro, planejamos incluir conjuntos de dados adicionais para avaliar a generalização. since MIMIC-CXR inclui relatórios de amostra com múltiplas vistas, como raios-x frontal e lateral, empregamos a seguinte estratégia para inferência: ‚ Para relatórios com duas imagens: Selecionamos aleatoriamente duas imagens para análise. ‚ Para relatórios com apenas uma imagem: Duplicamos a imagem única para garantir que o pipeline pudesse processá-la consistentemente. Após esse processo de seleção, todas as imagens foram redimensionadas para uma dimensão padrão de 512ˆ512 pixels. Além disso, foi aplicada normalização usando os valores de média e variância específicos ao encoder de visão escolhido. 4 Método Esta seção apresenta a arquitetura do modelo proposta, as etapas de treinamento e as métricas utilizadas durante o processo de avaliação. Arquitetura do Modelo. Esta obra aproveita a arquitetura e o pipeline de treinamento apresentados pelo BLIP-2 [16], um modelo de legenda para imagens naturais que utiliza modelos pré-treinados congelados. A vantagem chave dessa arquitetura é sua modularidade.
Isso permite a integração de diferentes codificadores de visão e decodificadores de texto, com apenas o módulo de alinhamento (Q-Former) requerendo treinamento específico. O código fonte foi baseado e adaptado da implementação do BLIP-2 no repositório LAVIS2. Etapa de Codificador de Imagem. A arquitetura, como ilustrada na Figura 1, começa com o codificador de imagem, responsável por extrair características relevantes de imagens de raio-x de tórax. Este trabalho adotou uma U-Net [25] pré-treinada anteriormente em um conjunto de dados de segmentação de raio-x de tórax, portanto podemos aproveitar seu conhecimento in-domicílio anterior. Essa escolha oferece um grande vantagem, pois a U-Net tem quase 15 vezes menos parâmetros em comparação com o Transformador de Visão (ViT-g/14) empregado no trabalho original do BLIP-2. Nesse ponto, utilizamos a saída de mapa de características do primeiro camada de upsampling da rede, flattenamos as dimensões de altura e largura, e em seguida, trocamos a dimensão de canal com essa dimensão espacial flattenada. Isso permite ter características prontas para serem processadas pela próxima rede neural. Em relação à extração de características de imagem, o estudo utiliza duas imagens para cada amostra, como mencionado anteriormente, portanto seguiu outros estudos [4, 35], extraiu sequencialmente características de ambas as imagens usando o codificador de imagem selecionado e média essas características.

Figura 1: Arquitetura proposta para geração de relatório de raio-x de tórax.

Módulo de Alinhamento Q-Former. O ponto central da arquitetura, o Q-Former é responsável por alinhar as características de imagem extraídas com a entrada do decodificador de texto. Como o Q-Former é um modelo baseado em BERT, aproveitamos pesos pré-treinados disponíveis do BioClinical-BERT [2], que foi treinado em resumos de PubMed e também em notas do MIMIC III.
Iniciando modelos de linguagem com pesos pré-treinados em domínio, em vez de usar a inicialização padrão do BERT, potencialmente aumenta a capacidade de alinhamento de recursos do modelo; comportamento visto em outras tarefas médicas [15]. Utilizamos a mesma configuração do código fonte base. Módulo de Decodificação de Texto. O módulo de decodificação de texto é responsável por gerar as descrições de raio de tórax dadas as características alinhadas pelo Q-Former. Semelhante à arquitetura BLIP-2, adotamos um Modelo de Linguagem Grande (LLM) como nosso módulo de decodificação de texto, devido à sua capacidade de geração de texto. Geralmente, um LLM recebe prompts de texto em sua entrada, que funcionam como um ponto de partida, a partir do qual o modelo gera a saída de texto. No entanto, nesse trabalho, como os inputs do LLM vêm das características alinhadas do Q-Former, esses inputs podem ser considerados como prompts suaves, ou seja, não são embeddings de texto explícitos, mas um vetor de características que deve ajudar e guiar o módulo de decodificação de texto do LLM em sua tarefa de geração de texto. Esse trabalho utiliza BioGPT [18], um LLM baseado na arquitetura GPT-2 [21], pois foi especificamente pré-treinado em domínio biomédico usando resumos de PubMed. É um modelo leve em comparação com o LLM original do BLIP-2, contendo 345 milhões de parâmetros em vez de 2,7 bilhões. 8 T.V. Vargas, H. Pedrini, A. Santanché 4.1 Etapas de Treinamento O processo de treinamento é dividido em duas etapas distintas, cada uma servindo um propósito específico em melhorar as capacidades de uma parte da nossa arquitetura, como ilustrado na Figura 1. Aprendizado de Representação. Nessa etapa de treinamento inicial, o Q-Former é treinado em conjunto com um encoder de imagem pré-treinado congelado. O objetivo primário é habilitar o Q-Former a servir como um ponte entre domínios de imagem e texto. Este passo prepara o módulo para alinhar, compreender e também gerar legendas significativas, melhorando o desempenho da arquitetura como um todo.
Durante essa fase, os saídos do Q-Former são otimizados usando três métodos (funções de custo) distintos, como descrito no trabalho BLIP-2 [16]. Adotamos o otimizador AdamW com decaimento de peso de 0,05. A taxa de aprendizado seguiu um cronograma de decaimento cosino durante 10 épocas, começando em 1e-4 e terminando em 1e-5. Um período de aquecimento de uma época foi empregado com taxa de aprendizado definida como 1e-6. Além disso, o checkpoint do modelo melhor foi selecionado com base na perda mais baixa global. Aprendizado Gerativo. Na segunda etapa de treinamento, nossa arquitetura inteira é montada. O módulo Q-Former treinado anteriormente é integrado com o BioGPT pré-treinado congelado. Em seguida, o Q-Former é refinado otimizando os saídos gerados pelo decodificador LLM. Durante esse processo de refinamento, o Q-Former atua como um guia, influenciando a geração de texto do BioGPT com base nas imagens de raio-x de tórax de entrada. Adotamos o otimizador AdamW com decaimento de peso de 0,05. A taxa de aprendizado seguiu um cronograma de decaimento cosino durante 10 épocas, começando em 1e-5 e terminando em 1e-6. Um período de aquecimento de dois épocas foi empregado com taxa de aprendizado definida como 1e-8. Nesse ponto, observamos resultados melhores ao desbloquear o BioGPT e congelar todos os outros módulos. Da mesma forma que antes, o checkpoint do modelo final foi selecionado com base na perda mais baixa. Todos os estágios de treinamento foram realizados na nuvem AWS utilizando uma instância ml.g5.xlarge contendo um GPU de 24Gb e 16Gb de memória. 4.2 Métricas e Avaliação Seguindo Chen et al. [4], Wang et al. [35], Zhou et al. [38], nossa avaliação visou avaliar tanto a qualidade das descrições de texto geradas quanto sua utilidade clínica potencial. Adotamos dois abordagens, explorando métricas estabelecidas de Geração de Linguagem Natural (NLG) (BLEU, METEOR e ROUGE-L) e, adicionalmente, incorporamos eficácia clínica (CE), onde empregamos o sistema CheXbert [28] para rotular automaticamente os relatórios gerados em 14 categorias.
Por meio da comparação dessas etiquetas com os diagnósticos de verdade, calculamos precisão, recall e F1-score, fornecendo insights sobre a capacidade do modelo de gerar relatórios clínicos precisos. Este trabalho apresenta tanto médias micro quanto macro de precisão, recall e F1-score [29].

9 5 Resultados Experimentais

Esta seção apresenta os achados da nossa avaliação sobre a eficácia do método proposto para gerar relatórios de texto clínico. Avaliamos o desempenho utilizando uma combinação de métricas de Geração de Linguagem Natural (NLG) e eficácia clínica (CE) sobre o conjunto de dados MIMIC-CXR. Nossa arquitetura consiste aproximadamente em 450 milhões de parâmetros. Em comparação com outras técnicas que também adotam LLM na etapa de decoder, XRayGPT [30] tem cerca de 7 bilhões de parâmetros exclusivamente no seu modelo de linguagem grande (LLM). Da mesma forma, Yang et al. [36] emprega o modelo LLM ChatGLM-6B, que compreende aproximadamente 6 bilhões de parâmetros. Med-PaLM [26], por outro lado, incorpora 540 bilhões de parâmetros no seu modelo de linguagem PaLM. Infelizmente, esses estudos não avaliam seus modelos usando conjuntos de dados comumente utilizados, portanto, nossa comparação é limitada ao número de parâmetros. Para comparar quantitativamente nossa abordagem com outros estudos na área, avaliamos a eficácia clínica dos relatórios usando etiquetas CheXbert. Precisão, recall e F1-score foram calculados para cada achado CheXbert. A Tabela 1 resume o desempenho CE global comparado a métodos existentes. Nossa metodologia alcançou a precisão mais alta (0,504) entre os métodos avaliados, indicando uma taxa baixa de achados positivos falsos. No entanto, o escore de recall (0,346) sugere que pode ignorar alguns achados relevantes em comparação com CvT21-2DistilGPT2 [19].

Tabela 1: Comparação do desempenho de diferentes métodos usando métricas de precisão, recall e F1.

Método Precisão Recall F1
R2Gen [4] 0,333 0,273 0,276
CMN [5] 0,334 0,275 0,278
METransformer [35] 0,364 0,309 0,311
COMG [10] 0,424 0,291
345 CvT21-2DistilGPT2 [19] 0,398 0,497 0,442 Nossos 0,504 0,346 0,410 A Tabela 2 apresenta uma comparação de métricas de geração de linguagem natural (GLN) entre técnicas diferentes e nosso modelo. Nossa modelo apresenta escores baixos se comparados às outras técnicas. Isso pode ser explicado pelo fato de que essas métricas de GLN foram desenvolvidas principalmente para avaliar a tradução automática, o que é claramente diferente da tarefa aqui em questão. Como nosso modelo de base é um modelo pré-treinado GPT-2, nossas previsões podem não se alinhar tão estreitamente com a verdadeira saída. Consequentemente, essa desalinhagem pode ser responsável pelos escores baixos observados em nossos métricos propostos. Uma análise mais aprofundada revelou uma nuances interessante entre métricas de eficácia clínica (EC) e métricas de GLN no contexto da geração de relatórios de exame de tórax. Especificamente, dentro de um subconjunto de previsões de alta performance com base em métricas EC—onde as previsões transmitiram efetivamente observações das radiografias torácicas—correspondentes métricas de GLN apresentaram escores mais baixos (ver Tabela 3). Essa observação sugere que as métricas de GLN sozinhas podem não ser o indicador mais confiável da qualidade do modelo para essa tarefa. A Tabela 3 apresenta os escores de métricas de GLN (por exemplo, BLEU, ROUGE-L) para um subconjunto de previsões do modelo que alcançaram a maior precisão, recall e F1-score na avaliação de eficácia clínica. Escores de GLN baixos nessas previsões de alta performance sugerem que as métricas de GLN sozinhas podem não ser o indicador mais confiável da qualidade do modelo para a geração de relatórios de exame de tórax.
086 0,138 0,222 Mesmo previsões clinicamente precisas segundo métricas CE podem não receber altos escores de NLG. Isso destaca as limitações das métricas de NLG em capturar o espectro completo de fatores que contribuem para relatórios clínicos de boa qualidade, como precisão factual, fraseação nuances e adesão a convenções de relatórios. Finalmente, como mostrado na Tabela 4, o modelo apresenta desempenho bom na identificação de achados como Dispositivos de Suporte (0,683 F1) e Cardiomegalia (0,521 F1). No entanto, luta para com achados menos prevalentes como Atelectasia (0,330 F1) e Opacidade Pulmonar (0,229 F1). Isso pode ser devido a limitações nos dados de treinamento ou à capacidade do modelo em capturar variações subtis nestes achados específicos. Esses resultados demonstram o potencial do método proposto para gerar relatórios de texto clinicamente relevantes e precisos. A alta precisão garante uma taxa baixa de falsos positivos, enquanto investigação adicional é necessária para melhorar a recuperação, especialmente para achados menos frequentes.

Tabela 4: Métricas de Eficácia Clínica (CE) para cada observação no conjunto de teste MIMIC-CXR. As colunas de Casos Positivos e Casos Negativos indicam a distribuição de rótulos positivos e negativos nos dados de verdade, permitindo a verificação de desbalanceamento de classes.

Observação Casos Positivos Casos Negativos Precisão Recall F1
Dispositivos de Suporte 1126 1642 0,688 0,678 0,683
Effusão Pleural 986 1782 0,695 0,445 0,543
Cardiomegalia 1018 1750 0,595 0,498 0,542
Atelectasia 750 2018 0,385 0,288 0,330
Opacidade Pulmonar 963 1805 0,469 0,152 0,229
Sem Achado 193 2575 0,135 0,534 0,216
Edema 564 2204 0,490 0,124 0,198
Enlarged Cardiomediastinum 200 2568 0,170 0,040 0,065
Pneumonia 155 2613 0,154 0,039 0,062
Consolidação 150 2618 0,087 0,013 0,023
Lesão Pulmonar 151 2617 0,000 0,000 0,000
Pneumotórax 74 2694 0,000 0,000 0,000
Fratura 117 2651 0,000 0,000 0,000
Outro Pleural 92 2676 0,000 0,000 0,000
Média Macro - - 0,276 0,201 0,206
Média Micro - - 0,504 0,346
410 6 Conclusões Em conclusão, nosso trabalho apresentou uma abordagem para gerar relatórios de texto clínico a partir de imagens de Raios-X do tórax. A seleção de modelos de tamanho relativamente pequeno e a característica modular do BLIP-2 tornaram o processo de treinamento menos intensivo em hardware, e esperamos que isso incentive futuras investigações sobre essa arquitetura. Nossa metodologia alcançou a precisão mais alta entre as técnicas avaliadas. No entanto, nossas avaliações também revelaram desafios para alcançar uma alta taxa de recuperação, particularmente para observações menos prevalentes. Portanto, o modelo precisa de investigações adicionais para melhorar a capacidade do modelo. Além disso, nossa análise dos métricos de NLG destaca as limitações de usar métricas de tradução de máquina para nossa tarefa específica, que não capturam a qualidade real do relatório. Esforços futuros devem se concentrar em desenvolver métodos de avaliação personalizados que sejam alinhados com nossos objetivos específicos. Em geral, nossos resultados demonstram o potencial de usar modelos pré-treinados, reduzidos em tamanho e específicos para domínio para gerar relatórios de texto clínico relevantes e precisos. No entanto, esforços de pesquisa e desenvolvimento contínuos são ainda necessários para melhorar a taxa de recuperação do modelo e refinar sua capacidade geral. Com essas melhorias, essa abordagem poderia se tornar uma ferramenta valiosa, contribuindo para um diagnóstico mais eficiente e preciso. Como direções futuras, pretendemos avaliar esse framework com outros conjuntos de dados, como BRAX [22] e IU XRay [7], para validar a generalização do modelo.