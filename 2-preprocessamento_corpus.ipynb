{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-AMpT5LyHGjN",
    "outputId": "8dccc0e4-bc77-4c72-8b80-c204365588f2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall pymupdf\n",
    "!pip install langdetect\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2nnTX7r5Qwt"
   },
   "source": [
    "Cria estrutura inicial do corpus com informa√ß√µes do artigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3mSixz11ZYl",
    "outputId": "830b4243-5449-48b0-8ac4-3cac4d5f3f4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo 1/26: https://sol.sbc.org.br/index.php/bracis/article/view/33550 ‚úÖ Extra√≠do.\n",
      "Extraindo 2/26: https://sol.sbc.org.br/index.php/bracis/article/view/33647 ‚úÖ Extra√≠do.\n",
      "Extraindo 3/26: https://sol.sbc.org.br/index.php/bracis/article/view/33556 ‚úÖ Extra√≠do.\n",
      "Extraindo 4/26: https://sol.sbc.org.br/index.php/bracis/article/view/33560 ‚úÖ Extra√≠do.\n",
      "Extraindo 5/26: https://sol.sbc.org.br/index.php/bracis/article/view/33562 ‚úÖ Extra√≠do.\n",
      "Extraindo 6/26: https://sol.sbc.org.br/index.php/bracis/article/view/33648 ‚úÖ Extra√≠do.\n",
      "Extraindo 7/26: https://sol.sbc.org.br/index.php/bracis/article/view/33649 ‚úÖ Extra√≠do.\n",
      "Extraindo 8/26: https://sol.sbc.org.br/index.php/bracis/article/view/33654 ‚úÖ Extra√≠do.\n",
      "Extraindo 9/26: https://sol.sbc.org.br/index.php/bracis/article/view/33576 ‚úÖ Extra√≠do.\n",
      "Extraindo 10/26: https://sol.sbc.org.br/index.php/bracis/article/view/33579 ‚úÖ Extra√≠do.\n",
      "Extraindo 11/26: https://sol.sbc.org.br/index.php/bracis/article/view/33629 ‚úÖ Extra√≠do.\n",
      "Extraindo 12/26: https://sol.sbc.org.br/index.php/bracis/article/view/33630 ‚úÖ Extra√≠do.\n",
      "Extraindo 13/26: https://sol.sbc.org.br/index.php/bracis/article/view/33658 ‚úÖ Extra√≠do.\n",
      "Extraindo 14/26: https://sol.sbc.org.br/index.php/bracis/article/view/33582 ‚úÖ Extra√≠do.\n",
      "Extraindo 15/26: https://sol.sbc.org.br/index.php/bracis/article/view/33590 ‚úÖ Extra√≠do.\n",
      "Extraindo 16/26: https://sol.sbc.org.br/index.php/bracis/article/view/33594 ‚úÖ Extra√≠do.\n",
      "Extraindo 17/26: https://sol.sbc.org.br/index.php/bracis/article/view/33598 ‚úÖ Extra√≠do.\n",
      "Extraindo 18/26: https://sol.sbc.org.br/index.php/bracis/article/view/33599 ‚úÖ Extra√≠do.\n",
      "Extraindo 19/26: https://sol.sbc.org.br/index.php/bracis/article/view/33602 ‚úÖ Extra√≠do.\n",
      "Extraindo 20/26: https://sol.sbc.org.br/index.php/bracis/article/view/33604 ‚úÖ Extra√≠do.\n",
      "Extraindo 21/26: https://sol.sbc.org.br/index.php/bracis/article/view/33606 ‚úÖ Extra√≠do.\n",
      "Extraindo 22/26: https://sol.sbc.org.br/index.php/bracis/article/view/33643 ‚úÖ Extra√≠do.\n",
      "Extraindo 23/26: https://sol.sbc.org.br/index.php/bracis/article/view/33663 ‚úÖ Extra√≠do.\n",
      "Extraindo 24/26: https://sol.sbc.org.br/index.php/bracis/article/view/33612 ‚úÖ Extra√≠do.\n",
      "Extraindo 25/26: https://sol.sbc.org.br/index.php/bracis/article/view/33613 ‚úÖ Extra√≠do.\n",
      "Extraindo 26/26: https://sol.sbc.org.br/index.php/bracis/article/view/33603 ‚úÖ Extra√≠do.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "estrutura_inicial = [];\n",
    "\n",
    "with open('artigos_info.json', 'r', encoding='utf-8') as f:\n",
    "  artigos_info = json.load(f)\n",
    "try:\n",
    "  for idx, artigo_info in enumerate(artigos_info):\n",
    "    url_artigo = artigo_info['url']\n",
    "    caminho_pdf = artigo_info['pdf']\n",
    "    print(f\"Extraindo {idx+1}/{len(artigos_info)}: {url_artigo}\", end=\" \")\n",
    "    # Verifica se o PDF existe\n",
    "    if not os.path.exists(caminho_pdf):\n",
    "      print(f\"PDF n√£o encontrado: {caminho_pdf}\")\n",
    "      exit()\n",
    "\n",
    "    # Extrai dados da p√°gina do artigo\n",
    "    response = requests.get(url_artigo, timeout=10)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # T√≠tulo\n",
    "    titulo_tag = soup.find('h1', class_='page_title')\n",
    "    titulo = titulo_tag.text.strip() if titulo_tag else \"T√≠tulo n√£o encontrado\"\n",
    "\n",
    "    # Autores\n",
    "    autores_tag = soup.find('ul', class_='item authors')\n",
    "    autores = []\n",
    "\n",
    "    if autores_tag:\n",
    "      for li in autores_tag.find_all('li'):\n",
    "        nome_tag = li.find('span', class_='name')\n",
    "        afiliacao_tag = li.find('span', class_='affiliation')\n",
    "        orcid_tag = li.find('span', class_='orcid')\n",
    "\n",
    "        nome = nome_tag.text.strip() if nome_tag else \"\"\n",
    "        afiliacao = afiliacao_tag.text.strip() if afiliacao_tag else \"\"\n",
    "        orcid = orcid_tag.find('a')['href'] if orcid_tag and orcid_tag.find('a') else ''\n",
    "\n",
    "        autores.append({\n",
    "            'nome': nome,\n",
    "            'afiliacao': afiliacao,\n",
    "            'orcid': orcid\n",
    "        })\n",
    "          \n",
    "    # Resumo (abstract)\n",
    "    resumo_div = soup.find('div', class_='item abstract')\n",
    "    if resumo_div:\n",
    "        # Extrai todo o texto da div, ignora o \"Resumo\" e <br>\n",
    "        texto_completo = resumo_div.get_text(separator=\" \", strip=True)\n",
    "        resumo = texto_completo.replace(\"Resumo\", \"\", 1).strip()\n",
    "    else:\n",
    "        resumo = \"\"\n",
    "\n",
    "    data_published_div = soup.find('div', class_='item published')\n",
    "    if data_published_div:\n",
    "        value_div = data_published_div.find('div', class_='value')\n",
    "        if value_div:\n",
    "            data_publicacao = value_div.text.strip()\n",
    "\n",
    "    # Palavras-chave\n",
    "    keywords = artigo_info[\"keywords\"]\n",
    "\n",
    "    estrutura = {\n",
    "      \"titulo\": titulo,\n",
    "      \"informacoes_url\": url_artigo,\n",
    "      \"idioma\": \"Ingl√™s\",\n",
    "      \"storage_key\": caminho_pdf,\n",
    "      \"autores\": autores,\n",
    "      \"data_publicacao\": data_publicacao,\n",
    "      \"resumo\": resumo,\n",
    "      \"keywords\": keywords,\n",
    "      \"referencias\": [],\n",
    "      \"artigo_completo_PT\": \"\",\n",
    "      \"artigo_completo_EN\": \"\",\n",
    "      \"artigo_tokenizado\": [],\n",
    "      \"pos_tagger\": [],\n",
    "      \"lema\": [],\n",
    "      \"dep\": []\n",
    "    }\n",
    "    estrutura_inicial.append(estrutura)\n",
    "    print(\"‚úÖ Extra√≠do.\")\n",
    "\n",
    "  with open(\"corpus_bracis.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(estrutura_inicial, f, ensure_ascii=False, indent=2)\n",
    "except Exception as e:\n",
    "  print(f\"Erro ao processar artigo {url_artigo}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2nnTX7r5Qwt"
   },
   "source": [
    "Preenche campo \"referencias\" no corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-AMpT5LyHGjN",
    "outputId": "8dccc0e4-bc77-4c72-8b80-c204365588f2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33550/33342\n",
      "üìö 24 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33647/33439\n",
      "üìö 44 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33556/33348\n",
      "üìö 34 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33560/33352\n",
      "üìö 38 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33562/33354\n",
      "üìö 25 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33648/33440\n",
      "üìö 27 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33649/33441\n",
      "üìö 30 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33654/33446\n",
      "üìö 31 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33576/33368\n",
      "üìö 24 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33579/33371\n",
      "üìö 41 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33629/33421\n",
      "üìö 27 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33630/33422\n",
      "üìö 23 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33658/33450\n",
      "üìö 31 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33582/33374\n",
      "üìö 23 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33590/33382\n",
      "üìö 33 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33594/33386\n",
      "üìö 39 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33598/33390\n",
      "üìö 26 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33599/33391\n",
      "üìö 47 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33602/33394\n",
      "üìö 13 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33604/33396\n",
      "üìö 22 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33606/33398\n",
      "üìö 21 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33643/33435\n",
      "üìö 39 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33663/33455\n",
      "üìö 25 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33612/33404\n",
      "üìö 22 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33613/33405\n",
      "üìö 36 refer√™ncia(s) coletada(s).\n",
      "üîç Acessando https://sol.sbc.org.br/index.php/bracis/article/view/33603/33395\n",
      "üìö 48 refer√™ncia(s) coletada(s).\n",
      "‚úÖ Finalizado.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "\n",
    "\n",
    "try:\n",
    "    with open('corpus_bracis.json', 'r', encoding='utf-8') as f:\n",
    "        corpus = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao abrir o arquivo JSON: {e}\")\n",
    "    sys.exit\n",
    "    \n",
    "# Abrir os dados\n",
    "with open(\"artigos_info.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    artigos_info = json.load(f)\n",
    "\n",
    "for idx, artigo in enumerate(artigos_info):\n",
    "    url = artigo[\"springer_url\"]\n",
    "    try:\n",
    "        print(f\"üîç Acessando {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Tentar localizar se√ß√£o de refer√™ncias\n",
    "        referencias = []\n",
    "        \n",
    "        paragrafos = soup.find_all(\"p\", class_=\"c-article-references__text\")\n",
    "\n",
    "        for p in paragrafos:\n",
    "            texto = p.get_text(strip=True)\n",
    "            if texto:\n",
    "                referencias.append(texto)\n",
    "\n",
    "        corpus[idx][\"referencias\"] = referencias\n",
    "        print(f\"üìö {len(referencias)} refer√™ncia(s) coletada(s).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao processar {url}: {e}\")\n",
    "\n",
    "# Salvar JSON atualizado\n",
    "with open('corpus_bracis.json', \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(corpus, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Finalizado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2nnTX7r5Qwt"
   },
   "source": [
    "Insere artigos no corpus (artigo_completo_PT e artigo_completo_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando: artigos_en/A Large Dataset of Spontaneous Speech with the Accent Spoken in S√£o Paulo for Automatic Speech Recognition Evaluation.pdf\n",
      "Processando: artigos_en/A Transformer-Based Tabular Approach to Detect Toxic Comments.pdf\n",
      "Processando: artigos_en/An Ensemble of LLMs Finetuned with LoRA for NER in Portuguese Legal Documents.pdf\n",
      "Processando: artigos_en/Aroeira A Curated Corpus for the Portuguese Language with a Large Number of Tokens.pdf\n",
      "Processando: artigos_en/Assessing European and Brazilian Portuguese LLMs for NER in Specialised Domains.pdf\n",
      "Processando: artigos_en/Automatic Text Simplification for the Legal Domain in Brazilian Portuguese.pdf\n",
      "Processando: artigos_en/Developing Resource-Efficient Clinical LLMs for Brazilian Portuguese.pdf\n",
      "Processando: artigos_en/Diversity in Data for Speech Processing in Brazilian Portuguese.pdf\n",
      "Processando: artigos_en/ERASMO Leveraging Large Language Models for Enhanced Clustering Segmentation.pdf\n",
      "Processando: artigos_en/Evaluating Large Language Models for Tax Law Reasoning.pdf\n",
      "Processando: artigos_en/Evaluating Sentiment Quantification Methods in Brazilian Portuguese Corpora.pdf\n",
      "Processando: artigos_en/Evaluating Short Text Stream Clustering on Large E-commerce Datasets.pdf\n",
      "Processando: artigos_en/Gender-Neutral English to Portuguese Machine Translator Promoting Inclusive Language.pdf\n",
      "Processando: artigos_en/GovBERT-BR A BERT-Based Language Model for Brazilian Portuguese Governmental Data.pdf\n",
      "Processando: artigos_en/InRanker Distilled Rankers for Zero-Shot Information Retrieval.pdf\n",
      "Processando: artigos_en/LLM-Driven Chest X-Ray Report Generation With a Modular, Reduced-Size Architecture.pdf\n",
      "Processando: artigos_en/Optimizing CleanUNet Architecture Parameters for Enhancing Speech Denoising.pdf\n",
      "Processando: artigos_en/Portuguese Emotion Detection Model Using BERTimbau Applied to COVID-19 News and Replies.pdf\n",
      "Processando: artigos_en/Pseudonymization in Legal Texts According to the LGPD A Named Entity Recognition Approach.pdf\n",
      "Processando: artigos_en/Question Answering with Texts and Tables Through Deep Reinforcement Learning.pdf\n",
      "Processando: artigos_en/SARA - A Generative AI for Legal Process Summarization Based on Chain of Density Prompt Engineering.pdf\n",
      "Processando: artigos_en/Scaling and Adapting Large Language Models for Portuguese Open Information Extraction A Comparative Study of Fine-Tuning and LoRA.pdf\n",
      "Processando: artigos_en/Tuning Hypothesis Creation Combining Discrete and Continuous Spaces for Zero-Shot Hate Speech Detection.pdf\n",
      "Processando: artigos_en/Unsupervised Statistical Keyword Extraction Pipeline Is LLM All You Need.pdf\n",
      "Processando: artigos_en/Using Complex Networks to Improve Legal Text Hierarchical Classification.pdf\n",
      "Processando: artigos_en/ptt5-v2 A Closer Look at Continued Pretraining of T5 Models for the Portuguese Language.pdf\n",
      "Todos os artigos foram incluidos.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Caminho do arquivo\n",
    "try:\n",
    "    with open('corpus_bracis.json', 'r', encoding='utf-8') as f:\n",
    "        corpus = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao abrir o arquivo JSON: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "try:\n",
    "  for idx, artigo in enumerate(artigos_info):\n",
    "        caminho_pdf = artigo['pdf']\n",
    "        print(f\"Processando: {caminho_pdf}\")\n",
    "      \n",
    "        caminho_artigo_traduzido = 'artigos_traduzidos_pt/'+ os.path.basename(caminho_pdf).replace('.pdf', '_traduzido.txt')\n",
    "        try:\n",
    "            with fitz.open(caminho_pdf) as doc:\n",
    "                artigo_completo_EN = \"\"\n",
    "                for pagina in doc:\n",
    "                    artigo_completo_EN += pagina.get_text()\n",
    "\n",
    "                    # Remove referencias\n",
    "            partes = artigo_completo_EN.split(\"References\")\n",
    "            if len(partes) > 1:\n",
    "              artigo_completo_EN = \"References\".join(partes[:-1]);\n",
    "        \n",
    "            partes = artigo_completo_EN.split(\"Bibliography\")\n",
    "            if len(partes) > 1:\n",
    "              artigo_completo_EN = \"Bibliography\".join(partes[:-1]);\n",
    "            artigo_completo_EN = artigo_completo_EN.strip().replace(\"\\n\", \" \")\n",
    "\n",
    "            with open(caminho_artigo_traduzido, \"r\", encoding=\"utf-8\") as f:\n",
    "                artigo_completo_PT = f.read()\n",
    "            artigo_completo_PT = artigo_completo_PT.strip().replace(\"\\n\", \" \")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao extrair PDF {caminho_pdf}: {e}\")\n",
    "            sys.exit()\n",
    "        corpus[idx][\"artigo_completo_PT\"] = artigo_completo_PT\n",
    "        corpus[idx][\"artigo_completo_EN\"] = artigo_completo_EN\n",
    "      \n",
    "  with open(\"corpus_bracis.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(corpus, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "  print(\"Todos os artigos foram incluidos.\")\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"Erro ao processar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "topicosdb",
   "language": "python",
   "name": "venvdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
