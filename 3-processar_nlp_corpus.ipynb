{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2587c-5238-4566-8af5-7a073a729f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e142a-4bcc-488f-a8d7-335243717c39",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92700042-f955-488b-887b-9fbeef0a3057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo spaCy para português encontrado. Carregando...\n",
      "✅ Concluido.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "nltk.download(\"rslp\", quiet=True)      # Stemmer para português\n",
    "\n",
    "stemmer_pt = RSLPStemmer()\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Modelo spaCy para português encontrado. Carregando...\")\n",
    "    nlp_pt = spacy.load(\"pt_core_news_lg\")\n",
    "except OSError:\n",
    "    print(\"Modelo spaCy para português não encontrado. Fazendo download...\")\n",
    "    os.system(\"python -m spacy download pt_core_news_lg\")\n",
    "    nlp_pt = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "try:\n",
    "    with open('corpus_bracis.json', 'r', encoding='utf-8') as f:\n",
    "        corpus = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao abrir o arquivo JSON: {e}\")\n",
    "    sys.exit\n",
    "\n",
    "print(\"✅ Concluido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcef6d9-f119-46b8-ba8f-f1bbe9c652ef",
   "metadata": {},
   "source": [
    "Extrai tokens, pos-tags e lemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa0cc82-51e3-4bd0-9eb2-818d6c728c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fazendo extração do artigo: A Large Dataset of Spontaneous Speech with the Accent Spoken in São Paulo for Automatic Speech Recognition Evaluation\n",
      "Fazendo extração do artigo: A Transformer-Based Tabular Approach to Detect Toxic Comments\n",
      "Fazendo extração do artigo: An Ensemble of LLMs Finetuned with LoRA for NER in Portuguese Legal Documents\n",
      "Fazendo extração do artigo: Aroeira: A Curated Corpus for the Portuguese Language with a Large Number of Tokens\n",
      "Fazendo extração do artigo: Assessing European and Brazilian Portuguese LLMs for NER in Specialised Domains\n",
      "Fazendo extração do artigo: Automatic Text Simplification for the Legal Domain in Brazilian Portuguese\n",
      "Fazendo extração do artigo: Developing Resource-Efficient Clinical LLMs for Brazilian Portuguese\n",
      "Fazendo extração do artigo: Diversity in Data for Speech Processing in Brazilian Portuguese\n",
      "Fazendo extração do artigo: ERASMO: Leveraging Large Language Models for Enhanced Clustering Segmentation\n",
      "Fazendo extração do artigo: Evaluating Large Language Models for Tax Law Reasoning\n",
      "Fazendo extração do artigo: Evaluating Sentiment Quantification Methods in Brazilian Portuguese Corpora\n",
      "Fazendo extração do artigo: Evaluating Short Text Stream Clustering on Large E-commerce Datasets\n",
      "Fazendo extração do artigo: Gender-Neutral English to Portuguese Machine Translator: Promoting Inclusive Language\n",
      "Fazendo extração do artigo: GovBERT-BR: A BERT-Based Language Model for Brazilian Portuguese Governmental Data\n",
      "Fazendo extração do artigo: InRanker: Distilled Rankers for Zero-Shot Information Retrieval\n",
      "Fazendo extração do artigo: LLM-Driven Chest X-Ray Report Generation With a Modular, Reduced-Size Architecture\n",
      "Fazendo extração do artigo: Optimizing CleanUNet Architecture Parameters for Enhancing Speech Denoising\n",
      "Fazendo extração do artigo: Portuguese Emotion Detection Model Using BERTimbau Applied to COVID-19 News and Replies\n",
      "Fazendo extração do artigo: Pseudonymization in Legal Texts According to the LGPD: A Named Entity Recognition Approach\n",
      "Fazendo extração do artigo: Question Answering with Texts and Tables Through Deep Reinforcement Learning\n",
      "Fazendo extração do artigo: SARA - A Generative AI for Legal Process Summarization Based on Chain of Density Prompt Engineering\n",
      "Fazendo extração do artigo: Scaling and Adapting Large Language Models for Portuguese Open Information Extraction: A Comparative Study of Fine-Tuning and LoRA\n",
      "Fazendo extração do artigo: Tuning Hypothesis Creation: Combining Discrete and Continuous Spaces for Zero-Shot Hate Speech Detection\n",
      "Fazendo extração do artigo: Unsupervised Statistical Keyword Extraction Pipeline: Is LLM All You Need?\n",
      "Fazendo extração do artigo: Using Complex Networks to Improve Legal Text Hierarchical Classification\n",
      "Fazendo extração do artigo: ptt5-v2: A Closer Look at Continued Pretraining of T5 Models for the Portuguese Language\n",
      "✅ Concluido.\n"
     ]
    }
   ],
   "source": [
    "def limpar_texto(texto):\n",
    "    # Normaliza quebras de linha e espaços\n",
    "    texto = re.sub(r'[\\n\\r\\t]+', ' ', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    # Substituir caracteres Unicode problemáticos\n",
    "    texto = texto.replace(\"−\", \"-\").replace(\"—\", \"-\").replace(\"–\", \"-\")\n",
    "    return texto.strip()\n",
    "\n",
    "for estrutura_artigo_corpus in corpus:\n",
    "    print(\"Fazendo extração do artigo: \" + estrutura_artigo_corpus[\"titulo\"])\n",
    "    artigo_pt = limpar_texto(estrutura_artigo_corpus[\"artigo_completo_PT\"])\n",
    "    try:\n",
    "        # Aumenta limite de texto para spaCy se necessário\n",
    "        if len(artigo_pt) > nlp_pt.max_length:\n",
    "            nlp_pt.max_length = len(artigo_pt) + 1000\n",
    "\n",
    "        artigo_nlp = nlp_pt(artigo_pt)\n",
    "\n",
    "        tokens = []\n",
    "        pos_tags = []\n",
    "        lemas = []\n",
    "        \n",
    "        for token in artigo_nlp:\n",
    "            # Ignora pontuações e números\n",
    "            if token.is_punct or token.is_digit or token.is_space:\n",
    "                continue\n",
    "                \n",
    "            tokens.append(token.text)\n",
    "            pos_tags.append(token.pos_)\n",
    "\n",
    "            # Se o lema for igual ao token (spaCy não alterou), tenta com stemmer RSLP\n",
    "            lema_spacy = limpar_str(token.lemma_)\n",
    "            if lema_spacy != token.text:\n",
    "                lemas.append(lema_spacy)\n",
    "            else:\n",
    "                try:\n",
    "                    # Usa o stemmer RSLP como backup\n",
    "                    lema_rslp = stemmer_pt.stem(token.text.lower())\n",
    "                    lemas.append(lema_rslp)\n",
    "                except:\n",
    "                    lemas.append(token.text)\n",
    "            \n",
    "        estrutura_artigo_corpus[\"artigo_tokenizado\"] = tokens\n",
    "        estrutura_artigo_corpus[\"pos_tagger\"] = pos_tags\n",
    "        estrutura_artigo_corpus[\"lema\"] = lemas\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar com spaCy: {e}\")\n",
    "        sys.exit\n",
    "        \n",
    "with open('corpus_bracis.json', \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(corpus, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "print(\"✅ Concluido.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319bde9-6f92-4ddd-86d3-b858a4efaf14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2871c3-0223-4ac6-813e-e38f0af69f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicosdb",
   "language": "python",
   "name": "venvdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
